{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9f477c-df5d-4e41-9db8-63857d73f777",
   "metadata": {},
   "source": [
    "# El siguiente código contiene:\n",
    "\n",
    "### 1. Pruebas para evaluación de diversos modelos LLM de Hugging Face\n",
    "### 2. Métricas de cada modelo\n",
    "### Fecha:2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced6196b-109c-489a-ab48-e65829ac6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForTokenClassification\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from json import JSONEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "89c25949-7688-45f5-be0b-5bfe88bdbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text=\"\"\"So, if you're a NASA scientist, you should be able to tell me the whole story about the Face On Mars, which obviously is evidence that there is life on Mars, and that the face was created by aliens, correct?\" No, twenty five years ago, our Viking 1 spacecraft was circling the planet, snapping photos, when it spotted the shadowy likeness of a human face. Us scientists figured out that it was just another Martian mesa, common around Cydonia, only this one had shadows that made it look like an Egyption Pharaoh. Very few days later, we revealed the image for all to see, and we made sure to note that it was a huge rock formation that just resembled a human head and face, but all of it was formed by shadows. We only announced it because we thought it would be a good way to engage the public with NASA's findings, and atrract attention to Mars-- and it did.\n",
    "The face on Mars soon became a pop icon; shot in movies, appeared in books, magazines, radio talk shows, and haunted grocery store checkout lines for 25 years. Some people thought the natural landform was evidence of life on Mars, and that us scientists wanted to hide it, but really, the defenders of the NASA budget wish there was ancient civilization on Mars. We decided to take another shot just to make sure we weren't wrong, on April 5, 1998. Michael Malin and his Mars Orbiter camera team took a picture that was ten times sharper than the original Viking photos, revealing a natural landform, which meant no alien monument. \"But that picture wasn't very clear at all, which could mean alien markings were hidden by haze\" Well no, yes that rumor started, but to prove them wrong on April 8, 2001 we decided to take another picture, making sure it was a cloudless summer day. Malin's team captured an amazing photo using the camera's absolute maximum revolution. With this camera you can discern things in a digital image, 3 times bigger than the pixel size which means if there were any signs of life, you could easily see what they were. What the picture showed was the butte or mesa, which are landforms common around the American West.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe9516-d6c8-4242-bb2c-aeadd7f41c1d",
   "metadata": {},
   "source": [
    "## Métricas FacebookAI/xlm-roberta-large-finetuned-conll03-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "b9c3289c-08e9-4546-81a1-324d29bb1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'I-ORG', 'score': 0.9999913, 'index': 8, 'word': '▁NASA', 'start': 16, 'end': 20}, {'entity': 'I-MISC', 'score': 0.9999641, 'index': 23, 'word': '▁Face', 'start': 88, 'end': 92}, {'entity': 'I-MISC', 'score': 0.99989665, 'index': 24, 'word': '▁On', 'start': 93, 'end': 95}, {'entity': 'I-MISC', 'score': 0.97350365, 'index': 25, 'word': '▁Mars', 'start': 96, 'end': 100}, {'entity': 'I-LOC', 'score': 0.9999362, 'index': 36, 'word': '▁Mars', 'start': 152, 'end': 156}, {'entity': 'I-MISC', 'score': 0.9992086, 'index': 58, 'word': '▁Viking', 'start': 240, 'end': 246}, {'entity': 'I-MISC', 'score': 0.9989502, 'index': 59, 'word': '▁1', 'start': 247, 'end': 248}, {'entity': 'I-MISC', 'score': 0.999977, 'index': 97, 'word': '▁Marti', 'start': 407, 'end': 412}, {'entity': 'I-MISC', 'score': 0.99619055, 'index': 98, 'word': 'an', 'start': 412, 'end': 414}, {'entity': 'I-LOC', 'score': 0.9999354, 'index': 103, 'word': '▁Cy', 'start': 435, 'end': 437}, {'entity': 'I-LOC', 'score': 0.99994576, 'index': 104, 'word': 'do', 'start': 437, 'end': 439}, {'entity': 'I-LOC', 'score': 0.99992585, 'index': 105, 'word': 'nia', 'start': 439, 'end': 442}, {'entity': 'I-MISC', 'score': 0.9999789, 'index': 119, 'word': '▁Egypt', 'start': 496, 'end': 501}, {'entity': 'I-MISC', 'score': 0.9614088, 'index': 120, 'word': 'ion', 'start': 501, 'end': 504}, {'entity': 'I-ORG', 'score': 0.99997246, 'index': 193, 'word': '▁NASA', 'start': 801, 'end': 805}, {'entity': 'I-LOC', 'score': 0.99979633, 'index': 205, 'word': '▁Mars', 'start': 843, 'end': 847}, {'entity': 'I-LOC', 'score': 0.9998061, 'index': 215, 'word': '▁Mars', 'start': 874, 'end': 878}, {'entity': 'I-LOC', 'score': 0.99984956, 'index': 264, 'word': '▁Mars', 'start': 1087, 'end': 1091}, {'entity': 'I-ORG', 'score': 0.99996305, 'index': 285, 'word': '▁NASA', 'start': 1168, 'end': 1172}, {'entity': 'I-LOC', 'score': 0.9998203, 'index': 295, 'word': '▁Mars', 'start': 1219, 'end': 1223}, {'entity': 'I-PER', 'score': 0.9999932, 'index': 319, 'word': '▁Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.99999106, 'index': 320, 'word': '▁Malin', 'start': 1319, 'end': 1324}, {'entity': 'I-MISC', 'score': 0.94105357, 'index': 323, 'word': '▁Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-MISC', 'score': 0.9839579, 'index': 324, 'word': '▁Or', 'start': 1338, 'end': 1340}, {'entity': 'I-MISC', 'score': 0.9913346, 'index': 325, 'word': 'bit', 'start': 1340, 'end': 1343}, {'entity': 'I-MISC', 'score': 0.9759228, 'index': 326, 'word': 'er', 'start': 1343, 'end': 1345}, {'entity': 'I-MISC', 'score': 0.999749, 'index': 341, 'word': '▁Viking', 'start': 1418, 'end': 1424}, {'entity': 'I-PER', 'score': 0.9999914, 'index': 416, 'word': '▁Malin', 'start': 1744, 'end': 1749}, {'entity': 'I-MISC', 'score': 0.92417294, 'index': 491, 'word': '▁American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.99954396, 'index': 492, 'word': '▁West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'I-LOC', 4: 'I-MISC', 5: 'I-ORG', 6: 'I-PER', 7: 'O'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "10ec8f4a-c541-4016-95f0-2c4f96893812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'I-MISC',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'I-MISC',\n",
       " 'I-MISC',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'I-LOC',\n",
       " 'O']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5b7f4-bb6f-43ec-b92b-18086e5ff6cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## etiquetas de referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a4544894-011c-4577-8760-6794dcd98d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "etiquetas_referencia2=[  'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'I-LOC',\n",
    " 'O'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68deec-07a8-42ef-96c3-7f3fc9b09c20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## tokens y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "67d00b63-43b7-4b2b-a8af-b0d0bb21740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n",
      "493\n",
      "493\n",
      "['▁So', ',', '▁if', '▁you', \"'\", 're', '▁a', '▁NASA', '▁scientist', ',', '▁you', '▁should', '▁be', '▁able', '▁to', '▁tell', '▁me', '▁the', '▁whole', '▁story', '▁about', '▁the', '▁Face', '▁On', '▁Mars', ',', '▁which', '▁obviously', '▁is', '▁evidence', '▁that', '▁there', '▁is', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁the', '▁face', '▁was', '▁created', '▁by', '▁alien', 's', ',', '▁correct', '?\"', '▁No', ',', '▁twenty', '▁five', '▁years', '▁ago', ',', '▁our', '▁Viking', '▁1', '▁space', 'craft', '▁was', '▁circ', 'ling', '▁the', '▁planet', ',', '▁sna', 'pping', '▁photos', ',', '▁when', '▁it', '▁spot', 'ted', '▁the', '▁shadow', 'y', '▁like', 'ness', '▁of', '▁a', '▁human', '▁face', '.', '▁Us', '▁scientist', 's', '▁figure', 'd', '▁out', '▁that', '▁it', '▁was', '▁just', '▁another', '▁Marti', 'an', '▁mesa', ',', '▁common', '▁around', '▁Cy', 'do', 'nia', ',', '▁only', '▁this', '▁one', '▁had', '▁shadow', 's', '▁that', '▁made', '▁it', '▁look', '▁like', '▁an', '▁Egypt', 'ion', '▁Phar', 'a', 'oh', '.', '▁Very', '▁few', '▁days', '▁later', ',', '▁we', '▁reveal', 'ed', '▁the', '▁image', '▁for', '▁all', '▁to', '▁see', ',', '▁and', '▁we', '▁made', '▁sure', '▁to', '▁note', '▁that', '▁it', '▁was', '▁a', '▁huge', '▁rock', '▁formation', '▁that', '▁just', '▁rese', 'mble', 'd', '▁a', '▁human', '▁head', '▁and', '▁face', ',', '▁but', '▁all', '▁of', '▁it', '▁was', '▁for', 'med', '▁by', '▁shadow', 's', '.', '▁We', '▁only', '▁announced', '▁it', '▁because', '▁we', '▁thought', '▁it', '▁would', '▁be', '▁a', '▁good', '▁way', '▁to', '▁engage', '▁the', '▁public', '▁with', '▁NASA', \"'\", 's', '▁finding', 's', ',', '▁and', '▁at', 'rra', 'ct', '▁attention', '▁to', '▁Mars', '-', '-', '▁and', '▁it', '▁did', '.', '▁The', '▁face', '▁on', '▁Mars', '▁soon', '▁became', '▁a', '▁pop', '▁icon', ';', '▁shot', '▁in', '▁movies', ',', '▁appeared', '▁in', '▁books', ',', '▁magazine', 's', ',', '▁radio', '▁talk', '▁shows', ',', '▁and', '▁ha', 'un', 'ted', '▁gro', 'cer', 'y', '▁store', '▁check', 'out', '▁lines', '▁for', '▁25', '▁years', '.', '▁Some', '▁people', '▁thought', '▁the', '▁natural', '▁land', 'form', '▁was', '▁evidence', '▁of', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁us', '▁scientist', 's', '▁wanted', '▁to', '▁hi', 'de', '▁it', ',', '▁but', '▁really', ',', '▁the', '▁defender', 's', '▁of', '▁the', '▁NASA', '▁budget', '▁wish', '▁there', '▁was', '▁an', 'cient', '▁civiliza', 'tion', '▁on', '▁Mars', '.', '▁We', '▁decided', '▁to', '▁take', '▁another', '▁shot', '▁just', '▁to', '▁make', '▁sure', '▁we', '▁were', 'n', \"'\", 't', '▁wrong', ',', '▁on', '▁April', '▁5', ',', '▁1998.', '▁Michael', '▁Malin', '▁and', '▁his', '▁Mars', '▁Or', 'bit', 'er', '▁camera', '▁team', '▁took', '▁a', '▁picture', '▁that', '▁was', '▁ten', '▁times', '▁sharp', 'er', '▁than', '▁the', '▁original', '▁Viking', '▁photos', ',', '▁reveal', 'ing', '▁a', '▁natural', '▁land', 'form', ',', '▁which', '▁meant', '▁no', '▁alien', '▁monument', '.', '▁\"', 'But', '▁that', '▁picture', '▁wasn', \"'\", 't', '▁very', '▁clear', '▁at', '▁all', ',', '▁which', '▁could', '▁mean', '▁alien', '▁mark', 'ings', '▁were', '▁hidden', '▁by', '▁ha', 'ze', '\"', '▁Well', '▁no', ',', '▁yes', '▁that', '▁rumor', '▁started', ',', '▁but', '▁to', '▁prove', '▁them', '▁wrong', '▁on', '▁April', '▁8', ',', '▁2001', '▁we', '▁decided', '▁to', '▁take', '▁another', '▁picture', ',', '▁making', '▁sure', '▁it', '▁was', '▁a', '▁cloud', 'less', '▁summer', '▁day', '.', '▁Malin', \"'\", 's', '▁team', '▁capture', 'd', '▁an', '▁amazing', '▁photo', '▁using', '▁the', '▁camera', \"'\", 's', '▁absolute', '▁maximum', '▁revolution', '.', '▁With', '▁this', '▁camera', '▁you', '▁can', '▁discern', '▁things', '▁in', '▁a', '▁digital', '▁image', ',', '▁3', '▁times', '▁bigger', '▁than', '▁the', '▁pixel', '▁size', '▁which', '▁means', '▁if', '▁there', '▁were', '▁any', '▁sign', 's', '▁of', '▁life', ',', '▁you', '▁could', '▁easily', '▁see', '▁what', '▁they', '▁were', '.', '▁What', '▁the', '▁picture', '▁showed', '▁was', '▁the', '▁but', 'te', '▁or', '▁mesa', ',', '▁which', '▁are', '▁land', 'form', 's', '▁common', '▁around', '▁the', '▁American', '▁West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(etiquetas_referencia2))\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8c8c5a59-e219-4df7-a90b-b4d04f794cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf77bb-a20d-477b-97ea-9963803c4aad",
   "metadata": {},
   "source": [
    "## evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9b975064-5919-4fe1-b266-8b9632d10e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'MISC': {'precision': 0.46153846153846156, 'recall': 0.5454545454545454, 'f1': 0.4999999999999999, 'number': 11}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3}, 'PER': {'precision': 1.0, 'recall': 0.6666666666666666, 'f1': 0.8, 'number': 3}, 'overall_precision': 0.65, 'overall_recall': 0.6842105263157895, 'overall_f1': 0.6666666666666667, 'overall_accuracy': 0.973630831643002}\n"
     ]
    }
   ],
   "source": [
    "#pip install seqeval\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia2])\n",
    "print(a)\n",
    "\n",
    "\n",
    "# 0: 'B-LOC',\n",
    "# 1: 'B-MISC',\n",
    "# 2: 'B-ORG',\n",
    "# 3: 'I-LOC',\n",
    "# 4: 'I-MISC',\n",
    "# 5: 'I-ORG',\n",
    "# 6: 'I-PER',\n",
    "# 7: 'O'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8faf91-0973-48db-996e-8c6ea5428ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "188bdc60-76fe-44a4-9424-b69cc0044b2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2 manu/lilt-infoxlm-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8d891893-0445-47d1-aa06-8674ce5d8cce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `liltrobertalike` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'liltrobertalike'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForMaskedLM\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanu/lilt-infoxlm-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:524\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    522\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 524\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    525\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    526\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    527\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    528\u001b[0m     code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[0;32m    529\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    532\u001b[0m )\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `liltrobertalike` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"manu/lilt-infoxlm-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a918756-380f-438f-a544-a4a6767e6eee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3 projecte-aina/DEBERTA_CIEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "00dc1b6a-7538-4a3e-abff-49b5d72321a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401 Client Error. (Request ID: Root=1-66d8d72f-7289e9a2446f9cd6499f5c4c;47b43d23-25aa-4ef7-9d7b-e9a7cf5f5410)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.\n",
      "Access to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n",
      "401 Client Error. (Request ID: Root=1-66d8d72f-71ad743658352e687ef5b188;1c733832-2d8a-4d28-887a-7ea406ceb2c7)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json.\n",
      "Access to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n",
      "401 Client Error. (Request ID: Root=1-66d8d730-07ef38330e0d1b8e1b566047;70456a39-7a1e-4b93-a93d-88e906692f7f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.\n",
      "Access to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n",
      "401 Client Error. (Request ID: Root=1-66d8d730-22a69e37194d3c814d6ebdb4;538c08db-b687-499f-b5fb-acf9138a79c9)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json.\n",
      "Access to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model projecte-aina/DEBERTA_CIEL with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForTokenClassification'>, <class 'transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForTokenClassification'>, <class 'transformers.models.deberta_v2.modeling_tf_deberta_v2.TFDebertaV2ForTokenClassification'>). See the original errors:\n\nwhile loading with AutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 666, in has_file\n    hf_raise_for_status(response)\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 321, in hf_raise_for_status\n    raise GatedRepoError(message, response) from e\nhuggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66d8d72f-7289e9a2446f9cd6499f5c4c;47b43d23-25aa-4ef7-9d7b-e9a7cf5f5410)\n\nCannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.\nAccess to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3572, in from_pretrained\n    if not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 670, in has_file\n    raise EnvironmentError(\nOSError: projecte-aina/DEBERTA_CIEL is a gated repository. Make sure to request access at https://huggingface.co/projecte-aina/DEBERTA_CIEL and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n\nwhile loading with TFAutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 666, in has_file\n    hf_raise_for_status(response)\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 321, in hf_raise_for_status\n    raise GatedRepoError(message, response) from e\nhuggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66d8d72f-71ad743658352e687ef5b188;1c733832-2d8a-4d28-887a-7ea406ceb2c7)\n\nCannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json.\nAccess to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2870, in from_pretrained\n    if has_file(pretrained_model_name_or_path, SAFE_WEIGHTS_INDEX_NAME, **has_file_kwargs):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 670, in has_file\n    raise EnvironmentError(\nOSError: projecte-aina/DEBERTA_CIEL is a gated repository. Make sure to request access at https://huggingface.co/projecte-aina/DEBERTA_CIEL and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n\nwhile loading with DebertaV2ForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 666, in has_file\n    hf_raise_for_status(response)\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 321, in hf_raise_for_status\n    raise GatedRepoError(message, response) from e\nhuggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66d8d730-07ef38330e0d1b8e1b566047;70456a39-7a1e-4b93-a93d-88e906692f7f)\n\nCannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.\nAccess to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3572, in from_pretrained\n    if not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 670, in has_file\n    raise EnvironmentError(\nOSError: projecte-aina/DEBERTA_CIEL is a gated repository. Make sure to request access at https://huggingface.co/projecte-aina/DEBERTA_CIEL and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n\nwhile loading with TFDebertaV2ForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 666, in has_file\n    hf_raise_for_status(response)\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 321, in hf_raise_for_status\n    raise GatedRepoError(message, response) from e\nhuggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66d8d730-22a69e37194d3c814d6ebdb4;538c08db-b687-499f-b5fb-acf9138a79c9)\n\nCannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json.\nAccess to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2870, in from_pretrained\n    if has_file(pretrained_model_name_or_path, SAFE_WEIGHTS_INDEX_NAME, **has_file_kwargs):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 670, in has_file\n    raise EnvironmentError(\nOSError: projecte-aina/DEBERTA_CIEL is a gated repository. Make sure to request access at https://huggingface.co/projecte-aina/DEBERTA_CIEL and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprojecte-aina/DEBERTA_CIEL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m ner_entity_results \u001b[38;5;241m=\u001b[39m pipe(text, aggregation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ner_entity_results)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    896\u001b[0m         model,\n\u001b[0;32m    897\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    898\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    899\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    900\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    902\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    903\u001b[0m     )\n\u001b[0;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:296\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    295\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    297\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m         )\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model projecte-aina/DEBERTA_CIEL with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForTokenClassification'>, <class 'transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForTokenClassification'>, <class 'transformers.models.deberta_v2.modeling_tf_deberta_v2.TFDebertaV2ForTokenClassification'>). See the original errors:\n\nwhile loading with AutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 666, in has_file\n    hf_raise_for_status(response)\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 321, in hf_raise_for_status\n    raise GatedRepoError(message, response) from e\nhuggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66d8d72f-7289e9a2446f9cd6499f5c4c;47b43d23-25aa-4ef7-9d7b-e9a7cf5f5410)\n\nCannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.\nAccess to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3572, in from_pretrained\n    if not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 670, in has_file\n    raise EnvironmentError(\nOSError: projecte-aina/DEBERTA_CIEL is a gated repository. Make sure to request access at https://huggingface.co/projecte-aina/DEBERTA_CIEL and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n\nwhile loading with TFAutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 666, in has_file\n    hf_raise_for_status(response)\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 321, in hf_raise_for_status\n    raise GatedRepoError(message, response) from e\nhuggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66d8d72f-71ad743658352e687ef5b188;1c733832-2d8a-4d28-887a-7ea406ceb2c7)\n\nCannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json.\nAccess to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2870, in from_pretrained\n    if has_file(pretrained_model_name_or_path, SAFE_WEIGHTS_INDEX_NAME, **has_file_kwargs):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 670, in has_file\n    raise EnvironmentError(\nOSError: projecte-aina/DEBERTA_CIEL is a gated repository. Make sure to request access at https://huggingface.co/projecte-aina/DEBERTA_CIEL and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n\nwhile loading with DebertaV2ForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 666, in has_file\n    hf_raise_for_status(response)\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 321, in hf_raise_for_status\n    raise GatedRepoError(message, response) from e\nhuggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66d8d730-07ef38330e0d1b8e1b566047;70456a39-7a1e-4b93-a93d-88e906692f7f)\n\nCannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.\nAccess to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3572, in from_pretrained\n    if not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 670, in has_file\n    raise EnvironmentError(\nOSError: projecte-aina/DEBERTA_CIEL is a gated repository. Make sure to request access at https://huggingface.co/projecte-aina/DEBERTA_CIEL and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n\nwhile loading with TFDebertaV2ForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 304, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 666, in has_file\n    hf_raise_for_status(response)\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 321, in hf_raise_for_status\n    raise GatedRepoError(message, response) from e\nhuggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66d8d730-22a69e37194d3c814d6ebdb4;538c08db-b687-499f-b5fb-acf9138a79c9)\n\nCannot access gated repo for url https://huggingface.co/projecte-aina/DEBERTA_CIEL/resolve/main/model.safetensors.index.json.\nAccess to model projecte-aina/DEBERTA_CIEL is restricted. You must be authenticated to access it.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2870, in from_pretrained\n    if has_file(pretrained_model_name_or_path, SAFE_WEIGHTS_INDEX_NAME, **has_file_kwargs):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 670, in has_file\n    raise EnvironmentError(\nOSError: projecte-aina/DEBERTA_CIEL is a gated repository. Make sure to request access at https://huggingface.co/projecte-aina/DEBERTA_CIEL and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"ner\", model=\"projecte-aina/DEBERTA_CIEL\")\n",
    "ner_entity_results = pipe(text, aggregation_strategy=\"simple\")\n",
    "print(ner_entity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "183cc325-3f42-4207-9d8c-18cf79db19ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_group\n",
      "GPE                            1\n",
      "location-other                 3\n",
      "organization-other             2\n",
      "organization-privatecompany    1\n",
      "person-other                   2\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "entity_group                 word         \n",
       "GPE                          Cy               1\n",
       "location-other               American         1\n",
       "                             West             1\n",
       "                             donia            1\n",
       "organization-other           Mars             1\n",
       "                             NASA             1\n",
       "organization-privatecompany  Orbiter          1\n",
       "person-other                 Malin            1\n",
       "                             Michael Malin    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame from flattened JSON\n",
    "with open(\"3 metricas projecte-ainaDEBERTA_CIEL.json\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "aux=obtener_dataframe(data)\n",
    "aux\n",
    "print(aux.groupby(['entity_group']).size())\n",
    "aux.groupby(['entity_group', 'word']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee85c10-af5e-475b-95f5-9b822eca4c04",
   "metadata": {},
   "source": [
    "## 4 gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ce00eaeb-0791-4f30-80e4-3dfc74125ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'B-ORG', 'score': 0.98227155, 'index': 8, 'word': 'NASA', 'start': 16, 'end': 20}, {'entity': 'B-MISC', 'score': 0.8342689, 'index': 23, 'word': 'Face', 'start': 88, 'end': 92}, {'entity': 'I-MISC', 'score': 0.84508187, 'index': 24, 'word': 'On', 'start': 93, 'end': 95}, {'entity': 'I-MISC', 'score': 0.67427176, 'index': 25, 'word': 'Mars', 'start': 96, 'end': 100}, {'entity': 'B-LOC', 'score': 0.97792244, 'index': 37, 'word': 'Mars', 'start': 152, 'end': 156}, {'entity': 'B-MISC', 'score': 0.9826985, 'index': 60, 'word': 'Viking', 'start': 240, 'end': 246}, {'entity': 'I-MISC', 'score': 0.9561864, 'index': 61, 'word': '1', 'start': 247, 'end': 248}, {'entity': 'B-MISC', 'score': 0.98883003, 'index': 98, 'word': 'Mart', 'start': 407, 'end': 411}, {'entity': 'I-MISC', 'score': 0.9191291, 'index': 99, 'word': '##ian', 'start': 411, 'end': 414}, {'entity': 'B-LOC', 'score': 0.95555824, 'index': 104, 'word': 'C', 'start': 435, 'end': 436}, {'entity': 'I-LOC', 'score': 0.856833, 'index': 105, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'I-LOC', 'score': 0.9699014, 'index': 106, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'B-MISC', 'score': 0.9784817, 'index': 121, 'word': 'Egypt', 'start': 496, 'end': 501}, {'entity': 'I-MISC', 'score': 0.5717449, 'index': 122, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'I-MISC', 'score': 0.4832976, 'index': 123, 'word': 'Ph', 'start': 505, 'end': 507}, {'entity': 'I-MISC', 'score': 0.6644676, 'index': 125, 'word': '##oh', 'start': 510, 'end': 512}, {'entity': 'B-ORG', 'score': 0.9827271, 'index': 194, 'word': 'NASA', 'start': 801, 'end': 805}, {'entity': 'B-LOC', 'score': 0.9836016, 'index': 205, 'word': 'Mars', 'start': 843, 'end': 847}, {'entity': 'B-LOC', 'score': 0.97961295, 'index': 215, 'word': 'Mars', 'start': 874, 'end': 878}, {'entity': 'B-LOC', 'score': 0.976, 'index': 263, 'word': 'Mars', 'start': 1087, 'end': 1091}, {'entity': 'B-ORG', 'score': 0.9822379, 'index': 282, 'word': 'NASA', 'start': 1168, 'end': 1172}, {'entity': 'B-LOC', 'score': 0.9719374, 'index': 291, 'word': 'Mars', 'start': 1219, 'end': 1223}, {'entity': 'B-PER', 'score': 0.9983796, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.99726665, 'index': 317, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'I-PER', 'score': 0.9892074, 'index': 318, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'B-ORG', 'score': 0.8298204, 'index': 321, 'word': 'Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-ORG', 'score': 0.74535716, 'index': 322, 'word': 'Or', 'start': 1338, 'end': 1340}, {'entity': 'I-ORG', 'score': 0.64858484, 'index': 323, 'word': '##biter', 'start': 1340, 'end': 1345}, {'entity': 'B-MISC', 'score': 0.9728442, 'index': 338, 'word': 'Viking', 'start': 1418, 'end': 1424}, {'entity': 'B-PER', 'score': 0.98814315, 'index': 415, 'word': 'Mali', 'start': 1744, 'end': 1748}, {'entity': 'I-PER', 'score': 0.64167535, 'index': 416, 'word': '##n', 'start': 1748, 'end': 1749}, {'entity': 'B-LOC', 'score': 0.9144795, 'index': 492, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.9814611, 'index': 493, 'word': 'West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "eccc12bc-6c9e-4abe-9d90-060a53e3493c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f98b85-62eb-4d82-a60a-af193ae7344d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4 etiquetas referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "1ad0fe2b-50d1-43f6-8faf-dde7abfc7463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'I-LOC',\n",
    " 'O'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3fd50c-594a-4bef-97c2-a1ecf79b449d",
   "metadata": {},
   "source": [
    "## 4 metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "389be715-c23c-45e6-871b-e284c8b5752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 0.5, 'recall': 0.5, 'f1': 0.5, 'number': 2}, 'MISC': {'precision': 0.16666666666666666, 'recall': 0.08333333333333333, 'f1': 0.1111111111111111, 'number': 12}, 'ORG': {'precision': 0.3, 'recall': 1.0, 'f1': 0.4615384615384615, 'number': 3}, 'PER': {'precision': 0.25, 'recall': 0.5, 'f1': 0.3333333333333333, 'number': 2}, 'overall_precision': 0.2727272727272727, 'overall_recall': 0.3157894736842105, 'overall_f1': 0.2926829268292683, 'overall_accuracy': 0.9534412955465587}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25badc08-f531-4cac-8579-ef13595194cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4d8e6-d1aa-42c3-a942-755cbb58cfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2df8815c-41d5-412e-a650-8892c4fb3bdb",
   "metadata": {},
   "source": [
    "## 5 mrm8488/distilbert-base-multi-cased-finetuned-typo-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a2162ff8-e760-433c-8266-9088bfe7764a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'ok', 'score': 0.894955, 'index': 1, 'word': 'So', 'start': 0, 'end': 2}, {'entity': 'ok', 'score': 0.9418772, 'index': 2, 'word': ',', 'start': 2, 'end': 3}, {'entity': 'ok', 'score': 0.9061194, 'index': 3, 'word': 'if', 'start': 4, 'end': 6}, {'entity': 'ok', 'score': 0.9935272, 'index': 4, 'word': 'you', 'start': 7, 'end': 10}, {'entity': 'ok', 'score': 0.9791702, 'index': 5, 'word': \"'\", 'start': 10, 'end': 11}, {'entity': 'ok', 'score': 0.99081236, 'index': 6, 'word': 're', 'start': 11, 'end': 13}, {'entity': 'ok', 'score': 0.979658, 'index': 7, 'word': 'a', 'start': 14, 'end': 15}, {'entity': 'ok', 'score': 0.7008045, 'index': 8, 'word': 'NASA', 'start': 16, 'end': 20}, {'entity': 'ok', 'score': 0.9583886, 'index': 9, 'word': 'scientist', 'start': 21, 'end': 30}, {'entity': 'ok', 'score': 0.9387937, 'index': 10, 'word': ',', 'start': 30, 'end': 31}, {'entity': 'typo', 'score': 0.955443, 'index': 11, 'word': 'you', 'start': 32, 'end': 35}, {'entity': 'typo', 'score': 0.5262742, 'index': 12, 'word': 'should', 'start': 36, 'end': 42}, {'entity': 'ok', 'score': 0.943428, 'index': 13, 'word': 'be', 'start': 43, 'end': 45}, {'entity': 'ok', 'score': 0.9880336, 'index': 14, 'word': 'able', 'start': 46, 'end': 50}, {'entity': 'ok', 'score': 0.99831665, 'index': 15, 'word': 'to', 'start': 51, 'end': 53}, {'entity': 'ok', 'score': 0.57461846, 'index': 16, 'word': 'tell', 'start': 54, 'end': 58}, {'entity': 'ok', 'score': 0.9945175, 'index': 17, 'word': 'me', 'start': 59, 'end': 61}, {'entity': 'ok', 'score': 0.810129, 'index': 18, 'word': 'the', 'start': 62, 'end': 65}, {'entity': 'ok', 'score': 0.5944688, 'index': 19, 'word': 'whole', 'start': 66, 'end': 71}, {'entity': 'ok', 'score': 0.9751041, 'index': 20, 'word': 'story', 'start': 72, 'end': 77}, {'entity': 'ok', 'score': 0.8432575, 'index': 21, 'word': 'about', 'start': 78, 'end': 83}, {'entity': 'typo', 'score': 0.93727547, 'index': 22, 'word': 'the', 'start': 84, 'end': 87}, {'entity': 'typo', 'score': 0.9830071, 'index': 23, 'word': 'Face', 'start': 88, 'end': 92}, {'entity': 'ok', 'score': 0.9957579, 'index': 24, 'word': 'On', 'start': 93, 'end': 95}, {'entity': 'ok', 'score': 0.98799103, 'index': 25, 'word': 'Mars', 'start': 96, 'end': 100}, {'entity': 'typo', 'score': 0.951169, 'index': 26, 'word': ',', 'start': 100, 'end': 101}, {'entity': 'typo', 'score': 0.9631144, 'index': 27, 'word': 'which', 'start': 102, 'end': 107}, {'entity': 'typo', 'score': 0.99146336, 'index': 28, 'word': 'obvious', 'start': 108, 'end': 115}, {'entity': 'ok', 'score': 0.9865329, 'index': 29, 'word': '##ly', 'start': 115, 'end': 117}, {'entity': 'ok', 'score': 0.99190086, 'index': 30, 'word': 'is', 'start': 118, 'end': 120}, {'entity': 'ok', 'score': 0.87074775, 'index': 31, 'word': 'evidence', 'start': 121, 'end': 129}, {'entity': 'ok', 'score': 0.99500775, 'index': 32, 'word': 'that', 'start': 130, 'end': 134}, {'entity': 'typo', 'score': 0.94334555, 'index': 33, 'word': 'there', 'start': 135, 'end': 140}, {'entity': 'ok', 'score': 0.8655428, 'index': 34, 'word': 'is', 'start': 141, 'end': 143}, {'entity': 'typo', 'score': 0.5147766, 'index': 35, 'word': 'life', 'start': 144, 'end': 148}, {'entity': 'ok', 'score': 0.9896828, 'index': 36, 'word': 'on', 'start': 149, 'end': 151}, {'entity': 'ok', 'score': 0.94172686, 'index': 37, 'word': 'Mars', 'start': 152, 'end': 156}, {'entity': 'typo', 'score': 0.92494434, 'index': 38, 'word': ',', 'start': 156, 'end': 157}, {'entity': 'ok', 'score': 0.9805345, 'index': 39, 'word': 'and', 'start': 158, 'end': 161}, {'entity': 'ok', 'score': 0.8247318, 'index': 40, 'word': 'that', 'start': 162, 'end': 166}, {'entity': 'typo', 'score': 0.8335082, 'index': 41, 'word': 'the', 'start': 167, 'end': 170}, {'entity': 'typo', 'score': 0.984109, 'index': 42, 'word': 'face', 'start': 171, 'end': 175}, {'entity': 'ok', 'score': 0.9346752, 'index': 43, 'word': 'was', 'start': 176, 'end': 179}, {'entity': 'typo', 'score': 0.61687183, 'index': 44, 'word': 'created', 'start': 180, 'end': 187}, {'entity': 'ok', 'score': 0.9703255, 'index': 45, 'word': 'by', 'start': 188, 'end': 190}, {'entity': 'typo', 'score': 0.6155792, 'index': 46, 'word': 'alien', 'start': 191, 'end': 196}, {'entity': 'ok', 'score': 0.93030524, 'index': 47, 'word': '##s', 'start': 196, 'end': 197}, {'entity': 'typo', 'score': 0.8034546, 'index': 48, 'word': ',', 'start': 197, 'end': 198}, {'entity': 'ok', 'score': 0.8684226, 'index': 49, 'word': 'correct', 'start': 199, 'end': 206}, {'entity': 'ok', 'score': 0.9899316, 'index': 50, 'word': '?', 'start': 206, 'end': 207}, {'entity': 'typo', 'score': 0.68965435, 'index': 51, 'word': '\"', 'start': 207, 'end': 208}, {'entity': 'typo', 'score': 0.5541892, 'index': 52, 'word': 'No', 'start': 209, 'end': 211}, {'entity': 'ok', 'score': 0.99887687, 'index': 53, 'word': ',', 'start': 211, 'end': 212}, {'entity': 'ok', 'score': 0.9993892, 'index': 54, 'word': 'twenty', 'start': 213, 'end': 219}, {'entity': 'ok', 'score': 0.9983181, 'index': 55, 'word': 'five', 'start': 220, 'end': 224}, {'entity': 'ok', 'score': 0.9524137, 'index': 56, 'word': 'years', 'start': 225, 'end': 230}, {'entity': 'ok', 'score': 0.98393893, 'index': 57, 'word': 'ago', 'start': 231, 'end': 234}, {'entity': 'ok', 'score': 0.877359, 'index': 58, 'word': ',', 'start': 234, 'end': 235}, {'entity': 'typo', 'score': 0.8272593, 'index': 59, 'word': 'our', 'start': 236, 'end': 239}, {'entity': 'ok', 'score': 0.88372874, 'index': 60, 'word': 'Viking', 'start': 240, 'end': 246}, {'entity': 'ok', 'score': 0.99642414, 'index': 61, 'word': '1', 'start': 247, 'end': 248}, {'entity': 'ok', 'score': 0.9792823, 'index': 62, 'word': 'spacecraft', 'start': 249, 'end': 259}, {'entity': 'ok', 'score': 0.7188466, 'index': 63, 'word': 'was', 'start': 260, 'end': 263}, {'entity': 'ok', 'score': 0.6053355, 'index': 64, 'word': 'ci', 'start': 264, 'end': 266}, {'entity': 'ok', 'score': 0.98061955, 'index': 65, 'word': '##rc', 'start': 266, 'end': 268}, {'entity': 'ok', 'score': 0.9918943, 'index': 66, 'word': '##ling', 'start': 268, 'end': 272}, {'entity': 'ok', 'score': 0.9991246, 'index': 67, 'word': 'the', 'start': 273, 'end': 276}, {'entity': 'ok', 'score': 0.99520606, 'index': 68, 'word': 'planet', 'start': 277, 'end': 283}, {'entity': 'typo', 'score': 0.9603083, 'index': 69, 'word': ',', 'start': 283, 'end': 284}, {'entity': 'typo', 'score': 0.9765087, 'index': 70, 'word': 'sna', 'start': 285, 'end': 288}, {'entity': 'ok', 'score': 0.9886219, 'index': 71, 'word': '##pping', 'start': 288, 'end': 293}, {'entity': 'ok', 'score': 0.99900526, 'index': 72, 'word': 'photos', 'start': 294, 'end': 300}, {'entity': 'typo', 'score': 0.8964089, 'index': 73, 'word': ',', 'start': 300, 'end': 301}, {'entity': 'typo', 'score': 0.8211978, 'index': 74, 'word': 'when', 'start': 302, 'end': 306}, {'entity': 'typo', 'score': 0.9168602, 'index': 75, 'word': 'it', 'start': 307, 'end': 309}, {'entity': 'ok', 'score': 0.9347477, 'index': 76, 'word': 'spotted', 'start': 310, 'end': 317}, {'entity': 'ok', 'score': 0.9530431, 'index': 77, 'word': 'the', 'start': 318, 'end': 321}, {'entity': 'typo', 'score': 0.52226573, 'index': 78, 'word': 'sh', 'start': 322, 'end': 324}, {'entity': 'ok', 'score': 0.98722374, 'index': 79, 'word': '##adow', 'start': 324, 'end': 328}, {'entity': 'ok', 'score': 0.9884067, 'index': 80, 'word': '##y', 'start': 328, 'end': 329}, {'entity': 'ok', 'score': 0.99751353, 'index': 81, 'word': 'like', 'start': 330, 'end': 334}, {'entity': 'ok', 'score': 0.98541266, 'index': 82, 'word': '##ness', 'start': 334, 'end': 338}, {'entity': 'ok', 'score': 0.99096996, 'index': 83, 'word': 'of', 'start': 339, 'end': 341}, {'entity': 'ok', 'score': 0.9794129, 'index': 84, 'word': 'a', 'start': 342, 'end': 343}, {'entity': 'ok', 'score': 0.9906974, 'index': 85, 'word': 'human', 'start': 344, 'end': 349}, {'entity': 'ok', 'score': 0.99601525, 'index': 86, 'word': 'face', 'start': 350, 'end': 354}, {'entity': 'typo', 'score': 0.80661726, 'index': 87, 'word': '.', 'start': 354, 'end': 355}, {'entity': 'typo', 'score': 0.8332319, 'index': 88, 'word': 'Us', 'start': 356, 'end': 358}, {'entity': 'ok', 'score': 0.9995962, 'index': 89, 'word': 'scientists', 'start': 359, 'end': 369}, {'entity': 'typo', 'score': 0.83959967, 'index': 90, 'word': 'figure', 'start': 370, 'end': 376}, {'entity': 'ok', 'score': 0.99624974, 'index': 91, 'word': '##d', 'start': 376, 'end': 377}, {'entity': 'ok', 'score': 0.9980217, 'index': 92, 'word': 'out', 'start': 378, 'end': 381}, {'entity': 'ok', 'score': 0.5464159, 'index': 93, 'word': 'that', 'start': 382, 'end': 386}, {'entity': 'typo', 'score': 0.95522094, 'index': 94, 'word': 'it', 'start': 387, 'end': 389}, {'entity': 'ok', 'score': 0.53848577, 'index': 95, 'word': 'was', 'start': 390, 'end': 393}, {'entity': 'typo', 'score': 0.9315185, 'index': 96, 'word': 'just', 'start': 394, 'end': 398}, {'entity': 'ok', 'score': 0.92040026, 'index': 97, 'word': 'another', 'start': 399, 'end': 406}, {'entity': 'typo', 'score': 0.5492458, 'index': 98, 'word': 'Mart', 'start': 407, 'end': 411}, {'entity': 'ok', 'score': 0.91541255, 'index': 99, 'word': '##ian', 'start': 411, 'end': 414}, {'entity': 'typo', 'score': 0.9870064, 'index': 100, 'word': 'mesa', 'start': 415, 'end': 419}, {'entity': 'typo', 'score': 0.9882908, 'index': 101, 'word': ',', 'start': 419, 'end': 420}, {'entity': 'typo', 'score': 0.94008124, 'index': 102, 'word': 'common', 'start': 421, 'end': 427}, {'entity': 'ok', 'score': 0.99831474, 'index': 103, 'word': 'around', 'start': 428, 'end': 434}, {'entity': 'ok', 'score': 0.9985098, 'index': 104, 'word': 'C', 'start': 435, 'end': 436}, {'entity': 'ok', 'score': 0.95086056, 'index': 105, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'ok', 'score': 0.99512345, 'index': 106, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'ok', 'score': 0.74133, 'index': 107, 'word': ',', 'start': 442, 'end': 443}, {'entity': 'ok', 'score': 0.976595, 'index': 108, 'word': 'only', 'start': 444, 'end': 448}, {'entity': 'ok', 'score': 0.999059, 'index': 109, 'word': 'this', 'start': 449, 'end': 453}, {'entity': 'ok', 'score': 0.9988238, 'index': 110, 'word': 'one', 'start': 454, 'end': 457}, {'entity': 'ok', 'score': 0.8208375, 'index': 111, 'word': 'had', 'start': 458, 'end': 461}, {'entity': 'typo', 'score': 0.70970184, 'index': 112, 'word': 'sh', 'start': 462, 'end': 464}, {'entity': 'ok', 'score': 0.96421015, 'index': 113, 'word': '##adow', 'start': 464, 'end': 468}, {'entity': 'ok', 'score': 0.9639605, 'index': 114, 'word': '##s', 'start': 468, 'end': 469}, {'entity': 'ok', 'score': 0.90302604, 'index': 115, 'word': 'that', 'start': 470, 'end': 474}, {'entity': 'ok', 'score': 0.6680046, 'index': 116, 'word': 'made', 'start': 475, 'end': 479}, {'entity': 'ok', 'score': 0.9925667, 'index': 117, 'word': 'it', 'start': 480, 'end': 482}, {'entity': 'typo', 'score': 0.5095613, 'index': 118, 'word': 'look', 'start': 483, 'end': 487}, {'entity': 'ok', 'score': 0.9739361, 'index': 119, 'word': 'like', 'start': 488, 'end': 492}, {'entity': 'ok', 'score': 0.9018896, 'index': 120, 'word': 'an', 'start': 493, 'end': 495}, {'entity': 'typo', 'score': 0.5793097, 'index': 121, 'word': 'Egypt', 'start': 496, 'end': 501}, {'entity': 'ok', 'score': 0.9924223, 'index': 122, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'ok', 'score': 0.9970651, 'index': 123, 'word': 'Ph', 'start': 505, 'end': 507}, {'entity': 'ok', 'score': 0.99351513, 'index': 124, 'word': '##ara', 'start': 507, 'end': 510}, {'entity': 'ok', 'score': 0.98944503, 'index': 125, 'word': '##oh', 'start': 510, 'end': 512}, {'entity': 'typo', 'score': 0.44638777, 'index': 126, 'word': '.', 'start': 512, 'end': 513}, {'entity': 'typo', 'score': 0.9027144, 'index': 127, 'word': 'Very', 'start': 514, 'end': 518}, {'entity': 'ok', 'score': 0.95206773, 'index': 128, 'word': 'few', 'start': 519, 'end': 522}, {'entity': 'ok', 'score': 0.88961726, 'index': 129, 'word': 'days', 'start': 523, 'end': 527}, {'entity': 'ok', 'score': 0.9765072, 'index': 130, 'word': 'later', 'start': 528, 'end': 533}, {'entity': 'typo', 'score': 0.884694, 'index': 131, 'word': ',', 'start': 533, 'end': 534}, {'entity': 'typo', 'score': 0.9981382, 'index': 132, 'word': 'we', 'start': 535, 'end': 537}, {'entity': 'typo', 'score': 0.8012792, 'index': 133, 'word': 'revealed', 'start': 538, 'end': 546}, {'entity': 'ok', 'score': 0.91450936, 'index': 134, 'word': 'the', 'start': 547, 'end': 550}, {'entity': 'typo', 'score': 0.9744159, 'index': 135, 'word': 'image', 'start': 551, 'end': 556}, {'entity': 'typo', 'score': 0.9916164, 'index': 136, 'word': 'for', 'start': 557, 'end': 560}, {'entity': 'typo', 'score': 0.5908708, 'index': 137, 'word': 'all', 'start': 561, 'end': 564}, {'entity': 'ok', 'score': 0.99475324, 'index': 138, 'word': 'to', 'start': 565, 'end': 567}, {'entity': 'ok', 'score': 0.8057313, 'index': 139, 'word': 'see', 'start': 568, 'end': 571}, {'entity': 'typo', 'score': 0.61359376, 'index': 140, 'word': ',', 'start': 571, 'end': 572}, {'entity': 'ok', 'score': 0.99861836, 'index': 141, 'word': 'and', 'start': 573, 'end': 576}, {'entity': 'ok', 'score': 0.6784175, 'index': 142, 'word': 'we', 'start': 577, 'end': 579}, {'entity': 'ok', 'score': 0.9465173, 'index': 143, 'word': 'made', 'start': 580, 'end': 584}, {'entity': 'ok', 'score': 0.9947103, 'index': 144, 'word': 'sure', 'start': 585, 'end': 589}, {'entity': 'ok', 'score': 0.99907136, 'index': 145, 'word': 'to', 'start': 590, 'end': 592}, {'entity': 'ok', 'score': 0.98897797, 'index': 146, 'word': 'note', 'start': 593, 'end': 597}, {'entity': 'ok', 'score': 0.9916495, 'index': 147, 'word': 'that', 'start': 598, 'end': 602}, {'entity': 'typo', 'score': 0.8916498, 'index': 148, 'word': 'it', 'start': 603, 'end': 605}, {'entity': 'ok', 'score': 0.59980196, 'index': 149, 'word': 'was', 'start': 606, 'end': 609}, {'entity': 'typo', 'score': 0.6458891, 'index': 150, 'word': 'a', 'start': 610, 'end': 611}, {'entity': 'ok', 'score': 0.86706686, 'index': 151, 'word': 'huge', 'start': 612, 'end': 616}, {'entity': 'ok', 'score': 0.74633974, 'index': 152, 'word': 'rock', 'start': 617, 'end': 621}, {'entity': 'ok', 'score': 0.9891533, 'index': 153, 'word': 'formation', 'start': 622, 'end': 631}, {'entity': 'ok', 'score': 0.8717077, 'index': 154, 'word': 'that', 'start': 632, 'end': 636}, {'entity': 'typo', 'score': 0.99484986, 'index': 155, 'word': 'just', 'start': 637, 'end': 641}, {'entity': 'typo', 'score': 0.5499501, 'index': 156, 'word': 'res', 'start': 642, 'end': 645}, {'entity': 'ok', 'score': 0.89776593, 'index': 157, 'word': '##emble', 'start': 645, 'end': 650}, {'entity': 'ok', 'score': 0.9908867, 'index': 158, 'word': '##d', 'start': 650, 'end': 651}, {'entity': 'ok', 'score': 0.747778, 'index': 159, 'word': 'a', 'start': 652, 'end': 653}, {'entity': 'ok', 'score': 0.9308926, 'index': 160, 'word': 'human', 'start': 654, 'end': 659}, {'entity': 'ok', 'score': 0.8658663, 'index': 161, 'word': 'head', 'start': 660, 'end': 664}, {'entity': 'ok', 'score': 0.9994103, 'index': 162, 'word': 'and', 'start': 665, 'end': 668}, {'entity': 'ok', 'score': 0.99929607, 'index': 163, 'word': 'face', 'start': 669, 'end': 673}, {'entity': 'ok', 'score': 0.86389965, 'index': 164, 'word': ',', 'start': 673, 'end': 674}, {'entity': 'ok', 'score': 0.99792165, 'index': 165, 'word': 'but', 'start': 675, 'end': 678}, {'entity': 'ok', 'score': 0.9912469, 'index': 166, 'word': 'all', 'start': 679, 'end': 682}, {'entity': 'ok', 'score': 0.9983467, 'index': 167, 'word': 'of', 'start': 683, 'end': 685}, {'entity': 'ok', 'score': 0.9972486, 'index': 168, 'word': 'it', 'start': 686, 'end': 688}, {'entity': 'ok', 'score': 0.99832314, 'index': 169, 'word': 'was', 'start': 689, 'end': 692}, {'entity': 'ok', 'score': 0.9965006, 'index': 170, 'word': 'formed', 'start': 693, 'end': 699}, {'entity': 'ok', 'score': 0.9985544, 'index': 171, 'word': 'by', 'start': 700, 'end': 702}, {'entity': 'ok', 'score': 0.98051447, 'index': 172, 'word': 'sh', 'start': 703, 'end': 705}, {'entity': 'ok', 'score': 0.9919695, 'index': 173, 'word': '##adow', 'start': 705, 'end': 709}, {'entity': 'ok', 'score': 0.986326, 'index': 174, 'word': '##s', 'start': 709, 'end': 710}, {'entity': 'typo', 'score': 0.79459554, 'index': 175, 'word': '.', 'start': 710, 'end': 711}, {'entity': 'typo', 'score': 0.99913234, 'index': 176, 'word': 'We', 'start': 712, 'end': 714}, {'entity': 'ok', 'score': 0.9165677, 'index': 177, 'word': 'only', 'start': 715, 'end': 719}, {'entity': 'ok', 'score': 0.9994357, 'index': 178, 'word': 'announced', 'start': 720, 'end': 729}, {'entity': 'ok', 'score': 0.9980812, 'index': 179, 'word': 'it', 'start': 730, 'end': 732}, {'entity': 'ok', 'score': 0.996609, 'index': 180, 'word': 'because', 'start': 733, 'end': 740}, {'entity': 'ok', 'score': 0.7698178, 'index': 181, 'word': 'we', 'start': 741, 'end': 743}, {'entity': 'ok', 'score': 0.9095254, 'index': 182, 'word': 'thought', 'start': 744, 'end': 751}, {'entity': 'ok', 'score': 0.9074025, 'index': 183, 'word': 'it', 'start': 752, 'end': 754}, {'entity': 'ok', 'score': 0.94705737, 'index': 184, 'word': 'would', 'start': 755, 'end': 760}, {'entity': 'ok', 'score': 0.8691749, 'index': 185, 'word': 'be', 'start': 761, 'end': 763}, {'entity': 'ok', 'score': 0.7643121, 'index': 186, 'word': 'a', 'start': 764, 'end': 765}, {'entity': 'ok', 'score': 0.92725027, 'index': 187, 'word': 'good', 'start': 766, 'end': 770}, {'entity': 'ok', 'score': 0.98407435, 'index': 188, 'word': 'way', 'start': 771, 'end': 774}, {'entity': 'ok', 'score': 0.9118401, 'index': 189, 'word': 'to', 'start': 775, 'end': 777}, {'entity': 'typo', 'score': 0.80820084, 'index': 190, 'word': 'engage', 'start': 778, 'end': 784}, {'entity': 'ok', 'score': 0.9221322, 'index': 191, 'word': 'the', 'start': 785, 'end': 788}, {'entity': 'ok', 'score': 0.8702925, 'index': 192, 'word': 'public', 'start': 789, 'end': 795}, {'entity': 'ok', 'score': 0.96217585, 'index': 193, 'word': 'with', 'start': 796, 'end': 800}, {'entity': 'ok', 'score': 0.5905169, 'index': 194, 'word': 'NASA', 'start': 801, 'end': 805}, {'entity': 'ok', 'score': 0.52509785, 'index': 195, 'word': \"'\", 'start': 805, 'end': 806}, {'entity': 'typo', 'score': 0.62146723, 'index': 196, 'word': 's', 'start': 806, 'end': 807}, {'entity': 'typo', 'score': 0.79807454, 'index': 197, 'word': 'findings', 'start': 808, 'end': 816}, {'entity': 'typo', 'score': 0.99402726, 'index': 198, 'word': ',', 'start': 816, 'end': 817}, {'entity': 'ok', 'score': 0.49941942, 'index': 199, 'word': 'and', 'start': 818, 'end': 821}, {'entity': 'typo', 'score': 0.9993143, 'index': 200, 'word': 'at', 'start': 822, 'end': 824}, {'entity': 'ok', 'score': 0.9600053, 'index': 201, 'word': '##rra', 'start': 824, 'end': 827}, {'entity': 'ok', 'score': 0.98252696, 'index': 202, 'word': '##ct', 'start': 827, 'end': 829}, {'entity': 'ok', 'score': 0.9993383, 'index': 203, 'word': 'attention', 'start': 830, 'end': 839}, {'entity': 'ok', 'score': 0.8149123, 'index': 204, 'word': 'to', 'start': 840, 'end': 842}, {'entity': 'ok', 'score': 0.928141, 'index': 205, 'word': 'Mars', 'start': 843, 'end': 847}, {'entity': 'typo', 'score': 0.98144484, 'index': 206, 'word': '-', 'start': 847, 'end': 848}, {'entity': 'typo', 'score': 0.9430255, 'index': 207, 'word': '-', 'start': 848, 'end': 849}, {'entity': 'ok', 'score': 0.9964923, 'index': 208, 'word': 'and', 'start': 850, 'end': 853}, {'entity': 'ok', 'score': 0.9620715, 'index': 209, 'word': 'it', 'start': 854, 'end': 856}, {'entity': 'ok', 'score': 0.9969836, 'index': 210, 'word': 'did', 'start': 857, 'end': 860}, {'entity': 'ok', 'score': 0.42969742, 'index': 211, 'word': '.', 'start': 860, 'end': 861}, {'entity': 'ok', 'score': 0.7296629, 'index': 212, 'word': 'The', 'start': 862, 'end': 865}, {'entity': 'ok', 'score': 0.6601624, 'index': 213, 'word': 'face', 'start': 866, 'end': 870}, {'entity': 'ok', 'score': 0.9987452, 'index': 214, 'word': 'on', 'start': 871, 'end': 873}, {'entity': 'ok', 'score': 0.995214, 'index': 215, 'word': 'Mars', 'start': 874, 'end': 878}, {'entity': 'typo', 'score': 0.76099324, 'index': 216, 'word': 'soon', 'start': 879, 'end': 883}, {'entity': 'ok', 'score': 0.93993735, 'index': 217, 'word': 'became', 'start': 884, 'end': 890}, {'entity': 'ok', 'score': 0.76330084, 'index': 218, 'word': 'a', 'start': 891, 'end': 892}, {'entity': 'typo', 'score': 0.7646893, 'index': 219, 'word': 'pop', 'start': 893, 'end': 896}, {'entity': 'ok', 'score': 0.99559265, 'index': 220, 'word': 'i', 'start': 897, 'end': 898}, {'entity': 'ok', 'score': 0.99413526, 'index': 221, 'word': '##con', 'start': 898, 'end': 901}, {'entity': 'ok', 'score': 0.7378378, 'index': 222, 'word': ';', 'start': 901, 'end': 902}, {'entity': 'typo', 'score': 0.90965664, 'index': 223, 'word': 'shot', 'start': 903, 'end': 907}, {'entity': 'ok', 'score': 0.998401, 'index': 224, 'word': 'in', 'start': 908, 'end': 910}, {'entity': 'ok', 'score': 0.99020493, 'index': 225, 'word': 'movies', 'start': 911, 'end': 917}, {'entity': 'ok', 'score': 0.8732322, 'index': 226, 'word': ',', 'start': 917, 'end': 918}, {'entity': 'typo', 'score': 0.9965013, 'index': 227, 'word': 'appeared', 'start': 919, 'end': 927}, {'entity': 'ok', 'score': 0.99827015, 'index': 228, 'word': 'in', 'start': 928, 'end': 930}, {'entity': 'ok', 'score': 0.94189245, 'index': 229, 'word': 'books', 'start': 931, 'end': 936}, {'entity': 'ok', 'score': 0.99873203, 'index': 230, 'word': ',', 'start': 936, 'end': 937}, {'entity': 'ok', 'score': 0.788286, 'index': 231, 'word': 'magazines', 'start': 938, 'end': 947}, {'entity': 'ok', 'score': 0.9985929, 'index': 232, 'word': ',', 'start': 947, 'end': 948}, {'entity': 'typo', 'score': 0.83558345, 'index': 233, 'word': 'radio', 'start': 949, 'end': 954}, {'entity': 'ok', 'score': 0.9941037, 'index': 234, 'word': 'talk', 'start': 955, 'end': 959}, {'entity': 'ok', 'score': 0.9955338, 'index': 235, 'word': 'shows', 'start': 960, 'end': 965}, {'entity': 'typo', 'score': 0.8871708, 'index': 236, 'word': ',', 'start': 965, 'end': 966}, {'entity': 'typo', 'score': 0.5271952, 'index': 237, 'word': 'and', 'start': 967, 'end': 970}, {'entity': 'typo', 'score': 0.99819934, 'index': 238, 'word': 'hau', 'start': 971, 'end': 974}, {'entity': 'ok', 'score': 0.8323784, 'index': 239, 'word': '##nted', 'start': 974, 'end': 978}, {'entity': 'typo', 'score': 0.97368705, 'index': 240, 'word': 'gr', 'start': 979, 'end': 981}, {'entity': 'ok', 'score': 0.54475385, 'index': 241, 'word': '##oce', 'start': 981, 'end': 984}, {'entity': 'ok', 'score': 0.9955764, 'index': 242, 'word': '##ry', 'start': 984, 'end': 986}, {'entity': 'ok', 'score': 0.9983236, 'index': 243, 'word': 'store', 'start': 987, 'end': 992}, {'entity': 'ok', 'score': 0.7288064, 'index': 244, 'word': 'check', 'start': 993, 'end': 998}, {'entity': 'ok', 'score': 0.9966523, 'index': 245, 'word': '##out', 'start': 998, 'end': 1001}, {'entity': 'ok', 'score': 0.99845624, 'index': 246, 'word': 'lines', 'start': 1002, 'end': 1007}, {'entity': 'typo', 'score': 0.6110894, 'index': 247, 'word': 'for', 'start': 1008, 'end': 1011}, {'entity': 'ok', 'score': 0.7403333, 'index': 248, 'word': '25', 'start': 1012, 'end': 1014}, {'entity': 'ok', 'score': 0.90443635, 'index': 249, 'word': 'years', 'start': 1015, 'end': 1020}, {'entity': 'typo', 'score': 0.7376988, 'index': 250, 'word': '.', 'start': 1020, 'end': 1021}, {'entity': 'typo', 'score': 0.7852515, 'index': 251, 'word': 'Some', 'start': 1022, 'end': 1026}, {'entity': 'ok', 'score': 0.76370704, 'index': 252, 'word': 'people', 'start': 1027, 'end': 1033}, {'entity': 'typo', 'score': 0.62583363, 'index': 253, 'word': 'thought', 'start': 1034, 'end': 1041}, {'entity': 'ok', 'score': 0.5030125, 'index': 254, 'word': 'the', 'start': 1042, 'end': 1045}, {'entity': 'typo', 'score': 0.720193, 'index': 255, 'word': 'natural', 'start': 1046, 'end': 1053}, {'entity': 'ok', 'score': 0.9988502, 'index': 256, 'word': 'land', 'start': 1054, 'end': 1058}, {'entity': 'ok', 'score': 0.97257465, 'index': 257, 'word': '##form', 'start': 1058, 'end': 1062}, {'entity': 'ok', 'score': 0.8562199, 'index': 258, 'word': 'was', 'start': 1063, 'end': 1066}, {'entity': 'typo', 'score': 0.8256486, 'index': 259, 'word': 'evidence', 'start': 1067, 'end': 1075}, {'entity': 'ok', 'score': 0.99360114, 'index': 260, 'word': 'of', 'start': 1076, 'end': 1078}, {'entity': 'ok', 'score': 0.8346857, 'index': 261, 'word': 'life', 'start': 1079, 'end': 1083}, {'entity': 'ok', 'score': 0.982291, 'index': 262, 'word': 'on', 'start': 1084, 'end': 1086}, {'entity': 'ok', 'score': 0.97399634, 'index': 263, 'word': 'Mars', 'start': 1087, 'end': 1091}, {'entity': 'typo', 'score': 0.90118784, 'index': 264, 'word': ',', 'start': 1091, 'end': 1092}, {'entity': 'ok', 'score': 0.59052175, 'index': 265, 'word': 'and', 'start': 1093, 'end': 1096}, {'entity': 'typo', 'score': 0.9531032, 'index': 266, 'word': 'that', 'start': 1097, 'end': 1101}, {'entity': 'typo', 'score': 0.5538794, 'index': 267, 'word': 'us', 'start': 1102, 'end': 1104}, {'entity': 'ok', 'score': 0.996549, 'index': 268, 'word': 'scientists', 'start': 1105, 'end': 1115}, {'entity': 'typo', 'score': 0.7577664, 'index': 269, 'word': 'wanted', 'start': 1116, 'end': 1122}, {'entity': 'ok', 'score': 0.94258755, 'index': 270, 'word': 'to', 'start': 1123, 'end': 1125}, {'entity': 'typo', 'score': 0.901848, 'index': 271, 'word': 'hide', 'start': 1126, 'end': 1130}, {'entity': 'ok', 'score': 0.87200266, 'index': 272, 'word': 'it', 'start': 1131, 'end': 1133}, {'entity': 'typo', 'score': 0.93054014, 'index': 273, 'word': ',', 'start': 1133, 'end': 1134}, {'entity': 'ok', 'score': 0.7310256, 'index': 274, 'word': 'but', 'start': 1135, 'end': 1138}, {'entity': 'ok', 'score': 0.98565906, 'index': 275, 'word': 'really', 'start': 1139, 'end': 1145}, {'entity': 'ok', 'score': 0.94660145, 'index': 276, 'word': ',', 'start': 1145, 'end': 1146}, {'entity': 'typo', 'score': 0.9548484, 'index': 277, 'word': 'the', 'start': 1147, 'end': 1150}, {'entity': 'ok', 'score': 0.85286355, 'index': 278, 'word': 'defender', 'start': 1151, 'end': 1159}, {'entity': 'ok', 'score': 0.919405, 'index': 279, 'word': '##s', 'start': 1159, 'end': 1160}, {'entity': 'ok', 'score': 0.97006005, 'index': 280, 'word': 'of', 'start': 1161, 'end': 1163}, {'entity': 'ok', 'score': 0.929363, 'index': 281, 'word': 'the', 'start': 1164, 'end': 1167}, {'entity': 'ok', 'score': 0.7226242, 'index': 282, 'word': 'NASA', 'start': 1168, 'end': 1172}, {'entity': 'typo', 'score': 0.8097878, 'index': 283, 'word': 'budget', 'start': 1173, 'end': 1179}, {'entity': 'ok', 'score': 0.95355994, 'index': 284, 'word': 'wish', 'start': 1180, 'end': 1184}, {'entity': 'typo', 'score': 0.9236313, 'index': 285, 'word': 'there', 'start': 1185, 'end': 1190}, {'entity': 'ok', 'score': 0.7190513, 'index': 286, 'word': 'was', 'start': 1191, 'end': 1194}, {'entity': 'ok', 'score': 0.66554, 'index': 287, 'word': 'ancient', 'start': 1195, 'end': 1202}, {'entity': 'ok', 'score': 0.60441756, 'index': 288, 'word': 'civili', 'start': 1203, 'end': 1209}, {'entity': 'typo', 'score': 0.6450192, 'index': 289, 'word': '##zation', 'start': 1209, 'end': 1215}, {'entity': 'ok', 'score': 0.6214468, 'index': 290, 'word': 'on', 'start': 1216, 'end': 1218}, {'entity': 'ok', 'score': 0.9483295, 'index': 291, 'word': 'Mars', 'start': 1219, 'end': 1223}, {'entity': 'typo', 'score': 0.7586299, 'index': 292, 'word': '.', 'start': 1223, 'end': 1224}, {'entity': 'typo', 'score': 0.8658028, 'index': 293, 'word': 'We', 'start': 1225, 'end': 1227}, {'entity': 'typo', 'score': 0.65636414, 'index': 294, 'word': 'decided', 'start': 1228, 'end': 1235}, {'entity': 'ok', 'score': 0.997926, 'index': 295, 'word': 'to', 'start': 1236, 'end': 1238}, {'entity': 'typo', 'score': 0.5479873, 'index': 296, 'word': 'take', 'start': 1239, 'end': 1243}, {'entity': 'ok', 'score': 0.99164516, 'index': 297, 'word': 'another', 'start': 1244, 'end': 1251}, {'entity': 'ok', 'score': 0.9741786, 'index': 298, 'word': 'shot', 'start': 1252, 'end': 1256}, {'entity': 'typo', 'score': 0.98279405, 'index': 299, 'word': 'just', 'start': 1257, 'end': 1261}, {'entity': 'ok', 'score': 0.86857635, 'index': 300, 'word': 'to', 'start': 1262, 'end': 1264}, {'entity': 'ok', 'score': 0.5942953, 'index': 301, 'word': 'make', 'start': 1265, 'end': 1269}, {'entity': 'ok', 'score': 0.9985331, 'index': 302, 'word': 'sure', 'start': 1270, 'end': 1274}, {'entity': 'typo', 'score': 0.7710986, 'index': 303, 'word': 'we', 'start': 1275, 'end': 1277}, {'entity': 'ok', 'score': 0.6990816, 'index': 304, 'word': 'were', 'start': 1278, 'end': 1282}, {'entity': 'ok', 'score': 0.8634561, 'index': 305, 'word': '##n', 'start': 1282, 'end': 1283}, {'entity': 'ok', 'score': 0.99818283, 'index': 306, 'word': \"'\", 'start': 1283, 'end': 1284}, {'entity': 'ok', 'score': 0.99803716, 'index': 307, 'word': 't', 'start': 1284, 'end': 1285}, {'entity': 'ok', 'score': 0.9882159, 'index': 308, 'word': 'wrong', 'start': 1286, 'end': 1291}, {'entity': 'typo', 'score': 0.88255525, 'index': 309, 'word': ',', 'start': 1291, 'end': 1292}, {'entity': 'typo', 'score': 0.9208057, 'index': 310, 'word': 'on', 'start': 1293, 'end': 1295}, {'entity': 'typo', 'score': 0.91730064, 'index': 311, 'word': 'April', 'start': 1296, 'end': 1301}, {'entity': 'ok', 'score': 0.5459038, 'index': 312, 'word': '5', 'start': 1302, 'end': 1303}, {'entity': 'ok', 'score': 0.5843325, 'index': 313, 'word': ',', 'start': 1303, 'end': 1304}, {'entity': 'typo', 'score': 0.8402537, 'index': 314, 'word': '1998', 'start': 1305, 'end': 1309}, {'entity': 'typo', 'score': 0.9408526, 'index': 315, 'word': '.', 'start': 1309, 'end': 1310}, {'entity': 'typo', 'score': 0.9976394, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'ok', 'score': 0.96120375, 'index': 317, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'ok', 'score': 0.923244, 'index': 318, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'ok', 'score': 0.97935665, 'index': 319, 'word': 'and', 'start': 1325, 'end': 1328}, {'entity': 'typo', 'score': 0.7523317, 'index': 320, 'word': 'his', 'start': 1329, 'end': 1332}, {'entity': 'typo', 'score': 0.91937953, 'index': 321, 'word': 'Mars', 'start': 1333, 'end': 1337}, {'entity': 'ok', 'score': 0.8556037, 'index': 322, 'word': 'Or', 'start': 1338, 'end': 1340}, {'entity': 'ok', 'score': 0.84279835, 'index': 323, 'word': '##biter', 'start': 1340, 'end': 1345}, {'entity': 'typo', 'score': 0.9986338, 'index': 324, 'word': 'camera', 'start': 1346, 'end': 1352}, {'entity': 'ok', 'score': 0.90062076, 'index': 325, 'word': 'team', 'start': 1353, 'end': 1357}, {'entity': 'typo', 'score': 0.91712064, 'index': 326, 'word': 'took', 'start': 1358, 'end': 1362}, {'entity': 'ok', 'score': 0.785382, 'index': 327, 'word': 'a', 'start': 1363, 'end': 1364}, {'entity': 'typo', 'score': 0.9040174, 'index': 328, 'word': 'picture', 'start': 1365, 'end': 1372}, {'entity': 'ok', 'score': 0.9086188, 'index': 329, 'word': 'that', 'start': 1373, 'end': 1377}, {'entity': 'typo', 'score': 0.65382206, 'index': 330, 'word': 'was', 'start': 1378, 'end': 1381}, {'entity': 'ok', 'score': 0.95172024, 'index': 331, 'word': 'ten', 'start': 1382, 'end': 1385}, {'entity': 'ok', 'score': 0.9873766, 'index': 332, 'word': 'times', 'start': 1386, 'end': 1391}, {'entity': 'ok', 'score': 0.67130685, 'index': 333, 'word': 'sharp', 'start': 1392, 'end': 1397}, {'entity': 'ok', 'score': 0.9719016, 'index': 334, 'word': '##er', 'start': 1397, 'end': 1399}, {'entity': 'ok', 'score': 0.99000204, 'index': 335, 'word': 'than', 'start': 1400, 'end': 1404}, {'entity': 'ok', 'score': 0.93401396, 'index': 336, 'word': 'the', 'start': 1405, 'end': 1408}, {'entity': 'ok', 'score': 0.77596074, 'index': 337, 'word': 'original', 'start': 1409, 'end': 1417}, {'entity': 'typo', 'score': 0.7519366, 'index': 338, 'word': 'Viking', 'start': 1418, 'end': 1424}, {'entity': 'ok', 'score': 0.66892004, 'index': 339, 'word': 'photos', 'start': 1425, 'end': 1431}, {'entity': 'typo', 'score': 0.98212206, 'index': 340, 'word': ',', 'start': 1431, 'end': 1432}, {'entity': 'typo', 'score': 0.99039525, 'index': 341, 'word': 'reveal', 'start': 1433, 'end': 1439}, {'entity': 'ok', 'score': 0.53409153, 'index': 342, 'word': '##ing', 'start': 1439, 'end': 1442}, {'entity': 'typo', 'score': 0.86396545, 'index': 343, 'word': 'a', 'start': 1443, 'end': 1444}, {'entity': 'typo', 'score': 0.6633433, 'index': 344, 'word': 'natural', 'start': 1445, 'end': 1452}, {'entity': 'ok', 'score': 0.9973527, 'index': 345, 'word': 'land', 'start': 1453, 'end': 1457}, {'entity': 'ok', 'score': 0.8840458, 'index': 346, 'word': '##form', 'start': 1457, 'end': 1461}, {'entity': 'typo', 'score': 0.9950237, 'index': 347, 'word': ',', 'start': 1461, 'end': 1462}, {'entity': 'typo', 'score': 0.9553996, 'index': 348, 'word': 'which', 'start': 1463, 'end': 1468}, {'entity': 'typo', 'score': 0.9745846, 'index': 349, 'word': 'meant', 'start': 1469, 'end': 1474}, {'entity': 'typo', 'score': 0.7203086, 'index': 350, 'word': 'no', 'start': 1475, 'end': 1477}, {'entity': 'ok', 'score': 0.91405845, 'index': 351, 'word': 'alien', 'start': 1478, 'end': 1483}, {'entity': 'ok', 'score': 0.99879634, 'index': 352, 'word': 'monument', 'start': 1484, 'end': 1492}, {'entity': 'typo', 'score': 0.56567025, 'index': 353, 'word': '.', 'start': 1492, 'end': 1493}, {'entity': 'typo', 'score': 0.971668, 'index': 354, 'word': '\"', 'start': 1494, 'end': 1495}, {'entity': 'ok', 'score': 0.9891775, 'index': 355, 'word': 'But', 'start': 1495, 'end': 1498}, {'entity': 'ok', 'score': 0.9991333, 'index': 356, 'word': 'that', 'start': 1499, 'end': 1503}, {'entity': 'ok', 'score': 0.9964754, 'index': 357, 'word': 'picture', 'start': 1504, 'end': 1511}, {'entity': 'ok', 'score': 0.89596707, 'index': 358, 'word': 'wasn', 'start': 1512, 'end': 1516}, {'entity': 'ok', 'score': 0.986717, 'index': 359, 'word': \"'\", 'start': 1516, 'end': 1517}, {'entity': 'ok', 'score': 0.98859054, 'index': 360, 'word': 't', 'start': 1517, 'end': 1518}, {'entity': 'ok', 'score': 0.9508471, 'index': 361, 'word': 'very', 'start': 1519, 'end': 1523}, {'entity': 'ok', 'score': 0.9967194, 'index': 362, 'word': 'clear', 'start': 1524, 'end': 1529}, {'entity': 'ok', 'score': 0.69285977, 'index': 363, 'word': 'at', 'start': 1530, 'end': 1532}, {'entity': 'ok', 'score': 0.9915514, 'index': 364, 'word': 'all', 'start': 1533, 'end': 1536}, {'entity': 'ok', 'score': 0.5899336, 'index': 365, 'word': ',', 'start': 1536, 'end': 1537}, {'entity': 'ok', 'score': 0.5996167, 'index': 366, 'word': 'which', 'start': 1538, 'end': 1543}, {'entity': 'typo', 'score': 0.8670836, 'index': 367, 'word': 'could', 'start': 1544, 'end': 1549}, {'entity': 'ok', 'score': 0.9761153, 'index': 368, 'word': 'mean', 'start': 1550, 'end': 1554}, {'entity': 'typo', 'score': 0.7312487, 'index': 369, 'word': 'alien', 'start': 1555, 'end': 1560}, {'entity': 'ok', 'score': 0.8062085, 'index': 370, 'word': 'marking', 'start': 1561, 'end': 1568}, {'entity': 'typo', 'score': 0.80552965, 'index': 371, 'word': '##s', 'start': 1568, 'end': 1569}, {'entity': 'typo', 'score': 0.97620255, 'index': 372, 'word': 'were', 'start': 1570, 'end': 1574}, {'entity': 'typo', 'score': 0.91833067, 'index': 373, 'word': 'hidden', 'start': 1575, 'end': 1581}, {'entity': 'typo', 'score': 0.8934933, 'index': 374, 'word': 'by', 'start': 1582, 'end': 1584}, {'entity': 'typo', 'score': 0.9554798, 'index': 375, 'word': 'ha', 'start': 1585, 'end': 1587}, {'entity': 'ok', 'score': 0.6562142, 'index': 376, 'word': '##ze', 'start': 1587, 'end': 1589}, {'entity': 'ok', 'score': 0.64348394, 'index': 377, 'word': '\"', 'start': 1589, 'end': 1590}, {'entity': 'ok', 'score': 0.949757, 'index': 378, 'word': 'Well', 'start': 1591, 'end': 1595}, {'entity': 'ok', 'score': 0.99965537, 'index': 379, 'word': 'no', 'start': 1596, 'end': 1598}, {'entity': 'ok', 'score': 0.9982284, 'index': 380, 'word': ',', 'start': 1598, 'end': 1599}, {'entity': 'typo', 'score': 0.7986995, 'index': 381, 'word': 'ye', 'start': 1600, 'end': 1602}, {'entity': 'ok', 'score': 0.9877543, 'index': 382, 'word': '##s', 'start': 1602, 'end': 1603}, {'entity': 'ok', 'score': 0.9989477, 'index': 383, 'word': 'that', 'start': 1604, 'end': 1608}, {'entity': 'typo', 'score': 0.8178804, 'index': 384, 'word': 'rum', 'start': 1609, 'end': 1612}, {'entity': 'ok', 'score': 0.9868613, 'index': 385, 'word': '##or', 'start': 1612, 'end': 1614}, {'entity': 'ok', 'score': 0.90991825, 'index': 386, 'word': 'started', 'start': 1615, 'end': 1622}, {'entity': 'typo', 'score': 0.5242705, 'index': 387, 'word': ',', 'start': 1622, 'end': 1623}, {'entity': 'ok', 'score': 0.9543154, 'index': 388, 'word': 'but', 'start': 1624, 'end': 1627}, {'entity': 'ok', 'score': 0.86853844, 'index': 389, 'word': 'to', 'start': 1628, 'end': 1630}, {'entity': 'ok', 'score': 0.7432325, 'index': 390, 'word': 'prove', 'start': 1631, 'end': 1636}, {'entity': 'ok', 'score': 0.9467291, 'index': 391, 'word': 'them', 'start': 1637, 'end': 1641}, {'entity': 'typo', 'score': 0.7128569, 'index': 392, 'word': 'wrong', 'start': 1642, 'end': 1647}, {'entity': 'ok', 'score': 0.62498444, 'index': 393, 'word': 'on', 'start': 1648, 'end': 1650}, {'entity': 'typo', 'score': 0.6839772, 'index': 394, 'word': 'April', 'start': 1651, 'end': 1656}, {'entity': 'ok', 'score': 0.68691665, 'index': 395, 'word': '8', 'start': 1657, 'end': 1658}, {'entity': 'ok', 'score': 0.84025437, 'index': 396, 'word': ',', 'start': 1658, 'end': 1659}, {'entity': 'typo', 'score': 0.9157925, 'index': 397, 'word': '2001', 'start': 1660, 'end': 1664}, {'entity': 'typo', 'score': 0.7844509, 'index': 398, 'word': 'we', 'start': 1665, 'end': 1667}, {'entity': 'typo', 'score': 0.5589368, 'index': 399, 'word': 'decided', 'start': 1668, 'end': 1675}, {'entity': 'ok', 'score': 0.99429286, 'index': 400, 'word': 'to', 'start': 1676, 'end': 1678}, {'entity': 'typo', 'score': 0.84421164, 'index': 401, 'word': 'take', 'start': 1679, 'end': 1683}, {'entity': 'ok', 'score': 0.978843, 'index': 402, 'word': 'another', 'start': 1684, 'end': 1691}, {'entity': 'ok', 'score': 0.59987646, 'index': 403, 'word': 'picture', 'start': 1692, 'end': 1699}, {'entity': 'typo', 'score': 0.7050702, 'index': 404, 'word': ',', 'start': 1699, 'end': 1700}, {'entity': 'ok', 'score': 0.95890915, 'index': 405, 'word': 'making', 'start': 1701, 'end': 1707}, {'entity': 'ok', 'score': 0.99975187, 'index': 406, 'word': 'sure', 'start': 1708, 'end': 1712}, {'entity': 'ok', 'score': 0.98025346, 'index': 407, 'word': 'it', 'start': 1713, 'end': 1715}, {'entity': 'ok', 'score': 0.83194023, 'index': 408, 'word': 'was', 'start': 1716, 'end': 1719}, {'entity': 'typo', 'score': 0.9467474, 'index': 409, 'word': 'a', 'start': 1720, 'end': 1721}, {'entity': 'typo', 'score': 0.91021883, 'index': 410, 'word': 'cloud', 'start': 1722, 'end': 1727}, {'entity': 'ok', 'score': 0.9859971, 'index': 411, 'word': '##less', 'start': 1727, 'end': 1731}, {'entity': 'typo', 'score': 0.98307884, 'index': 412, 'word': 'summer', 'start': 1732, 'end': 1738}, {'entity': 'ok', 'score': 0.994193, 'index': 413, 'word': 'day', 'start': 1739, 'end': 1742}, {'entity': 'typo', 'score': 0.4313977, 'index': 414, 'word': '.', 'start': 1742, 'end': 1743}, {'entity': 'typo', 'score': 0.96405447, 'index': 415, 'word': 'Mali', 'start': 1744, 'end': 1748}, {'entity': 'ok', 'score': 0.6155722, 'index': 416, 'word': '##n', 'start': 1748, 'end': 1749}, {'entity': 'ok', 'score': 0.9290005, 'index': 417, 'word': \"'\", 'start': 1749, 'end': 1750}, {'entity': 'ok', 'score': 0.90629065, 'index': 418, 'word': 's', 'start': 1750, 'end': 1751}, {'entity': 'ok', 'score': 0.868706, 'index': 419, 'word': 'team', 'start': 1752, 'end': 1756}, {'entity': 'ok', 'score': 0.8017526, 'index': 420, 'word': 'captured', 'start': 1757, 'end': 1765}, {'entity': 'ok', 'score': 0.7134167, 'index': 421, 'word': 'an', 'start': 1766, 'end': 1768}, {'entity': 'typo', 'score': 0.5621726, 'index': 422, 'word': 'ama', 'start': 1769, 'end': 1772}, {'entity': 'ok', 'score': 0.7826029, 'index': 423, 'word': '##zing', 'start': 1772, 'end': 1776}, {'entity': 'ok', 'score': 0.99448264, 'index': 424, 'word': 'photo', 'start': 1777, 'end': 1782}, {'entity': 'ok', 'score': 0.73524755, 'index': 425, 'word': 'using', 'start': 1783, 'end': 1788}, {'entity': 'ok', 'score': 0.6591615, 'index': 426, 'word': 'the', 'start': 1789, 'end': 1792}, {'entity': 'typo', 'score': 0.86533505, 'index': 427, 'word': 'camera', 'start': 1793, 'end': 1799}, {'entity': 'ok', 'score': 0.90541, 'index': 428, 'word': \"'\", 'start': 1799, 'end': 1800}, {'entity': 'ok', 'score': 0.9281528, 'index': 429, 'word': 's', 'start': 1800, 'end': 1801}, {'entity': 'typo', 'score': 0.8930355, 'index': 430, 'word': 'absolute', 'start': 1802, 'end': 1810}, {'entity': 'ok', 'score': 0.9972451, 'index': 431, 'word': 'maximum', 'start': 1811, 'end': 1818}, {'entity': 'ok', 'score': 0.999348, 'index': 432, 'word': 'revolution', 'start': 1819, 'end': 1829}, {'entity': 'typo', 'score': 0.6555487, 'index': 433, 'word': '.', 'start': 1829, 'end': 1830}, {'entity': 'ok', 'score': 0.5426304, 'index': 434, 'word': 'With', 'start': 1831, 'end': 1835}, {'entity': 'ok', 'score': 0.9795935, 'index': 435, 'word': 'this', 'start': 1836, 'end': 1840}, {'entity': 'ok', 'score': 0.9408998, 'index': 436, 'word': 'camera', 'start': 1841, 'end': 1847}, {'entity': 'typo', 'score': 0.9998209, 'index': 437, 'word': 'you', 'start': 1848, 'end': 1851}, {'entity': 'ok', 'score': 0.9576567, 'index': 438, 'word': 'can', 'start': 1852, 'end': 1855}, {'entity': 'typo', 'score': 0.573261, 'index': 439, 'word': 'disc', 'start': 1856, 'end': 1860}, {'entity': 'ok', 'score': 0.9691068, 'index': 440, 'word': '##ern', 'start': 1860, 'end': 1863}, {'entity': 'ok', 'score': 0.8217143, 'index': 441, 'word': 'things', 'start': 1864, 'end': 1870}, {'entity': 'typo', 'score': 0.82698673, 'index': 442, 'word': 'in', 'start': 1871, 'end': 1873}, {'entity': 'typo', 'score': 0.78578687, 'index': 443, 'word': 'a', 'start': 1874, 'end': 1875}, {'entity': 'typo', 'score': 0.74662477, 'index': 444, 'word': 'digital', 'start': 1876, 'end': 1883}, {'entity': 'ok', 'score': 0.5583283, 'index': 445, 'word': 'image', 'start': 1884, 'end': 1889}, {'entity': 'typo', 'score': 0.99453664, 'index': 446, 'word': ',', 'start': 1889, 'end': 1890}, {'entity': 'typo', 'score': 0.99513924, 'index': 447, 'word': '3', 'start': 1891, 'end': 1892}, {'entity': 'ok', 'score': 0.97854996, 'index': 448, 'word': 'times', 'start': 1893, 'end': 1898}, {'entity': 'ok', 'score': 0.9518669, 'index': 449, 'word': 'bigger', 'start': 1899, 'end': 1905}, {'entity': 'ok', 'score': 0.9959552, 'index': 450, 'word': 'than', 'start': 1906, 'end': 1910}, {'entity': 'ok', 'score': 0.70804185, 'index': 451, 'word': 'the', 'start': 1911, 'end': 1914}, {'entity': 'ok', 'score': 0.7743477, 'index': 452, 'word': 'pi', 'start': 1915, 'end': 1917}, {'entity': 'ok', 'score': 0.8590469, 'index': 453, 'word': '##xel', 'start': 1917, 'end': 1920}, {'entity': 'typo', 'score': 0.88074666, 'index': 454, 'word': 'size', 'start': 1921, 'end': 1925}, {'entity': 'typo', 'score': 0.977126, 'index': 455, 'word': 'which', 'start': 1926, 'end': 1931}, {'entity': 'typo', 'score': 0.71839124, 'index': 456, 'word': 'means', 'start': 1932, 'end': 1937}, {'entity': 'ok', 'score': 0.7110502, 'index': 457, 'word': 'if', 'start': 1938, 'end': 1940}, {'entity': 'ok', 'score': 0.72081035, 'index': 458, 'word': 'there', 'start': 1941, 'end': 1946}, {'entity': 'ok', 'score': 0.90700245, 'index': 459, 'word': 'were', 'start': 1947, 'end': 1951}, {'entity': 'ok', 'score': 0.9401142, 'index': 460, 'word': 'any', 'start': 1952, 'end': 1955}, {'entity': 'ok', 'score': 0.8010327, 'index': 461, 'word': 'signs', 'start': 1956, 'end': 1961}, {'entity': 'ok', 'score': 0.9774201, 'index': 462, 'word': 'of', 'start': 1962, 'end': 1964}, {'entity': 'ok', 'score': 0.72982603, 'index': 463, 'word': 'life', 'start': 1965, 'end': 1969}, {'entity': 'typo', 'score': 0.98334414, 'index': 464, 'word': ',', 'start': 1969, 'end': 1970}, {'entity': 'typo', 'score': 0.9922644, 'index': 465, 'word': 'you', 'start': 1971, 'end': 1974}, {'entity': 'ok', 'score': 0.9828555, 'index': 466, 'word': 'could', 'start': 1975, 'end': 1980}, {'entity': 'ok', 'score': 0.9977181, 'index': 467, 'word': 'easily', 'start': 1981, 'end': 1987}, {'entity': 'ok', 'score': 0.99048984, 'index': 468, 'word': 'see', 'start': 1988, 'end': 1991}, {'entity': 'ok', 'score': 0.99270344, 'index': 469, 'word': 'what', 'start': 1992, 'end': 1996}, {'entity': 'ok', 'score': 0.7724791, 'index': 470, 'word': 'they', 'start': 1997, 'end': 2001}, {'entity': 'ok', 'score': 0.9829512, 'index': 471, 'word': 'were', 'start': 2002, 'end': 2006}, {'entity': 'typo', 'score': 0.9201129, 'index': 472, 'word': '.', 'start': 2006, 'end': 2007}, {'entity': 'typo', 'score': 0.51953334, 'index': 473, 'word': 'What', 'start': 2008, 'end': 2012}, {'entity': 'ok', 'score': 0.9992943, 'index': 474, 'word': 'the', 'start': 2013, 'end': 2016}, {'entity': 'ok', 'score': 0.98957855, 'index': 475, 'word': 'picture', 'start': 2017, 'end': 2024}, {'entity': 'typo', 'score': 0.58303297, 'index': 476, 'word': 'showed', 'start': 2025, 'end': 2031}, {'entity': 'typo', 'score': 0.8085752, 'index': 477, 'word': 'was', 'start': 2032, 'end': 2035}, {'entity': 'typo', 'score': 0.9929074, 'index': 478, 'word': 'the', 'start': 2036, 'end': 2039}, {'entity': 'typo', 'score': 0.9818252, 'index': 479, 'word': 'but', 'start': 2040, 'end': 2043}, {'entity': 'ok', 'score': 0.5864657, 'index': 480, 'word': '##te', 'start': 2043, 'end': 2045}, {'entity': 'ok', 'score': 0.9995679, 'index': 481, 'word': 'or', 'start': 2046, 'end': 2048}, {'entity': 'ok', 'score': 0.8257528, 'index': 482, 'word': 'mesa', 'start': 2049, 'end': 2053}, {'entity': 'typo', 'score': 0.78061324, 'index': 483, 'word': ',', 'start': 2053, 'end': 2054}, {'entity': 'typo', 'score': 0.529569, 'index': 484, 'word': 'which', 'start': 2055, 'end': 2060}, {'entity': 'typo', 'score': 0.98165315, 'index': 485, 'word': 'are', 'start': 2061, 'end': 2064}, {'entity': 'ok', 'score': 0.7917957, 'index': 486, 'word': 'land', 'start': 2065, 'end': 2069}, {'entity': 'ok', 'score': 0.8484906, 'index': 487, 'word': '##form', 'start': 2069, 'end': 2073}, {'entity': 'typo', 'score': 0.5294084, 'index': 488, 'word': '##s', 'start': 2073, 'end': 2074}, {'entity': 'typo', 'score': 0.66130173, 'index': 489, 'word': 'common', 'start': 2075, 'end': 2081}, {'entity': 'typo', 'score': 0.79395646, 'index': 490, 'word': 'around', 'start': 2082, 'end': 2088}, {'entity': 'typo', 'score': 0.80247337, 'index': 491, 'word': 'the', 'start': 2089, 'end': 2092}, {'entity': 'typo', 'score': 0.90861714, 'index': 492, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'ok', 'score': 0.916328, 'index': 493, 'word': 'West', 'start': 2102, 'end': 2106}, {'entity': 'ok', 'score': 0.8712888, 'index': 494, 'word': '.', 'start': 2106, 'end': 2107}]\n",
      "{0: 'O', 1: 'typo', 2: 'ok'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilbert-base-multi-cased-finetuned-typo-detection\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"mrm8488/distilbert-base-multi-cased-finetuned-typo-detection\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a616298d-53a0-4153-992b-798f978ce2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'typo', 2: 'ok'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e685d8-ff8a-43ec-8ab0-958a89056ee8",
   "metadata": {},
   "source": [
    "## 6 sagorsarker/codeswitch-spaeng-ner-lince "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5e5dcd1c-f49d-486d-9d72-e0ce4498a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sagorsarker/codeswitch-spaeng-ner-lince were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'B-ORG', 'score': 0.90762955, 'index': 8, 'word': 'NASA', 'start': 16, 'end': 20}, {'entity': 'B-LOC', 'score': 0.45779723, 'index': 25, 'word': 'Mars', 'start': 96, 'end': 100}, {'entity': 'B-LOC', 'score': 0.67780584, 'index': 37, 'word': 'Mars', 'start': 152, 'end': 156}, {'entity': 'B-PROD', 'score': 0.823176, 'index': 60, 'word': 'Viking', 'start': 240, 'end': 246}, {'entity': 'I-PROD', 'score': 0.6758249, 'index': 61, 'word': '1', 'start': 247, 'end': 248}, {'entity': 'B-OTHER', 'score': 0.5148923, 'index': 98, 'word': 'Mart', 'start': 407, 'end': 411}, {'entity': 'B-OTHER', 'score': 0.42019445, 'index': 99, 'word': '##ian', 'start': 411, 'end': 414}, {'entity': 'B-LOC', 'score': 0.76582533, 'index': 104, 'word': 'C', 'start': 435, 'end': 436}, {'entity': 'B-LOC', 'score': 0.74930793, 'index': 105, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'I-LOC', 'score': 0.34308487, 'index': 106, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'B-OTHER', 'score': 0.60952294, 'index': 121, 'word': 'Egypt', 'start': 496, 'end': 501}, {'entity': 'I-OTHER', 'score': 0.6259159, 'index': 122, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'B-ORG', 'score': 0.9731985, 'index': 194, 'word': 'NASA', 'start': 801, 'end': 805}, {'entity': 'B-LOC', 'score': 0.62910813, 'index': 205, 'word': 'Mars', 'start': 843, 'end': 847}, {'entity': 'B-LOC', 'score': 0.6685378, 'index': 215, 'word': 'Mars', 'start': 874, 'end': 878}, {'entity': 'B-LOC', 'score': 0.71298903, 'index': 263, 'word': 'Mars', 'start': 1087, 'end': 1091}, {'entity': 'B-ORG', 'score': 0.9437394, 'index': 282, 'word': 'NASA', 'start': 1168, 'end': 1172}, {'entity': 'B-LOC', 'score': 0.60219324, 'index': 291, 'word': 'Mars', 'start': 1219, 'end': 1223}, {'entity': 'B-TIME', 'score': 0.9826582, 'index': 311, 'word': 'April', 'start': 1296, 'end': 1301}, {'entity': 'I-TIME', 'score': 0.9323657, 'index': 312, 'word': '5', 'start': 1302, 'end': 1303}, {'entity': 'I-TIME', 'score': 0.8260366, 'index': 313, 'word': ',', 'start': 1303, 'end': 1304}, {'entity': 'I-TIME', 'score': 0.887798, 'index': 314, 'word': '1998', 'start': 1305, 'end': 1309}, {'entity': 'B-PER', 'score': 0.9961196, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.99166673, 'index': 317, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'I-PER', 'score': 0.9911315, 'index': 318, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'B-PROD', 'score': 0.6550399, 'index': 321, 'word': 'Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-PROD', 'score': 0.80179524, 'index': 322, 'word': 'Or', 'start': 1338, 'end': 1340}, {'entity': 'I-PROD', 'score': 0.8049131, 'index': 323, 'word': '##biter', 'start': 1340, 'end': 1345}, {'entity': 'B-OTHER', 'score': 0.35082105, 'index': 338, 'word': 'Viking', 'start': 1418, 'end': 1424}, {'entity': 'B-TIME', 'score': 0.97395533, 'index': 394, 'word': 'April', 'start': 1651, 'end': 1656}, {'entity': 'I-TIME', 'score': 0.66215277, 'index': 395, 'word': '8', 'start': 1657, 'end': 1658}, {'entity': 'B-GROUP', 'score': 0.5102115, 'index': 415, 'word': 'Mali', 'start': 1744, 'end': 1748}, {'entity': 'B-GROUP', 'score': 0.34341586, 'index': 416, 'word': '##n', 'start': 1748, 'end': 1749}, {'entity': 'B-LOC', 'score': 0.8453891, 'index': 492, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.663866, 'index': 493, 'word': 'West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'B-EVENT', 1: 'B-GROUP', 2: 'B-LOC', 3: 'B-ORG', 4: 'B-OTHER', 5: 'B-PER', 6: 'B-PROD', 7: 'B-TIME', 8: 'B-TITLE', 9: 'I-EVENT', 10: 'I-GROUP', 11: 'I-LOC', 12: 'I-ORG', 13: 'I-OTHER', 14: 'I-PER', 15: 'I-PROD', 16: 'I-TIME', 17: 'I-TITLE', 18: 'O'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/codeswitch-spaeng-ner-lince\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"sagorsarker/codeswitch-spaeng-ner-lince\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "038a8d3c-b2d1-4ca8-9fbc-1ecfb970354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-EVENT', 1: 'B-GROUP', 2: 'B-LOC', 3: 'B-ORG', 4: 'B-OTHER', 5: 'B-PER', 6: 'B-PROD', 7: 'B-TIME', 8: 'B-TITLE', 9: 'I-EVENT', 10: 'I-GROUP', 11: 'I-LOC', 12: 'I-ORG', 13: 'I-OTHER', 14: 'I-PER', 15: 'I-PROD', 16: 'I-TIME', 17: 'I-TITLE', 18: 'O'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "7409783e-6acf-406b-973e-a07eba3d8a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n",
      "['O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROD', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROD', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'B-OTHER', 'I-PROD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTHER', 'B-OTHER', 'O', 'I-TIME', 'O', 'O', 'B-LOC', 'B-LOC', 'I-LOC', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTHER', 'I-OTHER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'I-TIME', 'O', 'I-TIME', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROD', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GROUP', 'O', 'O', 'O', 'I-TIME', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'O', 'B-PER', 'B-GROUP', 'B-GROUP', 'O', 'O', 'B-PROD', 'I-PROD', 'I-PROD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTHER', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GROUP', 'B-GROUP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print(labels)\n",
    "etiquetas_referencia=['O',\n",
    " 'I-TIME',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'I-TIME',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PROD',\n",
    " 'I-TIME',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PROD',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-TIME',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-TIME',\n",
    " 'O',\n",
    " 'B-OTHER',\n",
    " 'I-PROD',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-OTHER',\n",
    " 'B-OTHER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-OTHER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PROD',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PROD',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PROD',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PROD',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-TIME',\n",
    " 'I-TIME',\n",
    " 'I-TIME',\n",
    " 'I-TIME',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'B-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PROD',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-OTHER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-TIME',\n",
    " 'I-TIME',\n",
    " 'I-TIME',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    "'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'O']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "da34a22d-b2ee-4d3b-995a-7664435fe2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GROUP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3}, 'OTHER': {'precision': 0.8, 'recall': 0.8, 'f1': 0.8000000000000002, 'number': 5}, 'PER': {'precision': 1.0, 'recall': 0.3333333333333333, 'f1': 0.5, 'number': 3}, 'PROD': {'precision': 0.875, 'recall': 0.875, 'f1': 0.875, 'number': 8}, 'TIME': {'precision': 0.20588235294117646, 'recall': 1.0, 'f1': 0.34146341463414637, 'number': 7}, 'overall_precision': 0.423728813559322, 'overall_recall': 0.8620689655172413, 'overall_f1': 0.5681818181818181, 'overall_accuracy': 0.9291497975708503}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02ef71-62d9-40f1-98a8-138419cff895",
   "metadata": {},
   "source": [
    "## 7  gunghio/xlm-roberta-base-finetuned-panx-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "005c3078-1bf3-4fa7-a525-33b507bd3615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'B-ORG', 'score': 0.9839335, 'index': 8, 'word': '▁NASA', 'start': 16, 'end': 20}, {'entity': 'B-ORG', 'score': 0.9196439, 'index': 23, 'word': '▁Face', 'start': 88, 'end': 92}, {'entity': 'I-ORG', 'score': 0.9387356, 'index': 24, 'word': '▁On', 'start': 93, 'end': 95}, {'entity': 'I-ORG', 'score': 0.9189941, 'index': 25, 'word': '▁Mars', 'start': 96, 'end': 100}, {'entity': 'B-LOC', 'score': 0.8524113, 'index': 36, 'word': '▁Mars', 'start': 152, 'end': 156}, {'entity': 'B-ORG', 'score': 0.9265073, 'index': 58, 'word': '▁Viking', 'start': 240, 'end': 246}, {'entity': 'I-ORG', 'score': 0.9643027, 'index': 59, 'word': '▁1', 'start': 247, 'end': 248}, {'entity': 'I-ORG', 'score': 0.9504218, 'index': 60, 'word': '▁space', 'start': 249, 'end': 254}, {'entity': 'I-ORG', 'score': 0.96277016, 'index': 61, 'word': 'craft', 'start': 254, 'end': 259}, {'entity': 'B-ORG', 'score': 0.6181985, 'index': 97, 'word': '▁Marti', 'start': 407, 'end': 412}, {'entity': 'B-ORG', 'score': 0.61589026, 'index': 98, 'word': 'an', 'start': 412, 'end': 414}, {'entity': 'I-LOC', 'score': 0.49310815, 'index': 99, 'word': '▁mesa', 'start': 415, 'end': 419}, {'entity': 'B-LOC', 'score': 0.8700732, 'index': 103, 'word': '▁Cy', 'start': 435, 'end': 437}, {'entity': 'B-LOC', 'score': 0.84444326, 'index': 104, 'word': 'do', 'start': 437, 'end': 439}, {'entity': 'B-LOC', 'score': 0.85732704, 'index': 105, 'word': 'nia', 'start': 439, 'end': 442}, {'entity': 'B-ORG', 'score': 0.6250014, 'index': 119, 'word': '▁Egypt', 'start': 496, 'end': 501}, {'entity': 'B-ORG', 'score': 0.6232516, 'index': 120, 'word': 'ion', 'start': 501, 'end': 504}, {'entity': 'I-ORG', 'score': 0.5170185, 'index': 121, 'word': '▁Phar', 'start': 505, 'end': 509}, {'entity': 'I-ORG', 'score': 0.51358134, 'index': 122, 'word': 'a', 'start': 509, 'end': 510}, {'entity': 'I-ORG', 'score': 0.5280937, 'index': 123, 'word': 'oh', 'start': 510, 'end': 512}, {'entity': 'I-LOC', 'score': 0.46428245, 'index': 152, 'word': '▁formation', 'start': 622, 'end': 631}, {'entity': 'B-ORG', 'score': 0.90882486, 'index': 193, 'word': '▁NASA', 'start': 801, 'end': 805}, {'entity': 'B-LOC', 'score': 0.5685518, 'index': 205, 'word': '▁Mars', 'start': 843, 'end': 847}, {'entity': 'B-LOC', 'score': 0.89254224, 'index': 215, 'word': '▁Mars', 'start': 874, 'end': 878}, {'entity': 'B-LOC', 'score': 0.8768112, 'index': 264, 'word': '▁Mars', 'start': 1087, 'end': 1091}, {'entity': 'B-ORG', 'score': 0.9750642, 'index': 285, 'word': '▁NASA', 'start': 1168, 'end': 1172}, {'entity': 'I-ORG', 'score': 0.79819006, 'index': 286, 'word': '▁budget', 'start': 1173, 'end': 1179}, {'entity': 'B-ORG', 'score': 0.49600738, 'index': 290, 'word': '▁an', 'start': 1195, 'end': 1197}, {'entity': 'B-ORG', 'score': 0.504356, 'index': 291, 'word': 'cient', 'start': 1197, 'end': 1202}, {'entity': 'I-ORG', 'score': 0.58132344, 'index': 292, 'word': '▁civiliza', 'start': 1203, 'end': 1211}, {'entity': 'I-ORG', 'score': 0.51857996, 'index': 293, 'word': 'tion', 'start': 1211, 'end': 1215}, {'entity': 'B-LOC', 'score': 0.6887058, 'index': 295, 'word': '▁Mars', 'start': 1219, 'end': 1223}, {'entity': 'B-PER', 'score': 0.90942806, 'index': 319, 'word': '▁Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.82762694, 'index': 320, 'word': '▁Malin', 'start': 1319, 'end': 1324}, {'entity': 'B-ORG', 'score': 0.949565, 'index': 323, 'word': '▁Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-ORG', 'score': 0.97294813, 'index': 324, 'word': '▁Or', 'start': 1338, 'end': 1340}, {'entity': 'I-ORG', 'score': 0.97262955, 'index': 325, 'word': 'bit', 'start': 1340, 'end': 1343}, {'entity': 'I-ORG', 'score': 0.9700439, 'index': 326, 'word': 'er', 'start': 1343, 'end': 1345}, {'entity': 'I-ORG', 'score': 0.92750436, 'index': 327, 'word': '▁camera', 'start': 1346, 'end': 1352}, {'entity': 'I-ORG', 'score': 0.8475988, 'index': 328, 'word': '▁team', 'start': 1353, 'end': 1357}, {'entity': 'B-ORG', 'score': 0.64778906, 'index': 341, 'word': '▁Viking', 'start': 1418, 'end': 1424}, {'entity': 'I-ORG', 'score': 0.6530955, 'index': 342, 'word': '▁photos', 'start': 1425, 'end': 1431}, {'entity': 'B-ORG', 'score': 0.6267937, 'index': 443, 'word': '▁digital', 'start': 1876, 'end': 1883}, {'entity': 'I-ORG', 'score': 0.5914651, 'index': 444, 'word': '▁image', 'start': 1884, 'end': 1889}, {'entity': 'B-LOC', 'score': 0.92446667, 'index': 491, 'word': '▁American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.9377042, 'index': 492, 'word': '▁West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gunghio/xlm-roberta-base-finetuned-panx-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"gunghio/xlm-roberta-base-finetuned-panx-ner\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1b3162bf-20f2-4ebb-923d-bb24f41d88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'I-LOC', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'I-ORG', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "['▁So', ',', '▁if', '▁you', \"'\", 're', '▁a', '▁NASA', '▁scientist', ',', '▁you', '▁should', '▁be', '▁able', '▁to', '▁tell', '▁me', '▁the', '▁whole', '▁story', '▁about', '▁the', '▁Face', '▁On', '▁Mars', ',', '▁which', '▁obviously', '▁is', '▁evidence', '▁that', '▁there', '▁is', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁the', '▁face', '▁was', '▁created', '▁by', '▁alien', 's', ',', '▁correct', '?\"', '▁No', ',', '▁twenty', '▁five', '▁years', '▁ago', ',', '▁our', '▁Viking', '▁1', '▁space', 'craft', '▁was', '▁circ', 'ling', '▁the', '▁planet', ',', '▁sna', 'pping', '▁photos', ',', '▁when', '▁it', '▁spot', 'ted', '▁the', '▁shadow', 'y', '▁like', 'ness', '▁of', '▁a', '▁human', '▁face', '.', '▁Us', '▁scientist', 's', '▁figure', 'd', '▁out', '▁that', '▁it', '▁was', '▁just', '▁another', '▁Marti', 'an', '▁mesa', ',', '▁common', '▁around', '▁Cy', 'do', 'nia', ',', '▁only', '▁this', '▁one', '▁had', '▁shadow', 's', '▁that', '▁made', '▁it', '▁look', '▁like', '▁an', '▁Egypt', 'ion', '▁Phar', 'a', 'oh', '.', '▁Very', '▁few', '▁days', '▁later', ',', '▁we', '▁reveal', 'ed', '▁the', '▁image', '▁for', '▁all', '▁to', '▁see', ',', '▁and', '▁we', '▁made', '▁sure', '▁to', '▁note', '▁that', '▁it', '▁was', '▁a', '▁huge', '▁rock', '▁formation', '▁that', '▁just', '▁rese', 'mble', 'd', '▁a', '▁human', '▁head', '▁and', '▁face', ',', '▁but', '▁all', '▁of', '▁it', '▁was', '▁for', 'med', '▁by', '▁shadow', 's', '.', '▁We', '▁only', '▁announced', '▁it', '▁because', '▁we', '▁thought', '▁it', '▁would', '▁be', '▁a', '▁good', '▁way', '▁to', '▁engage', '▁the', '▁public', '▁with', '▁NASA', \"'\", 's', '▁finding', 's', ',', '▁and', '▁at', 'rra', 'ct', '▁attention', '▁to', '▁Mars', '-', '-', '▁and', '▁it', '▁did', '.', '▁The', '▁face', '▁on', '▁Mars', '▁soon', '▁became', '▁a', '▁pop', '▁icon', ';', '▁shot', '▁in', '▁movies', ',', '▁appeared', '▁in', '▁books', ',', '▁magazine', 's', ',', '▁radio', '▁talk', '▁shows', ',', '▁and', '▁ha', 'un', 'ted', '▁gro', 'cer', 'y', '▁store', '▁check', 'out', '▁lines', '▁for', '▁25', '▁years', '.', '▁Some', '▁people', '▁thought', '▁the', '▁natural', '▁land', 'form', '▁was', '▁evidence', '▁of', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁us', '▁scientist', 's', '▁wanted', '▁to', '▁hi', 'de', '▁it', ',', '▁but', '▁really', ',', '▁the', '▁defender', 's', '▁of', '▁the', '▁NASA', '▁budget', '▁wish', '▁there', '▁was', '▁an', 'cient', '▁civiliza', 'tion', '▁on', '▁Mars', '.', '▁We', '▁decided', '▁to', '▁take', '▁another', '▁shot', '▁just', '▁to', '▁make', '▁sure', '▁we', '▁were', 'n', \"'\", 't', '▁wrong', ',', '▁on', '▁April', '▁5', ',', '▁1998.', '▁Michael', '▁Malin', '▁and', '▁his', '▁Mars', '▁Or', 'bit', 'er', '▁camera', '▁team', '▁took', '▁a', '▁picture', '▁that', '▁was', '▁ten', '▁times', '▁sharp', 'er', '▁than', '▁the', '▁original', '▁Viking', '▁photos', ',', '▁reveal', 'ing', '▁a', '▁natural', '▁land', 'form', ',', '▁which', '▁meant', '▁no', '▁alien', '▁monument', '.', '▁\"', 'But', '▁that', '▁picture', '▁wasn', \"'\", 't', '▁very', '▁clear', '▁at', '▁all', ',', '▁which', '▁could', '▁mean', '▁alien', '▁mark', 'ings', '▁were', '▁hidden', '▁by', '▁ha', 'ze', '\"', '▁Well', '▁no', ',', '▁yes', '▁that', '▁rumor', '▁started', ',', '▁but', '▁to', '▁prove', '▁them', '▁wrong', '▁on', '▁April', '▁8', ',', '▁2001', '▁we', '▁decided', '▁to', '▁take', '▁another', '▁picture', ',', '▁making', '▁sure', '▁it', '▁was', '▁a', '▁cloud', 'less', '▁summer', '▁day', '.', '▁Malin', \"'\", 's', '▁team', '▁capture', 'd', '▁an', '▁amazing', '▁photo', '▁using', '▁the', '▁camera', \"'\", 's', '▁absolute', '▁maximum', '▁revolution', '.', '▁With', '▁this', '▁camera', '▁you', '▁can', '▁discern', '▁things', '▁in', '▁a', '▁digital', '▁image', ',', '▁3', '▁times', '▁bigger', '▁than', '▁the', '▁pixel', '▁size', '▁which', '▁means', '▁if', '▁there', '▁were', '▁any', '▁sign', 's', '▁of', '▁life', ',', '▁you', '▁could', '▁easily', '▁see', '▁what', '▁they', '▁were', '.', '▁What', '▁the', '▁picture', '▁showed', '▁was', '▁the', '▁but', 'te', '▁or', '▁mesa', ',', '▁which', '▁are', '▁land', 'form', 's', '▁common', '▁around', '▁the', '▁American', '▁West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia= [ 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "82c78205-60aa-4051-807a-9d88844d7622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 0.14285714285714285, 'recall': 0.5, 'f1': 0.22222222222222224, 'number': 2}, 'ORG': {'precision': 0.07142857142857142, 'recall': 0.6666666666666666, 'f1': 0.12903225806451613, 'number': 3}, 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'overall_precision': 0.13513513513513514, 'overall_recall': 0.7142857142857143, 'overall_f1': 0.22727272727272727, 'overall_accuracy': 0.8985801217038539}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a419ca8-e000-4bf0-81fa-de8970e98a22",
   "metadata": {},
   "source": [
    "## 8 51la5/roberta-large-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "544a1391-53ea-41a9-ab4f-6dead3c6089c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 51la5/roberta-large-NER were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'I-ORG', 'score': 0.9999913, 'index': 8, 'word': '▁NASA', 'start': 16, 'end': 20}, {'entity': 'I-MISC', 'score': 0.9999641, 'index': 23, 'word': '▁Face', 'start': 88, 'end': 92}, {'entity': 'I-MISC', 'score': 0.99989665, 'index': 24, 'word': '▁On', 'start': 93, 'end': 95}, {'entity': 'I-MISC', 'score': 0.97350365, 'index': 25, 'word': '▁Mars', 'start': 96, 'end': 100}, {'entity': 'I-LOC', 'score': 0.9999362, 'index': 36, 'word': '▁Mars', 'start': 152, 'end': 156}, {'entity': 'I-MISC', 'score': 0.9992086, 'index': 58, 'word': '▁Viking', 'start': 240, 'end': 246}, {'entity': 'I-MISC', 'score': 0.9989502, 'index': 59, 'word': '▁1', 'start': 247, 'end': 248}, {'entity': 'I-MISC', 'score': 0.999977, 'index': 97, 'word': '▁Marti', 'start': 407, 'end': 412}, {'entity': 'I-MISC', 'score': 0.99619055, 'index': 98, 'word': 'an', 'start': 412, 'end': 414}, {'entity': 'I-LOC', 'score': 0.9999354, 'index': 103, 'word': '▁Cy', 'start': 435, 'end': 437}, {'entity': 'I-LOC', 'score': 0.99994576, 'index': 104, 'word': 'do', 'start': 437, 'end': 439}, {'entity': 'I-LOC', 'score': 0.99992585, 'index': 105, 'word': 'nia', 'start': 439, 'end': 442}, {'entity': 'I-MISC', 'score': 0.9999789, 'index': 119, 'word': '▁Egypt', 'start': 496, 'end': 501}, {'entity': 'I-MISC', 'score': 0.9614088, 'index': 120, 'word': 'ion', 'start': 501, 'end': 504}, {'entity': 'I-ORG', 'score': 0.99997246, 'index': 193, 'word': '▁NASA', 'start': 801, 'end': 805}, {'entity': 'I-LOC', 'score': 0.99979633, 'index': 205, 'word': '▁Mars', 'start': 843, 'end': 847}, {'entity': 'I-LOC', 'score': 0.9998061, 'index': 215, 'word': '▁Mars', 'start': 874, 'end': 878}, {'entity': 'I-LOC', 'score': 0.99984956, 'index': 264, 'word': '▁Mars', 'start': 1087, 'end': 1091}, {'entity': 'I-ORG', 'score': 0.99996305, 'index': 285, 'word': '▁NASA', 'start': 1168, 'end': 1172}, {'entity': 'I-LOC', 'score': 0.9998203, 'index': 295, 'word': '▁Mars', 'start': 1219, 'end': 1223}, {'entity': 'I-PER', 'score': 0.9999932, 'index': 319, 'word': '▁Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.99999106, 'index': 320, 'word': '▁Malin', 'start': 1319, 'end': 1324}, {'entity': 'I-MISC', 'score': 0.94105357, 'index': 323, 'word': '▁Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-MISC', 'score': 0.9839579, 'index': 324, 'word': '▁Or', 'start': 1338, 'end': 1340}, {'entity': 'I-MISC', 'score': 0.9913346, 'index': 325, 'word': 'bit', 'start': 1340, 'end': 1343}, {'entity': 'I-MISC', 'score': 0.9759228, 'index': 326, 'word': 'er', 'start': 1343, 'end': 1345}, {'entity': 'I-MISC', 'score': 0.999749, 'index': 341, 'word': '▁Viking', 'start': 1418, 'end': 1424}, {'entity': 'I-PER', 'score': 0.9999914, 'index': 416, 'word': '▁Malin', 'start': 1744, 'end': 1749}, {'entity': 'I-MISC', 'score': 0.92417294, 'index': 491, 'word': '▁American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.99954396, 'index': 492, 'word': '▁West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'I-LOC', 4: 'I-MISC', 5: 'I-ORG', 6: 'I-PER', 7: 'O'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"51la5/roberta-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"51la5/roberta-large-NER\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "81460395-d28f-4f8c-802b-56db46014171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-LOC', 'O']\n",
      "['▁So', ',', '▁if', '▁you', \"'\", 're', '▁a', '▁NASA', '▁scientist', ',', '▁you', '▁should', '▁be', '▁able', '▁to', '▁tell', '▁me', '▁the', '▁whole', '▁story', '▁about', '▁the', '▁Face', '▁On', '▁Mars', ',', '▁which', '▁obviously', '▁is', '▁evidence', '▁that', '▁there', '▁is', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁the', '▁face', '▁was', '▁created', '▁by', '▁alien', 's', ',', '▁correct', '?\"', '▁No', ',', '▁twenty', '▁five', '▁years', '▁ago', ',', '▁our', '▁Viking', '▁1', '▁space', 'craft', '▁was', '▁circ', 'ling', '▁the', '▁planet', ',', '▁sna', 'pping', '▁photos', ',', '▁when', '▁it', '▁spot', 'ted', '▁the', '▁shadow', 'y', '▁like', 'ness', '▁of', '▁a', '▁human', '▁face', '.', '▁Us', '▁scientist', 's', '▁figure', 'd', '▁out', '▁that', '▁it', '▁was', '▁just', '▁another', '▁Marti', 'an', '▁mesa', ',', '▁common', '▁around', '▁Cy', 'do', 'nia', ',', '▁only', '▁this', '▁one', '▁had', '▁shadow', 's', '▁that', '▁made', '▁it', '▁look', '▁like', '▁an', '▁Egypt', 'ion', '▁Phar', 'a', 'oh', '.', '▁Very', '▁few', '▁days', '▁later', ',', '▁we', '▁reveal', 'ed', '▁the', '▁image', '▁for', '▁all', '▁to', '▁see', ',', '▁and', '▁we', '▁made', '▁sure', '▁to', '▁note', '▁that', '▁it', '▁was', '▁a', '▁huge', '▁rock', '▁formation', '▁that', '▁just', '▁rese', 'mble', 'd', '▁a', '▁human', '▁head', '▁and', '▁face', ',', '▁but', '▁all', '▁of', '▁it', '▁was', '▁for', 'med', '▁by', '▁shadow', 's', '.', '▁We', '▁only', '▁announced', '▁it', '▁because', '▁we', '▁thought', '▁it', '▁would', '▁be', '▁a', '▁good', '▁way', '▁to', '▁engage', '▁the', '▁public', '▁with', '▁NASA', \"'\", 's', '▁finding', 's', ',', '▁and', '▁at', 'rra', 'ct', '▁attention', '▁to', '▁Mars', '-', '-', '▁and', '▁it', '▁did', '.', '▁The', '▁face', '▁on', '▁Mars', '▁soon', '▁became', '▁a', '▁pop', '▁icon', ';', '▁shot', '▁in', '▁movies', ',', '▁appeared', '▁in', '▁books', ',', '▁magazine', 's', ',', '▁radio', '▁talk', '▁shows', ',', '▁and', '▁ha', 'un', 'ted', '▁gro', 'cer', 'y', '▁store', '▁check', 'out', '▁lines', '▁for', '▁25', '▁years', '.', '▁Some', '▁people', '▁thought', '▁the', '▁natural', '▁land', 'form', '▁was', '▁evidence', '▁of', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁us', '▁scientist', 's', '▁wanted', '▁to', '▁hi', 'de', '▁it', ',', '▁but', '▁really', ',', '▁the', '▁defender', 's', '▁of', '▁the', '▁NASA', '▁budget', '▁wish', '▁there', '▁was', '▁an', 'cient', '▁civiliza', 'tion', '▁on', '▁Mars', '.', '▁We', '▁decided', '▁to', '▁take', '▁another', '▁shot', '▁just', '▁to', '▁make', '▁sure', '▁we', '▁were', 'n', \"'\", 't', '▁wrong', ',', '▁on', '▁April', '▁5', ',', '▁1998.', '▁Michael', '▁Malin', '▁and', '▁his', '▁Mars', '▁Or', 'bit', 'er', '▁camera', '▁team', '▁took', '▁a', '▁picture', '▁that', '▁was', '▁ten', '▁times', '▁sharp', 'er', '▁than', '▁the', '▁original', '▁Viking', '▁photos', ',', '▁reveal', 'ing', '▁a', '▁natural', '▁land', 'form', ',', '▁which', '▁meant', '▁no', '▁alien', '▁monument', '.', '▁\"', 'But', '▁that', '▁picture', '▁wasn', \"'\", 't', '▁very', '▁clear', '▁at', '▁all', ',', '▁which', '▁could', '▁mean', '▁alien', '▁mark', 'ings', '▁were', '▁hidden', '▁by', '▁ha', 'ze', '\"', '▁Well', '▁no', ',', '▁yes', '▁that', '▁rumor', '▁started', ',', '▁but', '▁to', '▁prove', '▁them', '▁wrong', '▁on', '▁April', '▁8', ',', '▁2001', '▁we', '▁decided', '▁to', '▁take', '▁another', '▁picture', ',', '▁making', '▁sure', '▁it', '▁was', '▁a', '▁cloud', 'less', '▁summer', '▁day', '.', '▁Malin', \"'\", 's', '▁team', '▁capture', 'd', '▁an', '▁amazing', '▁photo', '▁using', '▁the', '▁camera', \"'\", 's', '▁absolute', '▁maximum', '▁revolution', '.', '▁With', '▁this', '▁camera', '▁you', '▁can', '▁discern', '▁things', '▁in', '▁a', '▁digital', '▁image', ',', '▁3', '▁times', '▁bigger', '▁than', '▁the', '▁pixel', '▁size', '▁which', '▁means', '▁if', '▁there', '▁were', '▁any', '▁sign', 's', '▁of', '▁life', ',', '▁you', '▁could', '▁easily', '▁see', '▁what', '▁they', '▁were', '.', '▁What', '▁the', '▁picture', '▁showed', '▁was', '▁the', '▁but', 'te', '▁or', '▁mesa', ',', '▁which', '▁are', '▁land', 'form', 's', '▁common', '▁around', '▁the', '▁American', '▁West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[ 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'I-LOC',\n",
    " 'O'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "637d5e3f-ff89-4c50-9cb3-c16421202abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'MISC': {'precision': 0.6153846153846154, 'recall': 0.7272727272727273, 'f1': 0.6666666666666667, 'number': 11}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3}, 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'overall_precision': 0.75, 'overall_recall': 0.8333333333333334, 'overall_f1': 0.7894736842105262, 'overall_accuracy': 0.9837728194726166}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a46082-0ffa-413c-ae69-4ffbf2081e71",
   "metadata": {},
   "source": [
    "## 9 dmargutierrezdistilbert-base-multilingual-cased-mapa_coarse-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "2483125d-bca3-41f4-94ff-e06cac64f34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'B-ADDRESS', 'score': 0.89953065, 'index': 104, 'word': 'C', 'start': 435, 'end': 436}, {'entity': 'I-ADDRESS', 'score': 0.776557, 'index': 105, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'I-ADDRESS', 'score': 0.87639356, 'index': 106, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'B-AMOUNT', 'score': 0.94337094, 'index': 248, 'word': '25', 'start': 1012, 'end': 1014}, {'entity': 'I-AMOUNT', 'score': 0.83502764, 'index': 249, 'word': 'years', 'start': 1015, 'end': 1020}, {'entity': 'B-DATE', 'score': 0.993107, 'index': 311, 'word': 'April', 'start': 1296, 'end': 1301}, {'entity': 'I-DATE', 'score': 0.9911287, 'index': 312, 'word': '5', 'start': 1302, 'end': 1303}, {'entity': 'I-DATE', 'score': 0.9842742, 'index': 313, 'word': ',', 'start': 1303, 'end': 1304}, {'entity': 'I-DATE', 'score': 0.9907127, 'index': 314, 'word': '1998', 'start': 1305, 'end': 1309}, {'entity': 'B-PERSON', 'score': 0.9272426, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PERSON', 'score': 0.9829417, 'index': 317, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'I-PERSON', 'score': 0.96556324, 'index': 318, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'B-ORGANISATION', 'score': 0.92684674, 'index': 321, 'word': 'Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-ORGANISATION', 'score': 0.9111312, 'index': 322, 'word': 'Or', 'start': 1338, 'end': 1340}, {'entity': 'I-ORGANISATION', 'score': 0.8044608, 'index': 323, 'word': '##biter', 'start': 1340, 'end': 1345}, {'entity': 'B-DATE', 'score': 0.9925527, 'index': 394, 'word': 'April', 'start': 1651, 'end': 1656}, {'entity': 'I-DATE', 'score': 0.989077, 'index': 395, 'word': '8', 'start': 1657, 'end': 1658}, {'entity': 'I-DATE', 'score': 0.9804143, 'index': 396, 'word': ',', 'start': 1658, 'end': 1659}, {'entity': 'I-DATE', 'score': 0.989312, 'index': 397, 'word': '2001', 'start': 1660, 'end': 1664}, {'entity': 'B-PERSON', 'score': 0.6016297, 'index': 415, 'word': 'Mali', 'start': 1744, 'end': 1748}, {'entity': 'I-PERSON', 'score': 0.7576901, 'index': 416, 'word': '##n', 'start': 1748, 'end': 1749}, {'entity': 'B-ADDRESS', 'score': 0.48142406, 'index': 492, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'I-ADDRESS', 'score': 0.456201, 'index': 493, 'word': 'West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-ORGANISATION', 2: 'I-ORGANISATION', 3: 'B-ADDRESS', 4: 'I-ADDRESS', 5: 'B-DATE', 6: 'I-DATE', 7: 'B-PERSON', 8: 'I-PERSON', 9: 'B-AMOUNT', 10: 'I-AMOUNT', 11: 'B-TIME', 12: 'I-TIME'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmargutierrez/distilbert-base-multilingual-cased-mapa_coarse-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dmargutierrez/distilbert-base-multilingual-cased-mapa_coarse-ner\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "5cbf0050-3211-4aae-a39c-2a4c16746915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANISATION', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANISATION', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'I-AMOUNT', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'B-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANISATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANISATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'I-DATE', 'O', 'I-DATE', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AMOUNT', 'I-AMOUNT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANISATION', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANISATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PERSON', 'O', 'O', 'O', 'I-DATE', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-PERSON', 'B-PERSON', 'I-PERSON', 'O', 'O', 'B-ORGANISATION', 'I-ORGANISATION', 'I-ORGANISATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADDRESS', 'I-ADDRESS', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[\n",
    "     'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORGANISATION',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ADDRESS',\n",
    " 'I-ADDRESS',\n",
    " 'I-ADDRESS',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORGANISATION',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-AMOUNT',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORGANISATION',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-DATE',\n",
    " 'I-DATE',\n",
    " 'I-DATE',\n",
    " 'I-DATE',\n",
    " 'O',\n",
    " 'B-PERSON',\n",
    " 'B-PERSON',\n",
    " 'I-PERSON',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-DATE',\n",
    " 'I-DATE',\n",
    " 'I-DATE',\n",
    " 'I-DATE',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PERSON',\n",
    " 'I-PERSON',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ADDRESS',\n",
    " 'I-ADDRESS',\n",
    " 'O'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "1c13159a-9cec-4ac6-b34d-29435aa11ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADDRESS': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'AMOUNT': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'DATE': {'precision': 0.058823529411764705, 'recall': 1.0, 'f1': 0.1111111111111111, 'number': 2}, 'ORGANISATION': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'PERSON': {'precision': 0.75, 'recall': 1.0, 'f1': 0.8571428571428571, 'number': 3}, 'overall_precision': 0.14285714285714285, 'overall_recall': 0.6363636363636364, 'overall_f1': 0.23333333333333328, 'overall_accuracy': 0.9048582995951417}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036df0d0-03d9-4be3-bd5a-76c1cbce8ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d464a1a6-7472-4961-907c-a58292fb42b2",
   "metadata": {},
   "source": [
    "## 10 mbrutonspa_enpt_mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "1dbdba8b-4c47-451c-ab80-a1a0a3f59b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'r0:arg1|tem', 'score': 0.8560656, 'index': 4, 'word': 'you', 'start': 7, 'end': 10}, {'entity': 'r0:root', 'score': 0.99593973, 'index': 6, 'word': 're', 'start': 11, 'end': 13}, {'entity': 'r1:arg1|tem', 'score': 0.7426193, 'index': 11, 'word': 'you', 'start': 32, 'end': 35}, {'entity': 'r1:root', 'score': 0.94838774, 'index': 13, 'word': 'be', 'start': 43, 'end': 45}, {'entity': 'r1:arg2|atr', 'score': 0.64391094, 'index': 14, 'word': 'able', 'start': 46, 'end': 50}, {'entity': 'r2:root', 'score': 0.80442923, 'index': 16, 'word': 'tell', 'start': 54, 'end': 58}, {'entity': 'r1:arg1|pat', 'score': 0.33438408, 'index': 20, 'word': 'story', 'start': 72, 'end': 77}, {'entity': 'r4:arg1|tem', 'score': 0.34810358, 'index': 27, 'word': 'which', 'start': 102, 'end': 107}, {'entity': 'r5:root', 'score': 0.5292147, 'index': 30, 'word': 'is', 'start': 118, 'end': 120}, {'entity': 'r4:arg2|atr', 'score': 0.46503437, 'index': 31, 'word': 'evidence', 'start': 121, 'end': 129}, {'entity': 'r6:arg2|atr', 'score': 0.07937594, 'index': 33, 'word': 'there', 'start': 135, 'end': 140}, {'entity': 'r6:root', 'score': 0.55574423, 'index': 34, 'word': 'is', 'start': 141, 'end': 143}, {'entity': 'r5:arg1|tem', 'score': 0.38110724, 'index': 35, 'word': 'life', 'start': 144, 'end': 148}, {'entity': 'r7:arg1|pat', 'score': 0.16291595, 'index': 42, 'word': 'face', 'start': 171, 'end': 175}, {'entity': 'r7:root', 'score': 0.6100769, 'index': 44, 'word': 'created', 'start': 180, 'end': 187}, {'entity': 'r7:arg0|agt', 'score': 0.15812016, 'index': 45, 'word': 'by', 'start': 188, 'end': 190}, {'entity': 'r6:arg0|agt', 'score': 0.18646379, 'index': 62, 'word': 'spacecraft', 'start': 249, 'end': 259}, {'entity': 'r7:root', 'score': 0.6611128, 'index': 64, 'word': 'ci', 'start': 264, 'end': 266}, {'entity': 'r7:root', 'score': 0.5775215, 'index': 65, 'word': '##rc', 'start': 266, 'end': 268}, {'entity': 'r7:root', 'score': 0.35779056, 'index': 66, 'word': '##ling', 'start': 268, 'end': 272}, {'entity': 'r7:arg1|pat', 'score': 0.39385104, 'index': 68, 'word': 'planet', 'start': 277, 'end': 283}, {'entity': 'r7:root', 'score': 0.42999077, 'index': 70, 'word': 'sna', 'start': 285, 'end': 288}, {'entity': 'r8:root', 'score': 0.26388708, 'index': 71, 'word': '##pping', 'start': 288, 'end': 293}, {'entity': 'r7:arg1|pat', 'score': 0.2658881, 'index': 72, 'word': 'photos', 'start': 294, 'end': 300}, {'entity': 'r8:arg0|agt', 'score': 0.19982801, 'index': 75, 'word': 'it', 'start': 307, 'end': 309}, {'entity': 'r8:root', 'score': 0.3022762, 'index': 76, 'word': 'spotted', 'start': 310, 'end': 317}, {'entity': 'r8:arg1|pat', 'score': 0.08930407, 'index': 81, 'word': 'like', 'start': 330, 'end': 334}, {'entity': 'r8:arg1|pat', 'score': 0.09168834, 'index': 82, 'word': '##ness', 'start': 334, 'end': 338}, {'entity': 'r8:arg0|agt', 'score': 0.054486226, 'index': 88, 'word': 'Us', 'start': 356, 'end': 358}, {'entity': 'r8:arg0|agt', 'score': 0.10709534, 'index': 89, 'word': 'scientists', 'start': 359, 'end': 369}, {'entity': 'r9:root', 'score': 0.18592234, 'index': 90, 'word': 'figure', 'start': 370, 'end': 376}, {'entity': 'r9:root', 'score': 0.15209025, 'index': 91, 'word': '##d', 'start': 376, 'end': 377}, {'entity': 'r8:arg1|tem', 'score': 0.045862645, 'index': 94, 'word': 'it', 'start': 387, 'end': 389}, {'entity': 'r9:root', 'score': 0.115167715, 'index': 95, 'word': 'was', 'start': 390, 'end': 393}, {'entity': 'r9:root', 'score': 0.04374532, 'index': 102, 'word': 'common', 'start': 421, 'end': 427}, {'entity': 'r8:arg1|tem', 'score': 0.040294692, 'index': 110, 'word': 'one', 'start': 454, 'end': 457}, {'entity': 'r9:root', 'score': 0.1052284, 'index': 111, 'word': 'had', 'start': 458, 'end': 461}, {'entity': 'r8:arg2|atr', 'score': 0.04526653, 'index': 112, 'word': 'sh', 'start': 462, 'end': 464}, {'entity': 'r8:arg2|atr', 'score': 0.050974954, 'index': 113, 'word': '##adow', 'start': 464, 'end': 468}, {'entity': 'r8:arg0|agt', 'score': 0.04716369, 'index': 115, 'word': 'that', 'start': 470, 'end': 474}, {'entity': 'r9:root', 'score': 0.12286918, 'index': 116, 'word': 'made', 'start': 475, 'end': 479}, {'entity': 'r9:root', 'score': 0.034002524, 'index': 117, 'word': 'it', 'start': 480, 'end': 482}, {'entity': 'r9:root', 'score': 0.085215025, 'index': 118, 'word': 'look', 'start': 483, 'end': 487}, {'entity': 'r9:root', 'score': 0.04240514, 'index': 119, 'word': 'like', 'start': 488, 'end': 492}, {'entity': 'r9:root', 'score': 0.04686627, 'index': 129, 'word': 'days', 'start': 523, 'end': 527}, {'entity': 'r9:root', 'score': 0.040811997, 'index': 130, 'word': 'later', 'start': 528, 'end': 533}, {'entity': 'r10:root', 'score': 0.040206842, 'index': 132, 'word': 'we', 'start': 535, 'end': 537}, {'entity': 'r9:root', 'score': 0.080649935, 'index': 133, 'word': 'revealed', 'start': 538, 'end': 546}, {'entity': 'r9:root', 'score': 0.042861167, 'index': 135, 'word': 'image', 'start': 551, 'end': 556}, {'entity': 'r9:root', 'score': 0.042270634, 'index': 136, 'word': 'for', 'start': 557, 'end': 560}, {'entity': 'r9:root', 'score': 0.08127655, 'index': 139, 'word': 'see', 'start': 568, 'end': 571}, {'entity': 'r9:root', 'score': 0.044283886, 'index': 142, 'word': 'we', 'start': 577, 'end': 579}, {'entity': 'r9:root', 'score': 0.08544878, 'index': 143, 'word': 'made', 'start': 580, 'end': 584}, {'entity': 'r9:root', 'score': 0.040639073, 'index': 144, 'word': 'sure', 'start': 585, 'end': 589}, {'entity': 'r9:root', 'score': 0.0474719, 'index': 148, 'word': 'it', 'start': 603, 'end': 605}, {'entity': 'r9:root', 'score': 0.078273796, 'index': 149, 'word': 'was', 'start': 606, 'end': 609}, {'entity': 'r9:root', 'score': 0.060446404, 'index': 153, 'word': 'formation', 'start': 622, 'end': 631}, {'entity': 'r9:arg1|pat', 'score': 0.030689918, 'index': 154, 'word': 'that', 'start': 632, 'end': 636}, {'entity': 'r9:root', 'score': 0.08887379, 'index': 156, 'word': 'res', 'start': 642, 'end': 645}, {'entity': 'r9:root', 'score': 0.07661998, 'index': 157, 'word': '##emble', 'start': 645, 'end': 650}, {'entity': 'r9:root', 'score': 0.0793471, 'index': 158, 'word': '##d', 'start': 650, 'end': 651}, {'entity': 'r8:arg2|atr', 'score': 0.034773763, 'index': 161, 'word': 'head', 'start': 660, 'end': 664}, {'entity': 'r9:arg1|pat', 'score': 0.034561444, 'index': 166, 'word': 'all', 'start': 679, 'end': 682}, {'entity': 'r9:arg1|pat', 'score': 0.038585175, 'index': 168, 'word': 'it', 'start': 686, 'end': 688}, {'entity': 'r9:root', 'score': 0.087780945, 'index': 169, 'word': 'was', 'start': 689, 'end': 692}, {'entity': 'r9:root', 'score': 0.0999068, 'index': 170, 'word': 'formed', 'start': 693, 'end': 699}, {'entity': 'r8:arg2|atr', 'score': 0.027370188, 'index': 171, 'word': 'by', 'start': 700, 'end': 702}, {'entity': 'r8:arg0|agt', 'score': 0.078722976, 'index': 176, 'word': 'We', 'start': 712, 'end': 714}, {'entity': 'r9:root', 'score': 0.12920254, 'index': 178, 'word': 'announced', 'start': 720, 'end': 729}, {'entity': 'r8:arg1|pat', 'score': 0.07986989, 'index': 179, 'word': 'it', 'start': 730, 'end': 732}, {'entity': 'r8:arg0|agt', 'score': 0.048153095, 'index': 181, 'word': 'we', 'start': 741, 'end': 743}, {'entity': 'r9:root', 'score': 0.10950655, 'index': 182, 'word': 'thought', 'start': 744, 'end': 751}, {'entity': 'r8:arg1|tem', 'score': 0.041486118, 'index': 183, 'word': 'it', 'start': 752, 'end': 754}, {'entity': 'r9:root', 'score': 0.05073052, 'index': 184, 'word': 'would', 'start': 755, 'end': 760}, {'entity': 'r9:root', 'score': 0.10624526, 'index': 185, 'word': 'be', 'start': 761, 'end': 763}, {'entity': 'r8:arg2|atr', 'score': 0.054641366, 'index': 188, 'word': 'way', 'start': 771, 'end': 774}, {'entity': 'r9:root', 'score': 0.10240572, 'index': 190, 'word': 'engage', 'start': 778, 'end': 784}, {'entity': 'r9:arg1|pat', 'score': 0.044021852, 'index': 192, 'word': 'public', 'start': 789, 'end': 795}, {'entity': 'r9:root', 'score': 0.090464294, 'index': 200, 'word': 'at', 'start': 822, 'end': 824}, {'entity': 'r9:root', 'score': 0.09293412, 'index': 201, 'word': '##rra', 'start': 824, 'end': 827}, {'entity': 'r9:root', 'score': 0.09475694, 'index': 202, 'word': '##ct', 'start': 827, 'end': 829}, {'entity': 'r9:arg1|pat', 'score': 0.056167223, 'index': 203, 'word': 'attention', 'start': 830, 'end': 839}, {'entity': 'r8:arg2|atr', 'score': 0.03035166, 'index': 204, 'word': 'to', 'start': 840, 'end': 842}, {'entity': 'r9:arg1|pat', 'score': 0.032432236, 'index': 209, 'word': 'it', 'start': 854, 'end': 856}, {'entity': 'r9:root', 'score': 0.083309464, 'index': 210, 'word': 'did', 'start': 857, 'end': 860}, {'entity': 'r8:arg1|tem', 'score': 0.038414363, 'index': 213, 'word': 'face', 'start': 866, 'end': 870}, {'entity': 'r8:arg2|atr', 'score': 0.02903995, 'index': 216, 'word': 'soon', 'start': 879, 'end': 883}, {'entity': 'r9:root', 'score': 0.07692599, 'index': 217, 'word': 'became', 'start': 884, 'end': 890}, {'entity': 'r8:arg2|atr', 'score': 0.051249504, 'index': 219, 'word': 'pop', 'start': 893, 'end': 896}, {'entity': 'r8:arg2|atr', 'score': 0.04843377, 'index': 220, 'word': 'i', 'start': 897, 'end': 898}, {'entity': 'r8:arg2|atr', 'score': 0.04545699, 'index': 221, 'word': '##con', 'start': 898, 'end': 901}, {'entity': 'r9:root', 'score': 0.08494211, 'index': 223, 'word': 'shot', 'start': 903, 'end': 907}, {'entity': 'r8:arg2|atr', 'score': 0.03157478, 'index': 224, 'word': 'in', 'start': 908, 'end': 910}, {'entity': 'r9:root', 'score': 0.1036089, 'index': 227, 'word': 'appeared', 'start': 919, 'end': 927}, {'entity': 'r8:arg2|atr', 'score': 0.03158019, 'index': 228, 'word': 'in', 'start': 928, 'end': 930}, {'entity': 'r8:argM|tmp', 'score': 0.091016516, 'index': 247, 'word': 'for', 'start': 1008, 'end': 1011}, {'entity': 'r8:arg1|tem', 'score': 0.06676676, 'index': 252, 'word': 'people', 'start': 1027, 'end': 1033}, {'entity': 'r8:root', 'score': 0.20488475, 'index': 253, 'word': 'thought', 'start': 1034, 'end': 1041}, {'entity': 'r7:arg1|tem', 'score': 0.36603826, 'index': 256, 'word': 'land', 'start': 1054, 'end': 1058}, {'entity': 'r7:arg1|tem', 'score': 0.31067976, 'index': 257, 'word': '##form', 'start': 1058, 'end': 1062}, {'entity': 'r7:root', 'score': 0.48778686, 'index': 258, 'word': 'was', 'start': 1063, 'end': 1066}, {'entity': 'r7:arg2|atr', 'score': 0.13593948, 'index': 259, 'word': 'evidence', 'start': 1067, 'end': 1075}, {'entity': 'r6:arg0|agt', 'score': 0.21349978, 'index': 268, 'word': 'scientists', 'start': 1105, 'end': 1115}, {'entity': 'r7:root', 'score': 0.48215854, 'index': 269, 'word': 'wanted', 'start': 1116, 'end': 1122}, {'entity': 'r7:root', 'score': 0.66229856, 'index': 271, 'word': 'hide', 'start': 1126, 'end': 1130}, {'entity': 'r7:arg1|pat', 'score': 0.34266004, 'index': 272, 'word': 'it', 'start': 1131, 'end': 1133}, {'entity': 'r6:arg0|agt', 'score': 0.27256963, 'index': 278, 'word': 'defender', 'start': 1151, 'end': 1159}, {'entity': 'r5:root', 'score': 0.4016244, 'index': 286, 'word': 'was', 'start': 1191, 'end': 1194}, {'entity': 'r6:arg0|agt', 'score': 0.3085603, 'index': 293, 'word': 'We', 'start': 1225, 'end': 1227}, {'entity': 'r7:root', 'score': 0.68402517, 'index': 294, 'word': 'decided', 'start': 1228, 'end': 1235}, {'entity': 'r7:root', 'score': 0.6312899, 'index': 296, 'word': 'take', 'start': 1239, 'end': 1243}, {'entity': 'r7:arg1|pat', 'score': 0.2447523, 'index': 298, 'word': 'shot', 'start': 1252, 'end': 1256}, {'entity': 'r5:argM|fin', 'score': 0.21691667, 'index': 300, 'word': 'to', 'start': 1262, 'end': 1264}, {'entity': 'r8:arg1|tem', 'score': 0.0507591, 'index': 303, 'word': 'we', 'start': 1275, 'end': 1277}, {'entity': 'r9:root', 'score': 0.110183075, 'index': 304, 'word': 'were', 'start': 1278, 'end': 1282}, {'entity': 'r9:root', 'score': 0.0827311, 'index': 305, 'word': '##n', 'start': 1282, 'end': 1283}, {'entity': 'r8:arg2|atr', 'score': 0.07966802, 'index': 308, 'word': 'wrong', 'start': 1286, 'end': 1291}, {'entity': 'r8:argM|tmp', 'score': 0.047043335, 'index': 310, 'word': 'on', 'start': 1293, 'end': 1295}, {'entity': 'r8:arg0|agt', 'score': 0.037761096, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'r9:root', 'score': 0.11788476, 'index': 326, 'word': 'took', 'start': 1358, 'end': 1362}, {'entity': 'r8:arg1|pat', 'score': 0.061609197, 'index': 328, 'word': 'picture', 'start': 1365, 'end': 1372}, {'entity': 'r8:arg1|tem', 'score': 0.04639362, 'index': 329, 'word': 'that', 'start': 1373, 'end': 1377}, {'entity': 'r9:root', 'score': 0.10497463, 'index': 330, 'word': 'was', 'start': 1378, 'end': 1381}, {'entity': 'r9:root', 'score': 0.037833076, 'index': 332, 'word': 'times', 'start': 1386, 'end': 1391}, {'entity': 'r9:root', 'score': 0.04543908, 'index': 333, 'word': 'sharp', 'start': 1392, 'end': 1397}, {'entity': 'r9:root', 'score': 0.043668192, 'index': 334, 'word': '##er', 'start': 1397, 'end': 1399}, {'entity': 'r9:root', 'score': 0.06038779, 'index': 339, 'word': 'photos', 'start': 1425, 'end': 1431}, {'entity': 'r9:root', 'score': 0.08361747, 'index': 341, 'word': 'reveal', 'start': 1433, 'end': 1439}, {'entity': 'r9:root', 'score': 0.0819967, 'index': 342, 'word': '##ing', 'start': 1439, 'end': 1442}, {'entity': 'r9:root', 'score': 0.050429754, 'index': 345, 'word': 'land', 'start': 1453, 'end': 1457}, {'entity': 'r9:root', 'score': 0.055836514, 'index': 346, 'word': '##form', 'start': 1457, 'end': 1461}, {'entity': 'r10:root', 'score': 0.030725654, 'index': 348, 'word': 'which', 'start': 1463, 'end': 1468}, {'entity': 'r9:root', 'score': 0.07811248, 'index': 349, 'word': 'meant', 'start': 1469, 'end': 1474}, {'entity': 'r9:root', 'score': 0.056091927, 'index': 352, 'word': 'monument', 'start': 1484, 'end': 1492}, {'entity': 'r10:root', 'score': 0.037297357, 'index': 357, 'word': 'picture', 'start': 1504, 'end': 1511}, {'entity': 'r9:root', 'score': 0.08379, 'index': 358, 'word': 'wasn', 'start': 1512, 'end': 1516}, {'entity': 'r9:root', 'score': 0.046028677, 'index': 362, 'word': 'clear', 'start': 1524, 'end': 1529}, {'entity': 'r9:root', 'score': 0.030750155, 'index': 363, 'word': 'at', 'start': 1530, 'end': 1532}, {'entity': 'r8:arg1|tem', 'score': 0.029666642, 'index': 366, 'word': 'which', 'start': 1538, 'end': 1543}, {'entity': 'r9:root', 'score': 0.07766737, 'index': 368, 'word': 'mean', 'start': 1550, 'end': 1554}, {'entity': 'r9:arg1|pat', 'score': 0.037586644, 'index': 370, 'word': 'marking', 'start': 1561, 'end': 1568}, {'entity': 'r9:arg1|pat', 'score': 0.039157256, 'index': 371, 'word': '##s', 'start': 1568, 'end': 1569}, {'entity': 'r9:root', 'score': 0.06878179, 'index': 372, 'word': 'were', 'start': 1570, 'end': 1574}, {'entity': 'r10:root', 'score': 0.06736001, 'index': 373, 'word': 'hidden', 'start': 1575, 'end': 1581}, {'entity': 'r8:arg2|atr', 'score': 0.030598668, 'index': 374, 'word': 'by', 'start': 1582, 'end': 1584}, {'entity': 'r9:root', 'score': 0.04365823, 'index': 381, 'word': 'ye', 'start': 1600, 'end': 1602}, {'entity': 'r9:root', 'score': 0.04754055, 'index': 382, 'word': '##s', 'start': 1602, 'end': 1603}, {'entity': 'r9:root', 'score': 0.04762847, 'index': 384, 'word': 'rum', 'start': 1609, 'end': 1612}, {'entity': 'r9:root', 'score': 0.038992863, 'index': 385, 'word': '##or', 'start': 1612, 'end': 1614}, {'entity': 'r9:root', 'score': 0.06939269, 'index': 386, 'word': 'started', 'start': 1615, 'end': 1622}, {'entity': 'r9:root', 'score': 0.08153154, 'index': 390, 'word': 'prove', 'start': 1631, 'end': 1636}, {'entity': 'r9:arg1|pat', 'score': 0.042257246, 'index': 391, 'word': 'them', 'start': 1637, 'end': 1641}, {'entity': 'r8:arg2|atr', 'score': 0.04214874, 'index': 392, 'word': 'wrong', 'start': 1642, 'end': 1647}, {'entity': 'r8:argM|tmp', 'score': 0.036950577, 'index': 393, 'word': 'on', 'start': 1648, 'end': 1650}, {'entity': 'r8:arg0|agt', 'score': 0.033291806, 'index': 398, 'word': 'we', 'start': 1665, 'end': 1667}, {'entity': 'r9:root', 'score': 0.0738949, 'index': 399, 'word': 'decided', 'start': 1668, 'end': 1675}, {'entity': 'r9:root', 'score': 0.08140796, 'index': 401, 'word': 'take', 'start': 1679, 'end': 1683}, {'entity': 'r9:arg1|pat', 'score': 0.04987211, 'index': 403, 'word': 'picture', 'start': 1692, 'end': 1699}, {'entity': 'r9:root', 'score': 0.06430703, 'index': 405, 'word': 'making', 'start': 1701, 'end': 1707}, {'entity': 'r8:arg2|atr', 'score': 0.038376, 'index': 406, 'word': 'sure', 'start': 1708, 'end': 1712}, {'entity': 'r9:arg1|pat', 'score': 0.03541435, 'index': 407, 'word': 'it', 'start': 1713, 'end': 1715}, {'entity': 'r9:root', 'score': 0.074262, 'index': 408, 'word': 'was', 'start': 1716, 'end': 1719}, {'entity': 'r8:arg2|atr', 'score': 0.044221986, 'index': 410, 'word': 'cloud', 'start': 1722, 'end': 1727}, {'entity': 'r8:arg2|atr', 'score': 0.039963495, 'index': 413, 'word': 'day', 'start': 1739, 'end': 1742}, {'entity': 'r9:root', 'score': 0.03479836, 'index': 415, 'word': 'Mali', 'start': 1744, 'end': 1748}, {'entity': 'r9:root', 'score': 0.032437824, 'index': 419, 'word': 'team', 'start': 1752, 'end': 1756}, {'entity': 'r9:root', 'score': 0.08305774, 'index': 420, 'word': 'captured', 'start': 1757, 'end': 1765}, {'entity': 'r9:arg1|pat', 'score': 0.04045077, 'index': 424, 'word': 'photo', 'start': 1777, 'end': 1782}, {'entity': 'r9:root', 'score': 0.066850476, 'index': 425, 'word': 'using', 'start': 1783, 'end': 1788}, {'entity': 'r9:root', 'score': 0.03912363, 'index': 427, 'word': 'camera', 'start': 1793, 'end': 1799}, {'entity': 'r9:root', 'score': 0.0366046, 'index': 434, 'word': 'With', 'start': 1831, 'end': 1835}, {'entity': 'r9:root', 'score': 0.034650415, 'index': 437, 'word': 'you', 'start': 1848, 'end': 1851}, {'entity': 'r9:root', 'score': 0.07881917, 'index': 439, 'word': 'disc', 'start': 1856, 'end': 1860}, {'entity': 'r9:root', 'score': 0.08386986, 'index': 440, 'word': '##ern', 'start': 1860, 'end': 1863}, {'entity': 'r9:root', 'score': 0.035086773, 'index': 441, 'word': 'things', 'start': 1864, 'end': 1870}, {'entity': 'r10:root', 'score': 0.033257537, 'index': 442, 'word': 'in', 'start': 1871, 'end': 1873}, {'entity': 'r9:root', 'score': 0.035879184, 'index': 455, 'word': 'which', 'start': 1926, 'end': 1931}, {'entity': 'r9:root', 'score': 0.0820557, 'index': 456, 'word': 'means', 'start': 1932, 'end': 1937}, {'entity': 'r9:root', 'score': 0.045568023, 'index': 458, 'word': 'there', 'start': 1941, 'end': 1946}, {'entity': 'r9:root', 'score': 0.07430672, 'index': 459, 'word': 'were', 'start': 1947, 'end': 1951}, {'entity': 'r9:root', 'score': 0.048614994, 'index': 461, 'word': 'signs', 'start': 1956, 'end': 1961}, {'entity': 'r9:root', 'score': 0.040848035, 'index': 465, 'word': 'you', 'start': 1971, 'end': 1974}, {'entity': 'r9:root', 'score': 0.06783971, 'index': 466, 'word': 'could', 'start': 1975, 'end': 1980}, {'entity': 'r9:root', 'score': 0.043203566, 'index': 467, 'word': 'easily', 'start': 1981, 'end': 1987}, {'entity': 'r9:root', 'score': 0.07753727, 'index': 468, 'word': 'see', 'start': 1988, 'end': 1991}, {'entity': 'r9:root', 'score': 0.03982749, 'index': 469, 'word': 'what', 'start': 1992, 'end': 1996}, {'entity': 'r9:root', 'score': 0.050202448, 'index': 470, 'word': 'they', 'start': 1997, 'end': 2001}, {'entity': 'r9:root', 'score': 0.07350303, 'index': 471, 'word': 'were', 'start': 2002, 'end': 2006}, {'entity': 'r9:root', 'score': 0.041813992, 'index': 473, 'word': 'What', 'start': 2008, 'end': 2012}, {'entity': 'r9:root', 'score': 0.043242317, 'index': 475, 'word': 'picture', 'start': 2017, 'end': 2024}, {'entity': 'r9:root', 'score': 0.071942516, 'index': 476, 'word': 'showed', 'start': 2025, 'end': 2031}, {'entity': 'r9:root', 'score': 0.065713726, 'index': 477, 'word': 'was', 'start': 2032, 'end': 2035}, {'entity': 'r9:root', 'score': 0.047005344, 'index': 479, 'word': 'but', 'start': 2040, 'end': 2043}, {'entity': 'r9:root', 'score': 0.053594884, 'index': 480, 'word': '##te', 'start': 2043, 'end': 2045}, {'entity': 'r9:root', 'score': 0.038346287, 'index': 484, 'word': 'which', 'start': 2055, 'end': 2060}, {'entity': 'r9:root', 'score': 0.071865395, 'index': 485, 'word': 'are', 'start': 2061, 'end': 2064}, {'entity': 'r9:root', 'score': 0.052885562, 'index': 486, 'word': 'land', 'start': 2065, 'end': 2069}, {'entity': 'r9:root', 'score': 0.058789182, 'index': 487, 'word': '##form', 'start': 2069, 'end': 2073}, {'entity': 'r9:root', 'score': 0.05849257, 'index': 488, 'word': '##s', 'start': 2073, 'end': 2074}, {'entity': 'r9:root', 'score': 0.05117639, 'index': 489, 'word': 'common', 'start': 2075, 'end': 2081}, {'entity': 'r9:root', 'score': 0.051266044, 'index': 490, 'word': 'around', 'start': 2082, 'end': 2088}]\n",
      "{0: 'O', 1: 'r0:arg0|agt', 2: 'r0:arg0|cau', 3: 'r0:arg0|exp', 4: 'r0:arg0|pat', 5: 'r0:arg0|src', 6: 'r0:arg1|ext', 7: 'r0:arg1|loc', 8: 'r0:arg1|pat', 9: 'r0:arg1|tem', 10: 'r0:arg2|atr', 11: 'r0:arg2|ben', 12: 'r0:arg2|efi', 13: 'r0:arg2|exp', 14: 'r0:arg2|ext', 15: 'r0:arg2|ins', 16: 'r0:arg2|loc', 17: 'r0:arg2|tem', 18: 'r0:arg3|ben', 19: 'r0:arg3|ein', 20: 'r0:arg3|exp', 21: 'r0:arg3|fin', 22: 'r0:arg3|ins', 23: 'r0:arg3|loc', 24: 'r0:arg3|ori', 25: 'r0:arg4|des', 26: 'r0:arg4|efi', 27: 'r0:argM|LOC', 28: 'r0:argM|adv', 29: 'r0:argM|atr', 30: 'r0:argM|cau', 31: 'r0:argM|ext', 32: 'r0:argM|fin', 33: 'r0:argM|ins', 34: 'r0:argM|loc', 35: 'r0:argM|mnr', 36: 'r0:argM|tmp', 37: 'r0:root', 38: 'r10:arg0|agt', 39: 'r10:arg1|pat', 40: 'r10:arg1|tem', 41: 'r10:arg2|atr', 42: 'r10:arg2|ben', 43: 'r10:arg2|efi', 44: 'r10:arg2|loc', 45: 'r10:arg3|ben', 46: 'r10:arg4|des', 47: 'r10:argM|adv', 48: 'r10:argM|atr', 49: 'r10:argM|fin', 50: 'r10:argM|loc', 51: 'r10:argM|tmp', 52: 'r10:root', 53: 'r11:arg0|agt', 54: 'r11:arg0|cau', 55: 'r11:arg1|pat', 56: 'r11:arg1|tem', 57: 'r11:arg2|atr', 58: 'r11:arg2|ben', 59: 'r11:arg2|loc', 60: 'r11:arg4|des', 61: 'r11:argM|adv', 62: 'r11:argM|loc', 63: 'r11:argM|mnr', 64: 'r11:argM|tmp', 65: 'r11:root', 66: 'r12:arg0|agt', 67: 'r12:arg0|cau', 68: 'r12:arg1|pat', 69: 'r12:arg1|tem', 70: 'r12:arg2|atr', 71: 'r12:argM|adv', 72: 'r12:argM|cau', 73: 'r12:argM|loc', 74: 'r12:argM|tmp', 75: 'r12:root', 76: 'r13:arg0|agt', 77: 'r13:arg0|cau', 78: 'r13:arg1|pat', 79: 'r13:arg1|tem', 80: 'r13:arg2|atr', 81: 'r13:arg2|ben', 82: 'r13:argM|adv', 83: 'r13:argM|atr', 84: 'r13:argM|loc', 85: 'r13:root', 86: 'r14:arg0|agt', 87: 'r14:arg1|pat', 88: 'r14:arg2|ben', 89: 'r14:argM|adv', 90: 'r14:argM|loc', 91: 'r14:argM|mnr', 92: 'r14:root', 93: 'r15:arg0|cau', 94: 'r15:arg1|tem', 95: 'r15:arg2|atr', 96: 'r15:arg3|ben', 97: 'r15:root', 98: 'r16:arg0|agt', 99: 'r16:arg0|cau', 100: 'r16:arg1|pat', 101: 'r16:arg1|tem', 102: 'r16:argM|loc', 103: 'r16:argM|tmp', 104: 'r16:root', 105: 'r1:arg0|agt', 106: 'r1:arg0|cau', 107: 'r1:arg0|exp', 108: 'r1:arg0|src', 109: 'r1:arg1|ext', 110: 'r1:arg1|loc', 111: 'r1:arg1|pat', 112: 'r1:arg1|tem', 113: 'r1:arg2|atr', 114: 'r1:arg2|ben', 115: 'r1:arg2|efi', 116: 'r1:arg2|exp', 117: 'r1:arg2|ext', 118: 'r1:arg2|ins', 119: 'r1:arg2|loc', 120: 'r1:arg3|atr', 121: 'r1:arg3|ben', 122: 'r1:arg3|des', 123: 'r1:arg3|ein', 124: 'r1:arg3|exp', 125: 'r1:arg3|fin', 126: 'r1:arg3|ins', 127: 'r1:arg3|ori', 128: 'r1:arg4|des', 129: 'r1:arg4|efi', 130: 'r1:argM|adv', 131: 'r1:argM|atr', 132: 'r1:argM|cau', 133: 'r1:argM|ext', 134: 'r1:argM|fin', 135: 'r1:argM|ins', 136: 'r1:argM|loc', 137: 'r1:argM|mnr', 138: 'r1:argM|tmp', 139: 'r1:root', 140: 'r2:arg0|agt', 141: 'r2:arg0|cau', 142: 'r2:arg0|exp', 143: 'r2:arg0|src', 144: 'r2:arg1|ext', 145: 'r2:arg1|loc', 146: 'r2:arg1|pat', 147: 'r2:arg1|tem', 148: 'r2:arg2|atr', 149: 'r2:arg2|ben', 150: 'r2:arg2|efi', 151: 'r2:arg2|exp', 152: 'r2:arg2|ext', 153: 'r2:arg2|ins', 154: 'r2:arg2|loc', 155: 'r2:arg3|atr', 156: 'r2:arg3|ben', 157: 'r2:arg3|ein', 158: 'r2:arg3|exp', 159: 'r2:arg3|fin', 160: 'r2:arg3|loc', 161: 'r2:arg3|ori', 162: 'r2:arg4|des', 163: 'r2:arg4|efi', 164: 'r2:argM|adv', 165: 'r2:argM|atr', 166: 'r2:argM|cau', 167: 'r2:argM|ext', 168: 'r2:argM|fin', 169: 'r2:argM|ins', 170: 'r2:argM|loc', 171: 'r2:argM|mnr', 172: 'r2:argM|tmp', 173: 'r2:root', 174: 'r3:arg0|agt', 175: 'r3:arg0|cau', 176: 'r3:arg0|exp', 177: 'r3:arg0|src', 178: 'r3:arg1|ext', 179: 'r3:arg1|loc', 180: 'r3:arg1|pat', 181: 'r3:arg1|tem', 182: 'r3:arg2|atr', 183: 'r3:arg2|ben', 184: 'r3:arg2|efi', 185: 'r3:arg2|exp', 186: 'r3:arg2|ext', 187: 'r3:arg2|ins', 188: 'r3:arg2|loc', 189: 'r3:arg2|tem', 190: 'r3:arg3|ben', 191: 'r3:arg3|ein', 192: 'r3:arg3|fin', 193: 'r3:arg3|loc', 194: 'r3:arg3|ori', 195: 'r3:arg4|des', 196: 'r3:arg4|efi', 197: 'r3:argM|adv', 198: 'r3:argM|atr', 199: 'r3:argM|cau', 200: 'r3:argM|ext', 201: 'r3:argM|fin', 202: 'r3:argM|ins', 203: 'r3:argM|loc', 204: 'r3:argM|mnr', 205: 'r3:argM|tmp', 206: 'r3:root', 207: 'r4:arg0|agt', 208: 'r4:arg0|cau', 209: 'r4:arg0|exp', 210: 'r4:arg0|src', 211: 'r4:arg1|ext', 212: 'r4:arg1|loc', 213: 'r4:arg1|pat', 214: 'r4:arg1|tem', 215: 'r4:arg2|atr', 216: 'r4:arg2|ben', 217: 'r4:arg2|efi', 218: 'r4:arg2|exp', 219: 'r4:arg2|ext', 220: 'r4:arg2|ins', 221: 'r4:arg2|loc', 222: 'r4:arg3|ben', 223: 'r4:arg3|ein', 224: 'r4:arg3|exp', 225: 'r4:arg3|fin', 226: 'r4:arg3|ori', 227: 'r4:arg4|des', 228: 'r4:arg4|efi', 229: 'r4:argM|adv', 230: 'r4:argM|atr', 231: 'r4:argM|cau', 232: 'r4:argM|ext', 233: 'r4:argM|fin', 234: 'r4:argM|ins', 235: 'r4:argM|loc', 236: 'r4:argM|mnr', 237: 'r4:argM|tmp', 238: 'r4:root', 239: 'r5:arg0|agt', 240: 'r5:arg0|cau', 241: 'r5:arg1|ext', 242: 'r5:arg1|loc', 243: 'r5:arg1|pat', 244: 'r5:arg1|tem', 245: 'r5:arg2|atr', 246: 'r5:arg2|ben', 247: 'r5:arg2|efi', 248: 'r5:arg2|exp', 249: 'r5:arg2|ext', 250: 'r5:arg2|loc', 251: 'r5:arg3|ben', 252: 'r5:arg3|ein', 253: 'r5:arg3|fin', 254: 'r5:arg3|ins', 255: 'r5:arg3|ori', 256: 'r5:arg4|des', 257: 'r5:arg4|efi', 258: 'r5:argM|adv', 259: 'r5:argM|atr', 260: 'r5:argM|cau', 261: 'r5:argM|ext', 262: 'r5:argM|fin', 263: 'r5:argM|loc', 264: 'r5:argM|mnr', 265: 'r5:argM|tmp', 266: 'r5:root', 267: 'r6:arg0|agt', 268: 'r6:arg0|cau', 269: 'r6:arg1|loc', 270: 'r6:arg1|pat', 271: 'r6:arg1|tem', 272: 'r6:arg2|atr', 273: 'r6:arg2|ben', 274: 'r6:arg2|efi', 275: 'r6:arg2|exp', 276: 'r6:arg2|ext', 277: 'r6:arg2|loc', 278: 'r6:arg3|ben', 279: 'r6:arg3|ori', 280: 'r6:arg4|des', 281: 'r6:argM|adv', 282: 'r6:argM|atr', 283: 'r6:argM|cau', 284: 'r6:argM|ext', 285: 'r6:argM|fin', 286: 'r6:argM|loc', 287: 'r6:argM|mnr', 288: 'r6:argM|tmp', 289: 'r6:root', 290: 'r7:arg0|agt', 291: 'r7:arg0|cau', 292: 'r7:arg1|loc', 293: 'r7:arg1|pat', 294: 'r7:arg1|tem', 295: 'r7:arg2|atr', 296: 'r7:arg2|ben', 297: 'r7:arg2|efi', 298: 'r7:arg2|loc', 299: 'r7:arg3|ben', 300: 'r7:arg3|exp', 301: 'r7:arg3|fin', 302: 'r7:arg3|ori', 303: 'r7:arg4|des', 304: 'r7:argM|adv', 305: 'r7:argM|atr', 306: 'r7:argM|cau', 307: 'r7:argM|fin', 308: 'r7:argM|ins', 309: 'r7:argM|loc', 310: 'r7:argM|mnr', 311: 'r7:argM|tmp', 312: 'r7:root', 313: 'r8:arg0|agt', 314: 'r8:arg0|cau', 315: 'r8:arg0|src', 316: 'r8:arg1|pat', 317: 'r8:arg1|tem', 318: 'r8:arg2|atr', 319: 'r8:arg2|ben', 320: 'r8:arg2|ext', 321: 'r8:arg2|loc', 322: 'r8:arg3|ori', 323: 'r8:arg4|des', 324: 'r8:argM|adv', 325: 'r8:argM|cau', 326: 'r8:argM|ext', 327: 'r8:argM|fin', 328: 'r8:argM|loc', 329: 'r8:argM|mnr', 330: 'r8:argM|tmp', 331: 'r8:root', 332: 'r9:arg0|agt', 333: 'r9:arg0|cau', 334: 'r9:arg1|pat', 335: 'r9:arg1|tem', 336: 'r9:arg2|atr', 337: 'r9:arg2|ben', 338: 'r9:arg2|ins', 339: 'r9:arg2|loc', 340: 'r9:arg4|des', 341: 'r9:argM|adv', 342: 'r9:argM|cau', 343: 'r9:argM|fin', 344: 'r9:argM|loc', 345: 'r9:argM|mnr', 346: 'r9:argM|tmp', 347: 'r9:root'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mbruton/spa_enpt_mBERT\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"mbruton/spa_enpt_mBERT\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "3476183b-6fcf-439b-8717-65e7248f0191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'r0:arg0|agt', 2: 'r0:arg0|cau', 3: 'r0:arg0|exp', 4: 'r0:arg0|pat', 5: 'r0:arg0|src', 6: 'r0:arg1|ext', 7: 'r0:arg1|loc', 8: 'r0:arg1|pat', 9: 'r0:arg1|tem', 10: 'r0:arg2|atr', 11: 'r0:arg2|ben', 12: 'r0:arg2|efi', 13: 'r0:arg2|exp', 14: 'r0:arg2|ext', 15: 'r0:arg2|ins', 16: 'r0:arg2|loc', 17: 'r0:arg2|tem', 18: 'r0:arg3|ben', 19: 'r0:arg3|ein', 20: 'r0:arg3|exp', 21: 'r0:arg3|fin', 22: 'r0:arg3|ins', 23: 'r0:arg3|loc', 24: 'r0:arg3|ori', 25: 'r0:arg4|des', 26: 'r0:arg4|efi', 27: 'r0:argM|LOC', 28: 'r0:argM|adv', 29: 'r0:argM|atr', 30: 'r0:argM|cau', 31: 'r0:argM|ext', 32: 'r0:argM|fin', 33: 'r0:argM|ins', 34: 'r0:argM|loc', 35: 'r0:argM|mnr', 36: 'r0:argM|tmp', 37: 'r0:root', 38: 'r10:arg0|agt', 39: 'r10:arg1|pat', 40: 'r10:arg1|tem', 41: 'r10:arg2|atr', 42: 'r10:arg2|ben', 43: 'r10:arg2|efi', 44: 'r10:arg2|loc', 45: 'r10:arg3|ben', 46: 'r10:arg4|des', 47: 'r10:argM|adv', 48: 'r10:argM|atr', 49: 'r10:argM|fin', 50: 'r10:argM|loc', 51: 'r10:argM|tmp', 52: 'r10:root', 53: 'r11:arg0|agt', 54: 'r11:arg0|cau', 55: 'r11:arg1|pat', 56: 'r11:arg1|tem', 57: 'r11:arg2|atr', 58: 'r11:arg2|ben', 59: 'r11:arg2|loc', 60: 'r11:arg4|des', 61: 'r11:argM|adv', 62: 'r11:argM|loc', 63: 'r11:argM|mnr', 64: 'r11:argM|tmp', 65: 'r11:root', 66: 'r12:arg0|agt', 67: 'r12:arg0|cau', 68: 'r12:arg1|pat', 69: 'r12:arg1|tem', 70: 'r12:arg2|atr', 71: 'r12:argM|adv', 72: 'r12:argM|cau', 73: 'r12:argM|loc', 74: 'r12:argM|tmp', 75: 'r12:root', 76: 'r13:arg0|agt', 77: 'r13:arg0|cau', 78: 'r13:arg1|pat', 79: 'r13:arg1|tem', 80: 'r13:arg2|atr', 81: 'r13:arg2|ben', 82: 'r13:argM|adv', 83: 'r13:argM|atr', 84: 'r13:argM|loc', 85: 'r13:root', 86: 'r14:arg0|agt', 87: 'r14:arg1|pat', 88: 'r14:arg2|ben', 89: 'r14:argM|adv', 90: 'r14:argM|loc', 91: 'r14:argM|mnr', 92: 'r14:root', 93: 'r15:arg0|cau', 94: 'r15:arg1|tem', 95: 'r15:arg2|atr', 96: 'r15:arg3|ben', 97: 'r15:root', 98: 'r16:arg0|agt', 99: 'r16:arg0|cau', 100: 'r16:arg1|pat', 101: 'r16:arg1|tem', 102: 'r16:argM|loc', 103: 'r16:argM|tmp', 104: 'r16:root', 105: 'r1:arg0|agt', 106: 'r1:arg0|cau', 107: 'r1:arg0|exp', 108: 'r1:arg0|src', 109: 'r1:arg1|ext', 110: 'r1:arg1|loc', 111: 'r1:arg1|pat', 112: 'r1:arg1|tem', 113: 'r1:arg2|atr', 114: 'r1:arg2|ben', 115: 'r1:arg2|efi', 116: 'r1:arg2|exp', 117: 'r1:arg2|ext', 118: 'r1:arg2|ins', 119: 'r1:arg2|loc', 120: 'r1:arg3|atr', 121: 'r1:arg3|ben', 122: 'r1:arg3|des', 123: 'r1:arg3|ein', 124: 'r1:arg3|exp', 125: 'r1:arg3|fin', 126: 'r1:arg3|ins', 127: 'r1:arg3|ori', 128: 'r1:arg4|des', 129: 'r1:arg4|efi', 130: 'r1:argM|adv', 131: 'r1:argM|atr', 132: 'r1:argM|cau', 133: 'r1:argM|ext', 134: 'r1:argM|fin', 135: 'r1:argM|ins', 136: 'r1:argM|loc', 137: 'r1:argM|mnr', 138: 'r1:argM|tmp', 139: 'r1:root', 140: 'r2:arg0|agt', 141: 'r2:arg0|cau', 142: 'r2:arg0|exp', 143: 'r2:arg0|src', 144: 'r2:arg1|ext', 145: 'r2:arg1|loc', 146: 'r2:arg1|pat', 147: 'r2:arg1|tem', 148: 'r2:arg2|atr', 149: 'r2:arg2|ben', 150: 'r2:arg2|efi', 151: 'r2:arg2|exp', 152: 'r2:arg2|ext', 153: 'r2:arg2|ins', 154: 'r2:arg2|loc', 155: 'r2:arg3|atr', 156: 'r2:arg3|ben', 157: 'r2:arg3|ein', 158: 'r2:arg3|exp', 159: 'r2:arg3|fin', 160: 'r2:arg3|loc', 161: 'r2:arg3|ori', 162: 'r2:arg4|des', 163: 'r2:arg4|efi', 164: 'r2:argM|adv', 165: 'r2:argM|atr', 166: 'r2:argM|cau', 167: 'r2:argM|ext', 168: 'r2:argM|fin', 169: 'r2:argM|ins', 170: 'r2:argM|loc', 171: 'r2:argM|mnr', 172: 'r2:argM|tmp', 173: 'r2:root', 174: 'r3:arg0|agt', 175: 'r3:arg0|cau', 176: 'r3:arg0|exp', 177: 'r3:arg0|src', 178: 'r3:arg1|ext', 179: 'r3:arg1|loc', 180: 'r3:arg1|pat', 181: 'r3:arg1|tem', 182: 'r3:arg2|atr', 183: 'r3:arg2|ben', 184: 'r3:arg2|efi', 185: 'r3:arg2|exp', 186: 'r3:arg2|ext', 187: 'r3:arg2|ins', 188: 'r3:arg2|loc', 189: 'r3:arg2|tem', 190: 'r3:arg3|ben', 191: 'r3:arg3|ein', 192: 'r3:arg3|fin', 193: 'r3:arg3|loc', 194: 'r3:arg3|ori', 195: 'r3:arg4|des', 196: 'r3:arg4|efi', 197: 'r3:argM|adv', 198: 'r3:argM|atr', 199: 'r3:argM|cau', 200: 'r3:argM|ext', 201: 'r3:argM|fin', 202: 'r3:argM|ins', 203: 'r3:argM|loc', 204: 'r3:argM|mnr', 205: 'r3:argM|tmp', 206: 'r3:root', 207: 'r4:arg0|agt', 208: 'r4:arg0|cau', 209: 'r4:arg0|exp', 210: 'r4:arg0|src', 211: 'r4:arg1|ext', 212: 'r4:arg1|loc', 213: 'r4:arg1|pat', 214: 'r4:arg1|tem', 215: 'r4:arg2|atr', 216: 'r4:arg2|ben', 217: 'r4:arg2|efi', 218: 'r4:arg2|exp', 219: 'r4:arg2|ext', 220: 'r4:arg2|ins', 221: 'r4:arg2|loc', 222: 'r4:arg3|ben', 223: 'r4:arg3|ein', 224: 'r4:arg3|exp', 225: 'r4:arg3|fin', 226: 'r4:arg3|ori', 227: 'r4:arg4|des', 228: 'r4:arg4|efi', 229: 'r4:argM|adv', 230: 'r4:argM|atr', 231: 'r4:argM|cau', 232: 'r4:argM|ext', 233: 'r4:argM|fin', 234: 'r4:argM|ins', 235: 'r4:argM|loc', 236: 'r4:argM|mnr', 237: 'r4:argM|tmp', 238: 'r4:root', 239: 'r5:arg0|agt', 240: 'r5:arg0|cau', 241: 'r5:arg1|ext', 242: 'r5:arg1|loc', 243: 'r5:arg1|pat', 244: 'r5:arg1|tem', 245: 'r5:arg2|atr', 246: 'r5:arg2|ben', 247: 'r5:arg2|efi', 248: 'r5:arg2|exp', 249: 'r5:arg2|ext', 250: 'r5:arg2|loc', 251: 'r5:arg3|ben', 252: 'r5:arg3|ein', 253: 'r5:arg3|fin', 254: 'r5:arg3|ins', 255: 'r5:arg3|ori', 256: 'r5:arg4|des', 257: 'r5:arg4|efi', 258: 'r5:argM|adv', 259: 'r5:argM|atr', 260: 'r5:argM|cau', 261: 'r5:argM|ext', 262: 'r5:argM|fin', 263: 'r5:argM|loc', 264: 'r5:argM|mnr', 265: 'r5:argM|tmp', 266: 'r5:root', 267: 'r6:arg0|agt', 268: 'r6:arg0|cau', 269: 'r6:arg1|loc', 270: 'r6:arg1|pat', 271: 'r6:arg1|tem', 272: 'r6:arg2|atr', 273: 'r6:arg2|ben', 274: 'r6:arg2|efi', 275: 'r6:arg2|exp', 276: 'r6:arg2|ext', 277: 'r6:arg2|loc', 278: 'r6:arg3|ben', 279: 'r6:arg3|ori', 280: 'r6:arg4|des', 281: 'r6:argM|adv', 282: 'r6:argM|atr', 283: 'r6:argM|cau', 284: 'r6:argM|ext', 285: 'r6:argM|fin', 286: 'r6:argM|loc', 287: 'r6:argM|mnr', 288: 'r6:argM|tmp', 289: 'r6:root', 290: 'r7:arg0|agt', 291: 'r7:arg0|cau', 292: 'r7:arg1|loc', 293: 'r7:arg1|pat', 294: 'r7:arg1|tem', 295: 'r7:arg2|atr', 296: 'r7:arg2|ben', 297: 'r7:arg2|efi', 298: 'r7:arg2|loc', 299: 'r7:arg3|ben', 300: 'r7:arg3|exp', 301: 'r7:arg3|fin', 302: 'r7:arg3|ori', 303: 'r7:arg4|des', 304: 'r7:argM|adv', 305: 'r7:argM|atr', 306: 'r7:argM|cau', 307: 'r7:argM|fin', 308: 'r7:argM|ins', 309: 'r7:argM|loc', 310: 'r7:argM|mnr', 311: 'r7:argM|tmp', 312: 'r7:root', 313: 'r8:arg0|agt', 314: 'r8:arg0|cau', 315: 'r8:arg0|src', 316: 'r8:arg1|pat', 317: 'r8:arg1|tem', 318: 'r8:arg2|atr', 319: 'r8:arg2|ben', 320: 'r8:arg2|ext', 321: 'r8:arg2|loc', 322: 'r8:arg3|ori', 323: 'r8:arg4|des', 324: 'r8:argM|adv', 325: 'r8:argM|cau', 326: 'r8:argM|ext', 327: 'r8:argM|fin', 328: 'r8:argM|loc', 329: 'r8:argM|mnr', 330: 'r8:argM|tmp', 331: 'r8:root', 332: 'r9:arg0|agt', 333: 'r9:arg0|cau', 334: 'r9:arg1|pat', 335: 'r9:arg1|tem', 336: 'r9:arg2|atr', 337: 'r9:arg2|ben', 338: 'r9:arg2|ins', 339: 'r9:arg2|loc', 340: 'r9:arg4|des', 341: 'r9:argM|adv', 342: 'r9:argM|cau', 343: 'r9:argM|fin', 344: 'r9:argM|loc', 345: 'r9:argM|mnr', 346: 'r9:argM|tmp', 347: 'r9:root'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84162c-8432-432d-b74b-c2ce4c6b96c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be77f7-a1bc-4afc-8fbb-8e138420e8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6da949b6-cae9-409d-a05d-d93f9f634bfd",
   "metadata": {},
   "source": [
    "## 11  benjamin/wtp-bert-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "6b9d327a-5f62-435e-923e-9a51d91c958a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `bert-char` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bert-char'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[315], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-bert-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-bert-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `bert-char` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-bert-mini\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-bert-mini\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ab65e-9856-45a9-aa54-c230abfe8d4b",
   "metadata": {},
   "source": [
    "## 12 Babelscapewikineural-multilingual-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "7453080b-53a9-4a84-ad26-8a5328fe1682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'B-ORG', 'score': 0.9951147, 'index': 8, 'word': 'NASA', 'start': 16, 'end': 20}, {'entity': 'I-MISC', 'score': 0.6191017, 'index': 23, 'word': 'Face', 'start': 88, 'end': 92}, {'entity': 'I-MISC', 'score': 0.5708344, 'index': 24, 'word': 'On', 'start': 93, 'end': 95}, {'entity': 'I-MISC', 'score': 0.5786274, 'index': 25, 'word': 'Mars', 'start': 96, 'end': 100}, {'entity': 'B-LOC', 'score': 0.877606, 'index': 37, 'word': 'Mars', 'start': 152, 'end': 156}, {'entity': 'I-MISC', 'score': 0.9051992, 'index': 60, 'word': 'Viking', 'start': 240, 'end': 246}, {'entity': 'I-MISC', 'score': 0.9834109, 'index': 61, 'word': '1', 'start': 247, 'end': 248}, {'entity': 'I-MISC', 'score': 0.48295248, 'index': 98, 'word': 'Mart', 'start': 407, 'end': 411}, {'entity': 'I-MISC', 'score': 0.47647634, 'index': 99, 'word': '##ian', 'start': 411, 'end': 414}, {'entity': 'B-LOC', 'score': 0.97810775, 'index': 104, 'word': 'C', 'start': 435, 'end': 436}, {'entity': 'I-LOC', 'score': 0.9512793, 'index': 105, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'I-LOC', 'score': 0.9480485, 'index': 106, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'B-LOC', 'score': 0.4522933, 'index': 121, 'word': 'Egypt', 'start': 496, 'end': 501}, {'entity': 'I-MISC', 'score': 0.43941417, 'index': 122, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'I-MISC', 'score': 0.33125964, 'index': 123, 'word': 'Ph', 'start': 505, 'end': 507}, {'entity': 'I-PER', 'score': 0.2833639, 'index': 124, 'word': '##ara', 'start': 507, 'end': 510}, {'entity': 'B-ORG', 'score': 0.9865861, 'index': 194, 'word': 'NASA', 'start': 801, 'end': 805}, {'entity': 'B-LOC', 'score': 0.814626, 'index': 205, 'word': 'Mars', 'start': 843, 'end': 847}, {'entity': 'B-LOC', 'score': 0.7951029, 'index': 215, 'word': 'Mars', 'start': 874, 'end': 878}, {'entity': 'B-LOC', 'score': 0.87542975, 'index': 263, 'word': 'Mars', 'start': 1087, 'end': 1091}, {'entity': 'B-ORG', 'score': 0.99368435, 'index': 282, 'word': 'NASA', 'start': 1168, 'end': 1172}, {'entity': 'B-LOC', 'score': 0.7881979, 'index': 291, 'word': 'Mars', 'start': 1219, 'end': 1223}, {'entity': 'B-PER', 'score': 0.9987973, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.99919313, 'index': 317, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'I-PER', 'score': 0.99887544, 'index': 318, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'I-MISC', 'score': 0.8786556, 'index': 321, 'word': 'Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-MISC', 'score': 0.9564052, 'index': 322, 'word': 'Or', 'start': 1338, 'end': 1340}, {'entity': 'I-MISC', 'score': 0.96700704, 'index': 323, 'word': '##biter', 'start': 1340, 'end': 1345}, {'entity': 'I-MISC', 'score': 0.89913684, 'index': 338, 'word': 'Viking', 'start': 1418, 'end': 1424}, {'entity': 'B-PER', 'score': 0.97243416, 'index': 415, 'word': 'Mali', 'start': 1744, 'end': 1748}, {'entity': 'I-PER', 'score': 0.963992, 'index': 416, 'word': '##n', 'start': 1748, 'end': 1749}, {'entity': 'I-MISC', 'score': 0.7103455, 'index': 492, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'I-MISC', 'score': 0.5068105, 'index': 493, 'word': 'West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "c2d485df-3baa-4175-84d1-ef36150a0f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-MISC', 'I-MISC', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=['O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'B-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    "'O',\n",
    " 'B-PER',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    "'O',\n",
    "'O',\n",
    " 'I-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-MISC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    "'O'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c1860d32-9747-4cb1-8215-13a6b2449d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 0.5, 'recall': 0.5, 'f1': 0.5, 'number': 2}, 'MISC': {'precision': 0.5, 'recall': 0.6, 'f1': 0.5454545454545454, 'number': 10}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3}, 'PER': {'precision': 0.6, 'recall': 1.0, 'f1': 0.7499999999999999, 'number': 3}, 'overall_precision': 0.5909090909090909, 'overall_recall': 0.7222222222222222, 'overall_f1': 0.65, 'overall_accuracy': 0.97165991902834}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152a918-a4b1-4f0d-aa5a-48158eb9bee8",
   "metadata": {},
   "source": [
    "## 13 julian-schelb/roberta-ner-multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "ac8d9116-063a-4edb-b1cb-1ad401ed67c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'B-ORG', 'score': 0.8837392, 'index': 8, 'word': '▁NASA', 'start': 16, 'end': 20}, {'entity': 'B-ORG', 'score': 0.38925776, 'index': 97, 'word': '▁Marti', 'start': 407, 'end': 412}, {'entity': 'B-LOC', 'score': 0.7155649, 'index': 103, 'word': '▁Cy', 'start': 435, 'end': 437}, {'entity': 'B-LOC', 'score': 0.64458394, 'index': 104, 'word': 'do', 'start': 437, 'end': 439}, {'entity': 'B-LOC', 'score': 0.623109, 'index': 105, 'word': 'nia', 'start': 439, 'end': 442}, {'entity': 'B-ORG', 'score': 0.34994408, 'index': 119, 'word': '▁Egypt', 'start': 496, 'end': 501}, {'entity': 'B-ORG', 'score': 0.32543704, 'index': 120, 'word': 'ion', 'start': 501, 'end': 504}, {'entity': 'I-PER', 'score': 0.4674562, 'index': 121, 'word': '▁Phar', 'start': 505, 'end': 509}, {'entity': 'I-PER', 'score': 0.53341544, 'index': 122, 'word': 'a', 'start': 509, 'end': 510}, {'entity': 'I-PER', 'score': 0.50114465, 'index': 123, 'word': 'oh', 'start': 510, 'end': 512}, {'entity': 'B-ORG', 'score': 0.5386203, 'index': 193, 'word': '▁NASA', 'start': 801, 'end': 805}, {'entity': 'B-ORG', 'score': 0.6421071, 'index': 285, 'word': '▁NASA', 'start': 1168, 'end': 1172}, {'entity': 'B-PER', 'score': 0.521761, 'index': 319, 'word': '▁Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.66133285, 'index': 320, 'word': '▁Malin', 'start': 1319, 'end': 1324}, {'entity': 'B-ORG', 'score': 0.87530375, 'index': 323, 'word': '▁Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-ORG', 'score': 0.775388, 'index': 324, 'word': '▁Or', 'start': 1338, 'end': 1340}, {'entity': 'I-ORG', 'score': 0.74558014, 'index': 325, 'word': 'bit', 'start': 1340, 'end': 1343}, {'entity': 'I-ORG', 'score': 0.77897793, 'index': 326, 'word': 'er', 'start': 1343, 'end': 1345}, {'entity': 'B-LOC', 'score': 0.781982, 'index': 491, 'word': '▁American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.5271412, 'index': 492, 'word': '▁West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"julian-schelb/roberta-ner-multilingual\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"julian-schelb/roberta-ner-multilingual\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "4c1445f5-cfaf-4c2f-9836-85a450c245c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "['▁So', ',', '▁if', '▁you', \"'\", 're', '▁a', '▁NASA', '▁scientist', ',', '▁you', '▁should', '▁be', '▁able', '▁to', '▁tell', '▁me', '▁the', '▁whole', '▁story', '▁about', '▁the', '▁Face', '▁On', '▁Mars', ',', '▁which', '▁obviously', '▁is', '▁evidence', '▁that', '▁there', '▁is', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁the', '▁face', '▁was', '▁created', '▁by', '▁alien', 's', ',', '▁correct', '?\"', '▁No', ',', '▁twenty', '▁five', '▁years', '▁ago', ',', '▁our', '▁Viking', '▁1', '▁space', 'craft', '▁was', '▁circ', 'ling', '▁the', '▁planet', ',', '▁sna', 'pping', '▁photos', ',', '▁when', '▁it', '▁spot', 'ted', '▁the', '▁shadow', 'y', '▁like', 'ness', '▁of', '▁a', '▁human', '▁face', '.', '▁Us', '▁scientist', 's', '▁figure', 'd', '▁out', '▁that', '▁it', '▁was', '▁just', '▁another', '▁Marti', 'an', '▁mesa', ',', '▁common', '▁around', '▁Cy', 'do', 'nia', ',', '▁only', '▁this', '▁one', '▁had', '▁shadow', 's', '▁that', '▁made', '▁it', '▁look', '▁like', '▁an', '▁Egypt', 'ion', '▁Phar', 'a', 'oh', '.', '▁Very', '▁few', '▁days', '▁later', ',', '▁we', '▁reveal', 'ed', '▁the', '▁image', '▁for', '▁all', '▁to', '▁see', ',', '▁and', '▁we', '▁made', '▁sure', '▁to', '▁note', '▁that', '▁it', '▁was', '▁a', '▁huge', '▁rock', '▁formation', '▁that', '▁just', '▁rese', 'mble', 'd', '▁a', '▁human', '▁head', '▁and', '▁face', ',', '▁but', '▁all', '▁of', '▁it', '▁was', '▁for', 'med', '▁by', '▁shadow', 's', '.', '▁We', '▁only', '▁announced', '▁it', '▁because', '▁we', '▁thought', '▁it', '▁would', '▁be', '▁a', '▁good', '▁way', '▁to', '▁engage', '▁the', '▁public', '▁with', '▁NASA', \"'\", 's', '▁finding', 's', ',', '▁and', '▁at', 'rra', 'ct', '▁attention', '▁to', '▁Mars', '-', '-', '▁and', '▁it', '▁did', '.', '▁The', '▁face', '▁on', '▁Mars', '▁soon', '▁became', '▁a', '▁pop', '▁icon', ';', '▁shot', '▁in', '▁movies', ',', '▁appeared', '▁in', '▁books', ',', '▁magazine', 's', ',', '▁radio', '▁talk', '▁shows', ',', '▁and', '▁ha', 'un', 'ted', '▁gro', 'cer', 'y', '▁store', '▁check', 'out', '▁lines', '▁for', '▁25', '▁years', '.', '▁Some', '▁people', '▁thought', '▁the', '▁natural', '▁land', 'form', '▁was', '▁evidence', '▁of', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁us', '▁scientist', 's', '▁wanted', '▁to', '▁hi', 'de', '▁it', ',', '▁but', '▁really', ',', '▁the', '▁defender', 's', '▁of', '▁the', '▁NASA', '▁budget', '▁wish', '▁there', '▁was', '▁an', 'cient', '▁civiliza', 'tion', '▁on', '▁Mars', '.', '▁We', '▁decided', '▁to', '▁take', '▁another', '▁shot', '▁just', '▁to', '▁make', '▁sure', '▁we', '▁were', 'n', \"'\", 't', '▁wrong', ',', '▁on', '▁April', '▁5', ',', '▁1998.', '▁Michael', '▁Malin', '▁and', '▁his', '▁Mars', '▁Or', 'bit', 'er', '▁camera', '▁team', '▁took', '▁a', '▁picture', '▁that', '▁was', '▁ten', '▁times', '▁sharp', 'er', '▁than', '▁the', '▁original', '▁Viking', '▁photos', ',', '▁reveal', 'ing', '▁a', '▁natural', '▁land', 'form', ',', '▁which', '▁meant', '▁no', '▁alien', '▁monument', '.', '▁\"', 'But', '▁that', '▁picture', '▁wasn', \"'\", 't', '▁very', '▁clear', '▁at', '▁all', ',', '▁which', '▁could', '▁mean', '▁alien', '▁mark', 'ings', '▁were', '▁hidden', '▁by', '▁ha', 'ze', '\"', '▁Well', '▁no', ',', '▁yes', '▁that', '▁rumor', '▁started', ',', '▁but', '▁to', '▁prove', '▁them', '▁wrong', '▁on', '▁April', '▁8', ',', '▁2001', '▁we', '▁decided', '▁to', '▁take', '▁another', '▁picture', ',', '▁making', '▁sure', '▁it', '▁was', '▁a', '▁cloud', 'less', '▁summer', '▁day', '.', '▁Malin', \"'\", 's', '▁team', '▁capture', 'd', '▁an', '▁amazing', '▁photo', '▁using', '▁the', '▁camera', \"'\", 's', '▁absolute', '▁maximum', '▁revolution', '.', '▁With', '▁this', '▁camera', '▁you', '▁can', '▁discern', '▁things', '▁in', '▁a', '▁digital', '▁image', ',', '▁3', '▁times', '▁bigger', '▁than', '▁the', '▁pixel', '▁size', '▁which', '▁means', '▁if', '▁there', '▁were', '▁any', '▁sign', 's', '▁of', '▁life', ',', '▁you', '▁could', '▁easily', '▁see', '▁what', '▁they', '▁were', '.', '▁What', '▁the', '▁picture', '▁showed', '▁was', '▁the', '▁but', 'te', '▁or', '▁mesa', ',', '▁which', '▁are', '▁land', 'form', 's', '▁common', '▁around', '▁the', '▁American', '▁West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[ 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'B-LOC',\n",
    " 'B-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'O'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6d02cb89-60ed-4ee3-b714-8da9b5066b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 4}, 'ORG': {'precision': 0.21428571428571427, 'recall': 1.0, 'f1': 0.35294117647058826, 'number': 3}, 'PER': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1': 0.8, 'number': 2}, 'overall_precision': 0.42857142857142855, 'overall_recall': 1.0, 'overall_f1': 0.6, 'overall_accuracy': 0.9634888438133874}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e73652-f3c6-438d-bd77-fce3074a2ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d4e36-8f02-4c4e-bd26-190703a10df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16569163-0440-4747-8af0-167e67d576a5",
   "metadata": {},
   "source": [
    "## 14 FacebookAI/xlm-roberta-large-finetuned-conll03-german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d52433ab-9b54-4078-b3a5-6bb493a128ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large-finetuned-conll03-german were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'I-MISC', 'score': 0.99818534, 'index': 8, 'word': '▁NASA', 'start': 16, 'end': 20}, {'entity': 'I-LOC', 'score': 0.980961, 'index': 25, 'word': '▁Mars', 'start': 96, 'end': 100}, {'entity': 'I-LOC', 'score': 0.98643017, 'index': 36, 'word': '▁Mars', 'start': 152, 'end': 156}, {'entity': 'I-MISC', 'score': 0.9991737, 'index': 58, 'word': '▁Viking', 'start': 240, 'end': 246}, {'entity': 'I-MISC', 'score': 0.9957217, 'index': 59, 'word': '▁1', 'start': 247, 'end': 248}, {'entity': 'I-MISC', 'score': 0.9766658, 'index': 97, 'word': '▁Marti', 'start': 407, 'end': 412}, {'entity': 'I-LOC', 'score': 0.9951835, 'index': 103, 'word': '▁Cy', 'start': 435, 'end': 437}, {'entity': 'I-LOC', 'score': 0.9865209, 'index': 104, 'word': 'do', 'start': 437, 'end': 439}, {'entity': 'I-LOC', 'score': 0.9743838, 'index': 105, 'word': 'nia', 'start': 439, 'end': 442}, {'entity': 'I-MISC', 'score': 0.98988336, 'index': 119, 'word': '▁Egypt', 'start': 496, 'end': 501}, {'entity': 'I-MISC', 'score': 0.6562522, 'index': 120, 'word': 'ion', 'start': 501, 'end': 504}, {'entity': 'I-PER', 'score': 0.3977966, 'index': 121, 'word': '▁Phar', 'start': 505, 'end': 509}, {'entity': 'I-PER', 'score': 0.695911, 'index': 122, 'word': 'a', 'start': 509, 'end': 510}, {'entity': 'I-ORG', 'score': 0.9998938, 'index': 193, 'word': '▁NASA', 'start': 801, 'end': 805}, {'entity': 'I-LOC', 'score': 0.9539074, 'index': 205, 'word': '▁Mars', 'start': 843, 'end': 847}, {'entity': 'I-LOC', 'score': 0.95086044, 'index': 215, 'word': '▁Mars', 'start': 874, 'end': 878}, {'entity': 'I-LOC', 'score': 0.9250947, 'index': 264, 'word': '▁Mars', 'start': 1087, 'end': 1091}, {'entity': 'I-MISC', 'score': 0.9836256, 'index': 285, 'word': '▁NASA', 'start': 1168, 'end': 1172}, {'entity': 'I-LOC', 'score': 0.8872926, 'index': 295, 'word': '▁Mars', 'start': 1219, 'end': 1223}, {'entity': 'I-PER', 'score': 0.9999858, 'index': 319, 'word': '▁Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.9999554, 'index': 320, 'word': '▁Malin', 'start': 1319, 'end': 1324}, {'entity': 'I-MISC', 'score': 0.99574405, 'index': 323, 'word': '▁Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-MISC', 'score': 0.98675495, 'index': 324, 'word': '▁Or', 'start': 1338, 'end': 1340}, {'entity': 'I-MISC', 'score': 0.9656561, 'index': 325, 'word': 'bit', 'start': 1340, 'end': 1343}, {'entity': 'I-MISC', 'score': 0.8312396, 'index': 326, 'word': 'er', 'start': 1343, 'end': 1345}, {'entity': 'I-MISC', 'score': 0.9995989, 'index': 341, 'word': '▁Viking', 'start': 1418, 'end': 1424}, {'entity': 'I-PER', 'score': 0.99997556, 'index': 416, 'word': '▁Malin', 'start': 1744, 'end': 1749}, {'entity': 'I-MISC', 'score': 0.49780306, 'index': 491, 'word': '▁American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.85682255, 'index': 492, 'word': '▁West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PER', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PER', 8: 'O'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-german\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-german\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "8e2d092a-0bbb-472d-b5f4-cc341a7f8fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-LOC', 'O']\n",
      "['▁So', ',', '▁if', '▁you', \"'\", 're', '▁a', '▁NASA', '▁scientist', ',', '▁you', '▁should', '▁be', '▁able', '▁to', '▁tell', '▁me', '▁the', '▁whole', '▁story', '▁about', '▁the', '▁Face', '▁On', '▁Mars', ',', '▁which', '▁obviously', '▁is', '▁evidence', '▁that', '▁there', '▁is', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁the', '▁face', '▁was', '▁created', '▁by', '▁alien', 's', ',', '▁correct', '?\"', '▁No', ',', '▁twenty', '▁five', '▁years', '▁ago', ',', '▁our', '▁Viking', '▁1', '▁space', 'craft', '▁was', '▁circ', 'ling', '▁the', '▁planet', ',', '▁sna', 'pping', '▁photos', ',', '▁when', '▁it', '▁spot', 'ted', '▁the', '▁shadow', 'y', '▁like', 'ness', '▁of', '▁a', '▁human', '▁face', '.', '▁Us', '▁scientist', 's', '▁figure', 'd', '▁out', '▁that', '▁it', '▁was', '▁just', '▁another', '▁Marti', 'an', '▁mesa', ',', '▁common', '▁around', '▁Cy', 'do', 'nia', ',', '▁only', '▁this', '▁one', '▁had', '▁shadow', 's', '▁that', '▁made', '▁it', '▁look', '▁like', '▁an', '▁Egypt', 'ion', '▁Phar', 'a', 'oh', '.', '▁Very', '▁few', '▁days', '▁later', ',', '▁we', '▁reveal', 'ed', '▁the', '▁image', '▁for', '▁all', '▁to', '▁see', ',', '▁and', '▁we', '▁made', '▁sure', '▁to', '▁note', '▁that', '▁it', '▁was', '▁a', '▁huge', '▁rock', '▁formation', '▁that', '▁just', '▁rese', 'mble', 'd', '▁a', '▁human', '▁head', '▁and', '▁face', ',', '▁but', '▁all', '▁of', '▁it', '▁was', '▁for', 'med', '▁by', '▁shadow', 's', '.', '▁We', '▁only', '▁announced', '▁it', '▁because', '▁we', '▁thought', '▁it', '▁would', '▁be', '▁a', '▁good', '▁way', '▁to', '▁engage', '▁the', '▁public', '▁with', '▁NASA', \"'\", 's', '▁finding', 's', ',', '▁and', '▁at', 'rra', 'ct', '▁attention', '▁to', '▁Mars', '-', '-', '▁and', '▁it', '▁did', '.', '▁The', '▁face', '▁on', '▁Mars', '▁soon', '▁became', '▁a', '▁pop', '▁icon', ';', '▁shot', '▁in', '▁movies', ',', '▁appeared', '▁in', '▁books', ',', '▁magazine', 's', ',', '▁radio', '▁talk', '▁shows', ',', '▁and', '▁ha', 'un', 'ted', '▁gro', 'cer', 'y', '▁store', '▁check', 'out', '▁lines', '▁for', '▁25', '▁years', '.', '▁Some', '▁people', '▁thought', '▁the', '▁natural', '▁land', 'form', '▁was', '▁evidence', '▁of', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁us', '▁scientist', 's', '▁wanted', '▁to', '▁hi', 'de', '▁it', ',', '▁but', '▁really', ',', '▁the', '▁defender', 's', '▁of', '▁the', '▁NASA', '▁budget', '▁wish', '▁there', '▁was', '▁an', 'cient', '▁civiliza', 'tion', '▁on', '▁Mars', '.', '▁We', '▁decided', '▁to', '▁take', '▁another', '▁shot', '▁just', '▁to', '▁make', '▁sure', '▁we', '▁were', 'n', \"'\", 't', '▁wrong', ',', '▁on', '▁April', '▁5', ',', '▁1998.', '▁Michael', '▁Malin', '▁and', '▁his', '▁Mars', '▁Or', 'bit', 'er', '▁camera', '▁team', '▁took', '▁a', '▁picture', '▁that', '▁was', '▁ten', '▁times', '▁sharp', 'er', '▁than', '▁the', '▁original', '▁Viking', '▁photos', ',', '▁reveal', 'ing', '▁a', '▁natural', '▁land', 'form', ',', '▁which', '▁meant', '▁no', '▁alien', '▁monument', '.', '▁\"', 'But', '▁that', '▁picture', '▁wasn', \"'\", 't', '▁very', '▁clear', '▁at', '▁all', ',', '▁which', '▁could', '▁mean', '▁alien', '▁mark', 'ings', '▁were', '▁hidden', '▁by', '▁ha', 'ze', '\"', '▁Well', '▁no', ',', '▁yes', '▁that', '▁rumor', '▁started', ',', '▁but', '▁to', '▁prove', '▁them', '▁wrong', '▁on', '▁April', '▁8', ',', '▁2001', '▁we', '▁decided', '▁to', '▁take', '▁another', '▁picture', ',', '▁making', '▁sure', '▁it', '▁was', '▁a', '▁cloud', 'less', '▁summer', '▁day', '.', '▁Malin', \"'\", 's', '▁team', '▁capture', 'd', '▁an', '▁amazing', '▁photo', '▁using', '▁the', '▁camera', \"'\", 's', '▁absolute', '▁maximum', '▁revolution', '.', '▁With', '▁this', '▁camera', '▁you', '▁can', '▁discern', '▁things', '▁in', '▁a', '▁digital', '▁image', ',', '▁3', '▁times', '▁bigger', '▁than', '▁the', '▁pixel', '▁size', '▁which', '▁means', '▁if', '▁there', '▁were', '▁any', '▁sign', 's', '▁of', '▁life', ',', '▁you', '▁could', '▁easily', '▁see', '▁what', '▁they', '▁were', '.', '▁What', '▁the', '▁picture', '▁showed', '▁was', '▁the', '▁but', 'te', '▁or', '▁mesa', ',', '▁which', '▁are', '▁land', 'form', 's', '▁common', '▁around', '▁the', '▁American', '▁West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[  'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'I-LOC',\n",
    " 'O'\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "e10a554b-5413-4438-814f-a832f845a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'MISC': {'precision': 0.625, 'recall': 0.9090909090909091, 'f1': 0.7407407407407406, 'number': 11}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'PER': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1': 0.8, 'number': 2}, 'overall_precision': 0.6666666666666666, 'overall_recall': 0.7777777777777778, 'overall_f1': 0.717948717948718, 'overall_accuracy': 0.973630831643002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cef19-f43f-4108-bbd1-3ff6e5748dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f34e36ec-8d26-49b4-89a5-15eaa203e168",
   "metadata": {},
   "source": [
    "## 15  jplu/tf-xlm-r-ner-40-lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d5376b5a-3cb7-4417-b2bf-d67e574d63f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "jplu/tf-xlm-r-ner-40-lang does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[335], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjplu/tf-xlm-r-ner-40-lang\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjplu/tf-xlm-r-ner-40-lang\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m      6\u001b[0m salida\u001b[38;5;241m=\u001b[39mclassifier(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3590\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3582\u001b[0m has_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m: revision,\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxies\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxies,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3587\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_files_only,\n\u001b[0;32m   3588\u001b[0m }\n\u001b[0;32m   3589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m-> 3590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   3591\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3592\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for TensorFlow weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3593\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3594\u001b[0m     )\n\u001b[0;32m   3595\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[0;32m   3596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   3597\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3598\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for Flax weights. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3599\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3600\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: jplu/tf-xlm-r-ner-40-lang does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-xlm-r-ner-40-lang\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"jplu/tf-xlm-r-ner-40-lang\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca4cd2-59bd-4d85-8388-3fbe79154757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3e28d-dfba-4507-98d3-24039cec98be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "a6a0c627-2ac4-48f0-8870-fd1a2479245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at jplu/tf-xlm-r-ner-40-lang were not used when initializing TFXLMRobertaForTokenClassification: ['dropout_38']\n",
      "- This IS expected if you are initializing TFXLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFXLMRobertaForTokenClassification were initialized from the model checkpoint at jplu/tf-xlm-r-ner-40-lang.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForTokenClassification for predictions without further training.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PER', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PER', 8: 'O'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"jplu/tf-xlm-r-ner-40-lang\",\n",
    "    tokenizer=(\n",
    "        'jplu/tf-xlm-r-ner-40-lang'),\n",
    "    framework=\"tf\"\n",
    ")\n",
    "\n",
    "a=nlp_ner(text)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f5c47e8f-d56c-4110-8909-a227ff2a7fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-LOC', 'O']\n",
      "['▁So', ',', '▁if', '▁you', \"'\", 're', '▁a', '▁NASA', '▁scientist', ',', '▁you', '▁should', '▁be', '▁able', '▁to', '▁tell', '▁me', '▁the', '▁whole', '▁story', '▁about', '▁the', '▁Face', '▁On', '▁Mars', ',', '▁which', '▁obviously', '▁is', '▁evidence', '▁that', '▁there', '▁is', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁the', '▁face', '▁was', '▁created', '▁by', '▁alien', 's', ',', '▁correct', '?\"', '▁No', ',', '▁twenty', '▁five', '▁years', '▁ago', ',', '▁our', '▁Viking', '▁1', '▁space', 'craft', '▁was', '▁circ', 'ling', '▁the', '▁planet', ',', '▁sna', 'pping', '▁photos', ',', '▁when', '▁it', '▁spot', 'ted', '▁the', '▁shadow', 'y', '▁like', 'ness', '▁of', '▁a', '▁human', '▁face', '.', '▁Us', '▁scientist', 's', '▁figure', 'd', '▁out', '▁that', '▁it', '▁was', '▁just', '▁another', '▁Marti', 'an', '▁mesa', ',', '▁common', '▁around', '▁Cy', 'do', 'nia', ',', '▁only', '▁this', '▁one', '▁had', '▁shadow', 's', '▁that', '▁made', '▁it', '▁look', '▁like', '▁an', '▁Egypt', 'ion', '▁Phar', 'a', 'oh', '.', '▁Very', '▁few', '▁days', '▁later', ',', '▁we', '▁reveal', 'ed', '▁the', '▁image', '▁for', '▁all', '▁to', '▁see', ',', '▁and', '▁we', '▁made', '▁sure', '▁to', '▁note', '▁that', '▁it', '▁was', '▁a', '▁huge', '▁rock', '▁formation', '▁that', '▁just', '▁rese', 'mble', 'd', '▁a', '▁human', '▁head', '▁and', '▁face', ',', '▁but', '▁all', '▁of', '▁it', '▁was', '▁for', 'med', '▁by', '▁shadow', 's', '.', '▁We', '▁only', '▁announced', '▁it', '▁because', '▁we', '▁thought', '▁it', '▁would', '▁be', '▁a', '▁good', '▁way', '▁to', '▁engage', '▁the', '▁public', '▁with', '▁NASA', \"'\", 's', '▁finding', 's', ',', '▁and', '▁at', 'rra', 'ct', '▁attention', '▁to', '▁Mars', '-', '-', '▁and', '▁it', '▁did', '.', '▁The', '▁face', '▁on', '▁Mars', '▁soon', '▁became', '▁a', '▁pop', '▁icon', ';', '▁shot', '▁in', '▁movies', ',', '▁appeared', '▁in', '▁books', ',', '▁magazine', 's', ',', '▁radio', '▁talk', '▁shows', ',', '▁and', '▁ha', 'un', 'ted', '▁gro', 'cer', 'y', '▁store', '▁check', 'out', '▁lines', '▁for', '▁25', '▁years', '.', '▁Some', '▁people', '▁thought', '▁the', '▁natural', '▁land', 'form', '▁was', '▁evidence', '▁of', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁us', '▁scientist', 's', '▁wanted', '▁to', '▁hi', 'de', '▁it', ',', '▁but', '▁really', ',', '▁the', '▁defender', 's', '▁of', '▁the', '▁NASA', '▁budget', '▁wish', '▁there', '▁was', '▁an', 'cient', '▁civiliza', 'tion', '▁on', '▁Mars', '.', '▁We', '▁decided', '▁to', '▁take', '▁another', '▁shot', '▁just', '▁to', '▁make', '▁sure', '▁we', '▁were', 'n', \"'\", 't', '▁wrong', ',', '▁on', '▁April', '▁5', ',', '▁1998.', '▁Michael', '▁Malin', '▁and', '▁his', '▁Mars', '▁Or', 'bit', 'er', '▁camera', '▁team', '▁took', '▁a', '▁picture', '▁that', '▁was', '▁ten', '▁times', '▁sharp', 'er', '▁than', '▁the', '▁original', '▁Viking', '▁photos', ',', '▁reveal', 'ing', '▁a', '▁natural', '▁land', 'form', ',', '▁which', '▁meant', '▁no', '▁alien', '▁monument', '.', '▁\"', 'But', '▁that', '▁picture', '▁wasn', \"'\", 't', '▁very', '▁clear', '▁at', '▁all', ',', '▁which', '▁could', '▁mean', '▁alien', '▁mark', 'ings', '▁were', '▁hidden', '▁by', '▁ha', 'ze', '\"', '▁Well', '▁no', ',', '▁yes', '▁that', '▁rumor', '▁started', ',', '▁but', '▁to', '▁prove', '▁them', '▁wrong', '▁on', '▁April', '▁8', ',', '▁2001', '▁we', '▁decided', '▁to', '▁take', '▁another', '▁picture', ',', '▁making', '▁sure', '▁it', '▁was', '▁a', '▁cloud', 'less', '▁summer', '▁day', '.', '▁Malin', \"'\", 's', '▁team', '▁capture', 'd', '▁an', '▁amazing', '▁photo', '▁using', '▁the', '▁camera', \"'\", 's', '▁absolute', '▁maximum', '▁revolution', '.', '▁With', '▁this', '▁camera', '▁you', '▁can', '▁discern', '▁things', '▁in', '▁a', '▁digital', '▁image', ',', '▁3', '▁times', '▁bigger', '▁than', '▁the', '▁pixel', '▁size', '▁which', '▁means', '▁if', '▁there', '▁were', '▁any', '▁sign', 's', '▁of', '▁life', ',', '▁you', '▁could', '▁easily', '▁see', '▁what', '▁they', '▁were', '.', '▁What', '▁the', '▁picture', '▁showed', '▁was', '▁the', '▁but', 'te', '▁or', '▁mesa', ',', '▁which', '▁are', '▁land', 'form', 's', '▁common', '▁around', '▁the', '▁American', '▁West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[ 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-MISC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "c3e108cf-b832-45a8-a3f0-fb6e037727de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'MISC': {'precision': 0.5625, 'recall': 0.9, 'f1': 0.6923076923076923, 'number': 10}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'PER': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1': 0.8, 'number': 2}, 'overall_precision': 0.6190476190476191, 'overall_recall': 0.7647058823529411, 'overall_f1': 0.6842105263157895, 'overall_accuracy': 0.9695740365111561}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9092a-d4c6-4365-9d73-205ea982bde6",
   "metadata": {},
   "source": [
    "## 16 sagorsarker/codeswitch-spaeng-lid-lince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "26547547-96a2-4cd1-a28f-d4b4c5ace9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sagorsarker/codeswitch-spaeng-lid-lince were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'en', 'score': 0.9998629, 'index': 1, 'word': 'So', 'start': 0, 'end': 2}, {'entity': 'other', 'score': 0.9999267, 'index': 2, 'word': ',', 'start': 2, 'end': 3}, {'entity': 'en', 'score': 0.99985707, 'index': 3, 'word': 'if', 'start': 4, 'end': 6}, {'entity': 'en', 'score': 0.99984396, 'index': 4, 'word': 'you', 'start': 7, 'end': 10}, {'entity': 'en', 'score': 0.9998392, 'index': 5, 'word': \"'\", 'start': 10, 'end': 11}, {'entity': 'en', 'score': 0.9998178, 'index': 6, 'word': 're', 'start': 11, 'end': 13}, {'entity': 'en', 'score': 0.99961334, 'index': 7, 'word': 'a', 'start': 14, 'end': 15}, {'entity': 'ne', 'score': 0.99565876, 'index': 8, 'word': 'NASA', 'start': 16, 'end': 20}, {'entity': 'en', 'score': 0.9997341, 'index': 9, 'word': 'scientist', 'start': 21, 'end': 30}, {'entity': 'other', 'score': 0.99992585, 'index': 10, 'word': ',', 'start': 30, 'end': 31}, {'entity': 'en', 'score': 0.99986625, 'index': 11, 'word': 'you', 'start': 32, 'end': 35}, {'entity': 'en', 'score': 0.99986553, 'index': 12, 'word': 'should', 'start': 36, 'end': 42}, {'entity': 'en', 'score': 0.9998518, 'index': 13, 'word': 'be', 'start': 43, 'end': 45}, {'entity': 'en', 'score': 0.9998604, 'index': 14, 'word': 'able', 'start': 46, 'end': 50}, {'entity': 'en', 'score': 0.99985576, 'index': 15, 'word': 'to', 'start': 51, 'end': 53}, {'entity': 'en', 'score': 0.999874, 'index': 16, 'word': 'tell', 'start': 54, 'end': 58}, {'entity': 'en', 'score': 0.9998467, 'index': 17, 'word': 'me', 'start': 59, 'end': 61}, {'entity': 'en', 'score': 0.99984443, 'index': 18, 'word': 'the', 'start': 62, 'end': 65}, {'entity': 'en', 'score': 0.99985886, 'index': 19, 'word': 'whole', 'start': 66, 'end': 71}, {'entity': 'en', 'score': 0.9998666, 'index': 20, 'word': 'story', 'start': 72, 'end': 77}, {'entity': 'en', 'score': 0.99987817, 'index': 21, 'word': 'about', 'start': 78, 'end': 83}, {'entity': 'en', 'score': 0.99987066, 'index': 22, 'word': 'the', 'start': 84, 'end': 87}, {'entity': 'en', 'score': 0.9998729, 'index': 23, 'word': 'Face', 'start': 88, 'end': 92}, {'entity': 'en', 'score': 0.9998679, 'index': 24, 'word': 'On', 'start': 93, 'end': 95}, {'entity': 'ne', 'score': 0.9943815, 'index': 25, 'word': 'Mars', 'start': 96, 'end': 100}, {'entity': 'other', 'score': 0.9999286, 'index': 26, 'word': ',', 'start': 100, 'end': 101}, {'entity': 'en', 'score': 0.9998467, 'index': 27, 'word': 'which', 'start': 102, 'end': 107}, {'entity': 'en', 'score': 0.9998258, 'index': 28, 'word': 'obvious', 'start': 108, 'end': 115}, {'entity': 'en', 'score': 0.99984264, 'index': 29, 'word': '##ly', 'start': 115, 'end': 117}, {'entity': 'en', 'score': 0.9998216, 'index': 30, 'word': 'is', 'start': 118, 'end': 120}, {'entity': 'en', 'score': 0.9998173, 'index': 31, 'word': 'evidence', 'start': 121, 'end': 129}, {'entity': 'en', 'score': 0.9998375, 'index': 32, 'word': 'that', 'start': 130, 'end': 134}, {'entity': 'en', 'score': 0.9998186, 'index': 33, 'word': 'there', 'start': 135, 'end': 140}, {'entity': 'en', 'score': 0.99982053, 'index': 34, 'word': 'is', 'start': 141, 'end': 143}, {'entity': 'en', 'score': 0.99982506, 'index': 35, 'word': 'life', 'start': 144, 'end': 148}, {'entity': 'en', 'score': 0.99981946, 'index': 36, 'word': 'on', 'start': 149, 'end': 151}, {'entity': 'ne', 'score': 0.99421823, 'index': 37, 'word': 'Mars', 'start': 152, 'end': 156}, {'entity': 'other', 'score': 0.9999292, 'index': 38, 'word': ',', 'start': 156, 'end': 157}, {'entity': 'en', 'score': 0.99983776, 'index': 39, 'word': 'and', 'start': 158, 'end': 161}, {'entity': 'en', 'score': 0.9998344, 'index': 40, 'word': 'that', 'start': 162, 'end': 166}, {'entity': 'en', 'score': 0.9997547, 'index': 41, 'word': 'the', 'start': 167, 'end': 170}, {'entity': 'en', 'score': 0.99978274, 'index': 42, 'word': 'face', 'start': 171, 'end': 175}, {'entity': 'en', 'score': 0.9997836, 'index': 43, 'word': 'was', 'start': 176, 'end': 179}, {'entity': 'en', 'score': 0.99971956, 'index': 44, 'word': 'created', 'start': 180, 'end': 187}, {'entity': 'en', 'score': 0.99975187, 'index': 45, 'word': 'by', 'start': 188, 'end': 190}, {'entity': 'en', 'score': 0.9997137, 'index': 46, 'word': 'alien', 'start': 191, 'end': 196}, {'entity': 'en', 'score': 0.99977976, 'index': 47, 'word': '##s', 'start': 196, 'end': 197}, {'entity': 'other', 'score': 0.9999249, 'index': 48, 'word': ',', 'start': 197, 'end': 198}, {'entity': 'en', 'score': 0.99981195, 'index': 49, 'word': 'correct', 'start': 199, 'end': 206}, {'entity': 'other', 'score': 0.9999312, 'index': 50, 'word': '?', 'start': 206, 'end': 207}, {'entity': 'other', 'score': 0.9999089, 'index': 51, 'word': '\"', 'start': 207, 'end': 208}, {'entity': 'en', 'score': 0.9864689, 'index': 52, 'word': 'No', 'start': 209, 'end': 211}, {'entity': 'other', 'score': 0.99993014, 'index': 53, 'word': ',', 'start': 211, 'end': 212}, {'entity': 'en', 'score': 0.9997601, 'index': 54, 'word': 'twenty', 'start': 213, 'end': 219}, {'entity': 'en', 'score': 0.9996724, 'index': 55, 'word': 'five', 'start': 220, 'end': 224}, {'entity': 'en', 'score': 0.99973506, 'index': 56, 'word': 'years', 'start': 225, 'end': 230}, {'entity': 'en', 'score': 0.9997675, 'index': 57, 'word': 'ago', 'start': 231, 'end': 234}, {'entity': 'other', 'score': 0.9999273, 'index': 58, 'word': ',', 'start': 234, 'end': 235}, {'entity': 'en', 'score': 0.9992092, 'index': 59, 'word': 'our', 'start': 236, 'end': 239}, {'entity': 'ne', 'score': 0.62706536, 'index': 60, 'word': 'Viking', 'start': 240, 'end': 246}, {'entity': 'other', 'score': 0.9996927, 'index': 61, 'word': '1', 'start': 247, 'end': 248}, {'entity': 'en', 'score': 0.9995969, 'index': 62, 'word': 'spacecraft', 'start': 249, 'end': 259}, {'entity': 'en', 'score': 0.9997328, 'index': 63, 'word': 'was', 'start': 260, 'end': 263}, {'entity': 'en', 'score': 0.9997286, 'index': 64, 'word': 'ci', 'start': 264, 'end': 266}, {'entity': 'en', 'score': 0.9997693, 'index': 65, 'word': '##rc', 'start': 266, 'end': 268}, {'entity': 'en', 'score': 0.99981683, 'index': 66, 'word': '##ling', 'start': 268, 'end': 272}, {'entity': 'en', 'score': 0.99974436, 'index': 67, 'word': 'the', 'start': 273, 'end': 276}, {'entity': 'en', 'score': 0.999772, 'index': 68, 'word': 'planet', 'start': 277, 'end': 283}, {'entity': 'other', 'score': 0.9999294, 'index': 69, 'word': ',', 'start': 283, 'end': 284}, {'entity': 'en', 'score': 0.9995789, 'index': 70, 'word': 'sna', 'start': 285, 'end': 288}, {'entity': 'en', 'score': 0.9996741, 'index': 71, 'word': '##pping', 'start': 288, 'end': 293}, {'entity': 'en', 'score': 0.99957246, 'index': 72, 'word': 'photos', 'start': 294, 'end': 300}, {'entity': 'other', 'score': 0.99993145, 'index': 73, 'word': ',', 'start': 300, 'end': 301}, {'entity': 'en', 'score': 0.9997521, 'index': 74, 'word': 'when', 'start': 302, 'end': 306}, {'entity': 'en', 'score': 0.99976677, 'index': 75, 'word': 'it', 'start': 307, 'end': 309}, {'entity': 'en', 'score': 0.99971706, 'index': 76, 'word': 'spotted', 'start': 310, 'end': 317}, {'entity': 'en', 'score': 0.99961495, 'index': 77, 'word': 'the', 'start': 318, 'end': 321}, {'entity': 'en', 'score': 0.9996594, 'index': 78, 'word': 'sh', 'start': 322, 'end': 324}, {'entity': 'en', 'score': 0.99976844, 'index': 79, 'word': '##adow', 'start': 324, 'end': 328}, {'entity': 'en', 'score': 0.9997464, 'index': 80, 'word': '##y', 'start': 328, 'end': 329}, {'entity': 'en', 'score': 0.9997322, 'index': 81, 'word': 'like', 'start': 330, 'end': 334}, {'entity': 'en', 'score': 0.99969065, 'index': 82, 'word': '##ness', 'start': 334, 'end': 338}, {'entity': 'en', 'score': 0.9996531, 'index': 83, 'word': 'of', 'start': 339, 'end': 341}, {'entity': 'en', 'score': 0.9992725, 'index': 84, 'word': 'a', 'start': 342, 'end': 343}, {'entity': 'en', 'score': 0.99959975, 'index': 85, 'word': 'human', 'start': 344, 'end': 349}, {'entity': 'en', 'score': 0.99973136, 'index': 86, 'word': 'face', 'start': 350, 'end': 354}, {'entity': 'other', 'score': 0.9999311, 'index': 87, 'word': '.', 'start': 354, 'end': 355}, {'entity': 'en', 'score': 0.9996131, 'index': 88, 'word': 'Us', 'start': 356, 'end': 358}, {'entity': 'en', 'score': 0.9996656, 'index': 89, 'word': 'scientists', 'start': 359, 'end': 369}, {'entity': 'en', 'score': 0.99980336, 'index': 90, 'word': 'figure', 'start': 370, 'end': 376}, {'entity': 'en', 'score': 0.999795, 'index': 91, 'word': '##d', 'start': 376, 'end': 377}, {'entity': 'en', 'score': 0.99979943, 'index': 92, 'word': 'out', 'start': 378, 'end': 381}, {'entity': 'en', 'score': 0.9997974, 'index': 93, 'word': 'that', 'start': 382, 'end': 386}, {'entity': 'en', 'score': 0.99973387, 'index': 94, 'word': 'it', 'start': 387, 'end': 389}, {'entity': 'en', 'score': 0.9997162, 'index': 95, 'word': 'was', 'start': 390, 'end': 393}, {'entity': 'en', 'score': 0.9996761, 'index': 96, 'word': 'just', 'start': 394, 'end': 398}, {'entity': 'en', 'score': 0.9995012, 'index': 97, 'word': 'another', 'start': 399, 'end': 406}, {'entity': 'en', 'score': 0.92382264, 'index': 98, 'word': 'Mart', 'start': 407, 'end': 411}, {'entity': 'en', 'score': 0.9973562, 'index': 99, 'word': '##ian', 'start': 411, 'end': 414}, {'entity': 'en', 'score': 0.7907492, 'index': 100, 'word': 'mesa', 'start': 415, 'end': 419}, {'entity': 'other', 'score': 0.9999194, 'index': 101, 'word': ',', 'start': 419, 'end': 420}, {'entity': 'en', 'score': 0.9995608, 'index': 102, 'word': 'common', 'start': 421, 'end': 427}, {'entity': 'en', 'score': 0.9996966, 'index': 103, 'word': 'around', 'start': 428, 'end': 434}, {'entity': 'ne', 'score': 0.9975068, 'index': 104, 'word': 'C', 'start': 435, 'end': 436}, {'entity': 'ne', 'score': 0.9957877, 'index': 105, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'ne', 'score': 0.9961337, 'index': 106, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'other', 'score': 0.9999093, 'index': 107, 'word': ',', 'start': 442, 'end': 443}, {'entity': 'en', 'score': 0.99976605, 'index': 108, 'word': 'only', 'start': 444, 'end': 448}, {'entity': 'en', 'score': 0.9997769, 'index': 109, 'word': 'this', 'start': 449, 'end': 453}, {'entity': 'en', 'score': 0.99976176, 'index': 110, 'word': 'one', 'start': 454, 'end': 457}, {'entity': 'en', 'score': 0.9997123, 'index': 111, 'word': 'had', 'start': 458, 'end': 461}, {'entity': 'en', 'score': 0.9997372, 'index': 112, 'word': 'sh', 'start': 462, 'end': 464}, {'entity': 'en', 'score': 0.9998115, 'index': 113, 'word': '##adow', 'start': 464, 'end': 468}, {'entity': 'en', 'score': 0.99980253, 'index': 114, 'word': '##s', 'start': 468, 'end': 469}, {'entity': 'en', 'score': 0.99979705, 'index': 115, 'word': 'that', 'start': 470, 'end': 474}, {'entity': 'en', 'score': 0.99979264, 'index': 116, 'word': 'made', 'start': 475, 'end': 479}, {'entity': 'en', 'score': 0.9998178, 'index': 117, 'word': 'it', 'start': 480, 'end': 482}, {'entity': 'en', 'score': 0.99979025, 'index': 118, 'word': 'look', 'start': 483, 'end': 487}, {'entity': 'en', 'score': 0.99977773, 'index': 119, 'word': 'like', 'start': 488, 'end': 492}, {'entity': 'en', 'score': 0.99962866, 'index': 120, 'word': 'an', 'start': 493, 'end': 495}, {'entity': 'ne', 'score': 0.68299395, 'index': 121, 'word': 'Egypt', 'start': 496, 'end': 501}, {'entity': 'en', 'score': 0.9760886, 'index': 122, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'en', 'score': 0.9997099, 'index': 123, 'word': 'Ph', 'start': 505, 'end': 507}, {'entity': 'en', 'score': 0.9998056, 'index': 124, 'word': '##ara', 'start': 507, 'end': 510}, {'entity': 'en', 'score': 0.99973387, 'index': 125, 'word': '##oh', 'start': 510, 'end': 512}, {'entity': 'other', 'score': 0.9999268, 'index': 126, 'word': '.', 'start': 512, 'end': 513}, {'entity': 'en', 'score': 0.99796546, 'index': 127, 'word': 'Very', 'start': 514, 'end': 518}, {'entity': 'en', 'score': 0.9990651, 'index': 128, 'word': 'few', 'start': 519, 'end': 522}, {'entity': 'en', 'score': 0.9994499, 'index': 129, 'word': 'days', 'start': 523, 'end': 527}, {'entity': 'en', 'score': 0.9995864, 'index': 130, 'word': 'later', 'start': 528, 'end': 533}, {'entity': 'other', 'score': 0.99990845, 'index': 131, 'word': ',', 'start': 533, 'end': 534}, {'entity': 'en', 'score': 0.99973947, 'index': 132, 'word': 'we', 'start': 535, 'end': 537}, {'entity': 'en', 'score': 0.99979526, 'index': 133, 'word': 'revealed', 'start': 538, 'end': 546}, {'entity': 'en', 'score': 0.9997683, 'index': 134, 'word': 'the', 'start': 547, 'end': 550}, {'entity': 'en', 'score': 0.999793, 'index': 135, 'word': 'image', 'start': 551, 'end': 556}, {'entity': 'en', 'score': 0.99985754, 'index': 136, 'word': 'for', 'start': 557, 'end': 560}, {'entity': 'en', 'score': 0.999843, 'index': 137, 'word': 'all', 'start': 561, 'end': 564}, {'entity': 'en', 'score': 0.99983895, 'index': 138, 'word': 'to', 'start': 565, 'end': 567}, {'entity': 'en', 'score': 0.9998221, 'index': 139, 'word': 'see', 'start': 568, 'end': 571}, {'entity': 'other', 'score': 0.99992704, 'index': 140, 'word': ',', 'start': 571, 'end': 572}, {'entity': 'en', 'score': 0.9997582, 'index': 141, 'word': 'and', 'start': 573, 'end': 576}, {'entity': 'en', 'score': 0.99974746, 'index': 142, 'word': 'we', 'start': 577, 'end': 579}, {'entity': 'en', 'score': 0.9997589, 'index': 143, 'word': 'made', 'start': 580, 'end': 584}, {'entity': 'en', 'score': 0.9997596, 'index': 144, 'word': 'sure', 'start': 585, 'end': 589}, {'entity': 'en', 'score': 0.9997719, 'index': 145, 'word': 'to', 'start': 590, 'end': 592}, {'entity': 'en', 'score': 0.99976, 'index': 146, 'word': 'note', 'start': 593, 'end': 597}, {'entity': 'en', 'score': 0.9997385, 'index': 147, 'word': 'that', 'start': 598, 'end': 602}, {'entity': 'en', 'score': 0.999699, 'index': 148, 'word': 'it', 'start': 603, 'end': 605}, {'entity': 'en', 'score': 0.9996177, 'index': 149, 'word': 'was', 'start': 606, 'end': 609}, {'entity': 'en', 'score': 0.99906355, 'index': 150, 'word': 'a', 'start': 610, 'end': 611}, {'entity': 'en', 'score': 0.9993754, 'index': 151, 'word': 'huge', 'start': 612, 'end': 616}, {'entity': 'en', 'score': 0.99958175, 'index': 152, 'word': 'rock', 'start': 617, 'end': 621}, {'entity': 'en', 'score': 0.9996152, 'index': 153, 'word': 'formation', 'start': 622, 'end': 631}, {'entity': 'en', 'score': 0.9995758, 'index': 154, 'word': 'that', 'start': 632, 'end': 636}, {'entity': 'en', 'score': 0.9995844, 'index': 155, 'word': 'just', 'start': 637, 'end': 641}, {'entity': 'en', 'score': 0.99950886, 'index': 156, 'word': 'res', 'start': 642, 'end': 645}, {'entity': 'en', 'score': 0.9995715, 'index': 157, 'word': '##emble', 'start': 645, 'end': 650}, {'entity': 'en', 'score': 0.99958056, 'index': 158, 'word': '##d', 'start': 650, 'end': 651}, {'entity': 'en', 'score': 0.99861777, 'index': 159, 'word': 'a', 'start': 652, 'end': 653}, {'entity': 'en', 'score': 0.99934405, 'index': 160, 'word': 'human', 'start': 654, 'end': 659}, {'entity': 'en', 'score': 0.9995927, 'index': 161, 'word': 'head', 'start': 660, 'end': 664}, {'entity': 'en', 'score': 0.9994898, 'index': 162, 'word': 'and', 'start': 665, 'end': 668}, {'entity': 'en', 'score': 0.9995479, 'index': 163, 'word': 'face', 'start': 669, 'end': 673}, {'entity': 'other', 'score': 0.9999279, 'index': 164, 'word': ',', 'start': 673, 'end': 674}, {'entity': 'en', 'score': 0.9997607, 'index': 165, 'word': 'but', 'start': 675, 'end': 678}, {'entity': 'en', 'score': 0.99969375, 'index': 166, 'word': 'all', 'start': 679, 'end': 682}, {'entity': 'en', 'score': 0.99976605, 'index': 167, 'word': 'of', 'start': 683, 'end': 685}, {'entity': 'en', 'score': 0.99975795, 'index': 168, 'word': 'it', 'start': 686, 'end': 688}, {'entity': 'en', 'score': 0.99969816, 'index': 169, 'word': 'was', 'start': 689, 'end': 692}, {'entity': 'en', 'score': 0.9996636, 'index': 170, 'word': 'formed', 'start': 693, 'end': 699}, {'entity': 'en', 'score': 0.9995894, 'index': 171, 'word': 'by', 'start': 700, 'end': 702}, {'entity': 'en', 'score': 0.9996039, 'index': 172, 'word': 'sh', 'start': 703, 'end': 705}, {'entity': 'en', 'score': 0.9997403, 'index': 173, 'word': '##adow', 'start': 705, 'end': 709}, {'entity': 'en', 'score': 0.9997271, 'index': 174, 'word': '##s', 'start': 709, 'end': 710}, {'entity': 'other', 'score': 0.9999298, 'index': 175, 'word': '.', 'start': 710, 'end': 711}, {'entity': 'en', 'score': 0.9996859, 'index': 176, 'word': 'We', 'start': 712, 'end': 714}, {'entity': 'en', 'score': 0.99971765, 'index': 177, 'word': 'only', 'start': 715, 'end': 719}, {'entity': 'en', 'score': 0.99975497, 'index': 178, 'word': 'announced', 'start': 720, 'end': 729}, {'entity': 'en', 'score': 0.9997633, 'index': 179, 'word': 'it', 'start': 730, 'end': 732}, {'entity': 'en', 'score': 0.9997826, 'index': 180, 'word': 'because', 'start': 733, 'end': 740}, {'entity': 'en', 'score': 0.99974495, 'index': 181, 'word': 'we', 'start': 741, 'end': 743}, {'entity': 'en', 'score': 0.9997533, 'index': 182, 'word': 'thought', 'start': 744, 'end': 751}, {'entity': 'en', 'score': 0.9997551, 'index': 183, 'word': 'it', 'start': 752, 'end': 754}, {'entity': 'en', 'score': 0.99974245, 'index': 184, 'word': 'would', 'start': 755, 'end': 760}, {'entity': 'en', 'score': 0.9996723, 'index': 185, 'word': 'be', 'start': 761, 'end': 763}, {'entity': 'en', 'score': 0.9992391, 'index': 186, 'word': 'a', 'start': 764, 'end': 765}, {'entity': 'en', 'score': 0.9996873, 'index': 187, 'word': 'good', 'start': 766, 'end': 770}, {'entity': 'en', 'score': 0.9997737, 'index': 188, 'word': 'way', 'start': 771, 'end': 774}, {'entity': 'en', 'score': 0.99975866, 'index': 189, 'word': 'to', 'start': 775, 'end': 777}, {'entity': 'en', 'score': 0.99975985, 'index': 190, 'word': 'engage', 'start': 778, 'end': 784}, {'entity': 'en', 'score': 0.9996723, 'index': 191, 'word': 'the', 'start': 785, 'end': 788}, {'entity': 'en', 'score': 0.9997609, 'index': 192, 'word': 'public', 'start': 789, 'end': 795}, {'entity': 'en', 'score': 0.9997042, 'index': 193, 'word': 'with', 'start': 796, 'end': 800}, {'entity': 'ne', 'score': 0.99519366, 'index': 194, 'word': 'NASA', 'start': 801, 'end': 805}, {'entity': 'en', 'score': 0.99966383, 'index': 195, 'word': \"'\", 'start': 805, 'end': 806}, {'entity': 'en', 'score': 0.99948585, 'index': 196, 'word': 's', 'start': 806, 'end': 807}, {'entity': 'en', 'score': 0.9994748, 'index': 197, 'word': 'findings', 'start': 808, 'end': 816}, {'entity': 'other', 'score': 0.99991834, 'index': 198, 'word': ',', 'start': 816, 'end': 817}, {'entity': 'en', 'score': 0.99959546, 'index': 199, 'word': 'and', 'start': 818, 'end': 821}, {'entity': 'en', 'score': 0.9996618, 'index': 200, 'word': 'at', 'start': 822, 'end': 824}, {'entity': 'en', 'score': 0.99977297, 'index': 201, 'word': '##rra', 'start': 824, 'end': 827}, {'entity': 'en', 'score': 0.9997421, 'index': 202, 'word': '##ct', 'start': 827, 'end': 829}, {'entity': 'en', 'score': 0.9997882, 'index': 203, 'word': 'attention', 'start': 830, 'end': 839}, {'entity': 'en', 'score': 0.99970275, 'index': 204, 'word': 'to', 'start': 840, 'end': 842}, {'entity': 'ne', 'score': 0.9933374, 'index': 205, 'word': 'Mars', 'start': 843, 'end': 847}, {'entity': 'other', 'score': 0.99984145, 'index': 206, 'word': '-', 'start': 847, 'end': 848}, {'entity': 'other', 'score': 0.9995536, 'index': 207, 'word': '-', 'start': 848, 'end': 849}, {'entity': 'en', 'score': 0.999739, 'index': 208, 'word': 'and', 'start': 850, 'end': 853}, {'entity': 'en', 'score': 0.9997385, 'index': 209, 'word': 'it', 'start': 854, 'end': 856}, {'entity': 'en', 'score': 0.9997167, 'index': 210, 'word': 'did', 'start': 857, 'end': 860}, {'entity': 'other', 'score': 0.99991965, 'index': 211, 'word': '.', 'start': 860, 'end': 861}, {'entity': 'en', 'score': 0.9994844, 'index': 212, 'word': 'The', 'start': 862, 'end': 865}, {'entity': 'en', 'score': 0.9995809, 'index': 213, 'word': 'face', 'start': 866, 'end': 870}, {'entity': 'en', 'score': 0.9993393, 'index': 214, 'word': 'on', 'start': 871, 'end': 873}, {'entity': 'ne', 'score': 0.9903474, 'index': 215, 'word': 'Mars', 'start': 874, 'end': 878}, {'entity': 'en', 'score': 0.99960655, 'index': 216, 'word': 'soon', 'start': 879, 'end': 883}, {'entity': 'en', 'score': 0.9992306, 'index': 217, 'word': 'became', 'start': 884, 'end': 890}, {'entity': 'en', 'score': 0.9976654, 'index': 218, 'word': 'a', 'start': 891, 'end': 892}, {'entity': 'en', 'score': 0.99933064, 'index': 219, 'word': 'pop', 'start': 893, 'end': 896}, {'entity': 'en', 'score': 0.99950266, 'index': 220, 'word': 'i', 'start': 897, 'end': 898}, {'entity': 'en', 'score': 0.99954295, 'index': 221, 'word': '##con', 'start': 898, 'end': 901}, {'entity': 'other', 'score': 0.99992657, 'index': 222, 'word': ';', 'start': 901, 'end': 902}, {'entity': 'en', 'score': 0.9994814, 'index': 223, 'word': 'shot', 'start': 903, 'end': 907}, {'entity': 'en', 'score': 0.9994511, 'index': 224, 'word': 'in', 'start': 908, 'end': 910}, {'entity': 'en', 'score': 0.9992155, 'index': 225, 'word': 'movies', 'start': 911, 'end': 917}, {'entity': 'other', 'score': 0.99992335, 'index': 226, 'word': ',', 'start': 917, 'end': 918}, {'entity': 'en', 'score': 0.9993299, 'index': 227, 'word': 'appeared', 'start': 919, 'end': 927}, {'entity': 'en', 'score': 0.9994456, 'index': 228, 'word': 'in', 'start': 928, 'end': 930}, {'entity': 'en', 'score': 0.9991399, 'index': 229, 'word': 'books', 'start': 931, 'end': 936}, {'entity': 'other', 'score': 0.9999262, 'index': 230, 'word': ',', 'start': 936, 'end': 937}, {'entity': 'en', 'score': 0.99925786, 'index': 231, 'word': 'magazines', 'start': 938, 'end': 947}, {'entity': 'other', 'score': 0.99992716, 'index': 232, 'word': ',', 'start': 947, 'end': 948}, {'entity': 'en', 'score': 0.99942744, 'index': 233, 'word': 'radio', 'start': 949, 'end': 954}, {'entity': 'en', 'score': 0.9996457, 'index': 234, 'word': 'talk', 'start': 955, 'end': 959}, {'entity': 'en', 'score': 0.9996043, 'index': 235, 'word': 'shows', 'start': 960, 'end': 965}, {'entity': 'other', 'score': 0.9999255, 'index': 236, 'word': ',', 'start': 965, 'end': 966}, {'entity': 'en', 'score': 0.99946564, 'index': 237, 'word': 'and', 'start': 967, 'end': 970}, {'entity': 'en', 'score': 0.9997336, 'index': 238, 'word': 'hau', 'start': 971, 'end': 974}, {'entity': 'en', 'score': 0.99972874, 'index': 239, 'word': '##nted', 'start': 974, 'end': 978}, {'entity': 'en', 'score': 0.9996462, 'index': 240, 'word': 'gr', 'start': 979, 'end': 981}, {'entity': 'en', 'score': 0.9997458, 'index': 241, 'word': '##oce', 'start': 981, 'end': 984}, {'entity': 'en', 'score': 0.9996829, 'index': 242, 'word': '##ry', 'start': 984, 'end': 986}, {'entity': 'en', 'score': 0.99970573, 'index': 243, 'word': 'store', 'start': 987, 'end': 992}, {'entity': 'en', 'score': 0.9997999, 'index': 244, 'word': 'check', 'start': 993, 'end': 998}, {'entity': 'en', 'score': 0.9997851, 'index': 245, 'word': '##out', 'start': 998, 'end': 1001}, {'entity': 'en', 'score': 0.9997584, 'index': 246, 'word': 'lines', 'start': 1002, 'end': 1007}, {'entity': 'en', 'score': 0.99963033, 'index': 247, 'word': 'for', 'start': 1008, 'end': 1011}, {'entity': 'other', 'score': 0.90661347, 'index': 248, 'word': '25', 'start': 1012, 'end': 1014}, {'entity': 'en', 'score': 0.99967, 'index': 249, 'word': 'years', 'start': 1015, 'end': 1020}, {'entity': 'other', 'score': 0.99992573, 'index': 250, 'word': '.', 'start': 1020, 'end': 1021}, {'entity': 'en', 'score': 0.9997687, 'index': 251, 'word': 'Some', 'start': 1022, 'end': 1026}, {'entity': 'en', 'score': 0.9997943, 'index': 252, 'word': 'people', 'start': 1027, 'end': 1033}, {'entity': 'en', 'score': 0.9997925, 'index': 253, 'word': 'thought', 'start': 1034, 'end': 1041}, {'entity': 'en', 'score': 0.9996296, 'index': 254, 'word': 'the', 'start': 1042, 'end': 1045}, {'entity': 'en', 'score': 0.99950707, 'index': 255, 'word': 'natural', 'start': 1046, 'end': 1053}, {'entity': 'en', 'score': 0.9997507, 'index': 256, 'word': 'land', 'start': 1054, 'end': 1058}, {'entity': 'en', 'score': 0.99979764, 'index': 257, 'word': '##form', 'start': 1058, 'end': 1062}, {'entity': 'en', 'score': 0.99975556, 'index': 258, 'word': 'was', 'start': 1063, 'end': 1066}, {'entity': 'en', 'score': 0.99978846, 'index': 259, 'word': 'evidence', 'start': 1067, 'end': 1075}, {'entity': 'en', 'score': 0.9998209, 'index': 260, 'word': 'of', 'start': 1076, 'end': 1078}, {'entity': 'en', 'score': 0.99982053, 'index': 261, 'word': 'life', 'start': 1079, 'end': 1083}, {'entity': 'en', 'score': 0.999806, 'index': 262, 'word': 'on', 'start': 1084, 'end': 1086}, {'entity': 'ne', 'score': 0.9924434, 'index': 263, 'word': 'Mars', 'start': 1087, 'end': 1091}, {'entity': 'other', 'score': 0.9999304, 'index': 264, 'word': ',', 'start': 1091, 'end': 1092}, {'entity': 'en', 'score': 0.9998184, 'index': 265, 'word': 'and', 'start': 1093, 'end': 1096}, {'entity': 'en', 'score': 0.9998319, 'index': 266, 'word': 'that', 'start': 1097, 'end': 1101}, {'entity': 'en', 'score': 0.9995714, 'index': 267, 'word': 'us', 'start': 1102, 'end': 1104}, {'entity': 'en', 'score': 0.9997626, 'index': 268, 'word': 'scientists', 'start': 1105, 'end': 1115}, {'entity': 'en', 'score': 0.9998241, 'index': 269, 'word': 'wanted', 'start': 1116, 'end': 1122}, {'entity': 'en', 'score': 0.9998323, 'index': 270, 'word': 'to', 'start': 1123, 'end': 1125}, {'entity': 'en', 'score': 0.99986374, 'index': 271, 'word': 'hide', 'start': 1126, 'end': 1130}, {'entity': 'en', 'score': 0.99984705, 'index': 272, 'word': 'it', 'start': 1131, 'end': 1133}, {'entity': 'other', 'score': 0.99992883, 'index': 273, 'word': ',', 'start': 1133, 'end': 1134}, {'entity': 'en', 'score': 0.99986625, 'index': 274, 'word': 'but', 'start': 1135, 'end': 1138}, {'entity': 'en', 'score': 0.99982065, 'index': 275, 'word': 'really', 'start': 1139, 'end': 1145}, {'entity': 'other', 'score': 0.9999231, 'index': 276, 'word': ',', 'start': 1145, 'end': 1146}, {'entity': 'en', 'score': 0.99983907, 'index': 277, 'word': 'the', 'start': 1147, 'end': 1150}, {'entity': 'en', 'score': 0.9998418, 'index': 278, 'word': 'defender', 'start': 1151, 'end': 1159}, {'entity': 'en', 'score': 0.9998036, 'index': 279, 'word': '##s', 'start': 1159, 'end': 1160}, {'entity': 'en', 'score': 0.9998399, 'index': 280, 'word': 'of', 'start': 1161, 'end': 1163}, {'entity': 'en', 'score': 0.9997856, 'index': 281, 'word': 'the', 'start': 1164, 'end': 1167}, {'entity': 'ne', 'score': 0.9955486, 'index': 282, 'word': 'NASA', 'start': 1168, 'end': 1172}, {'entity': 'en', 'score': 0.9998142, 'index': 283, 'word': 'budget', 'start': 1173, 'end': 1179}, {'entity': 'en', 'score': 0.9998621, 'index': 284, 'word': 'wish', 'start': 1180, 'end': 1184}, {'entity': 'en', 'score': 0.99985635, 'index': 285, 'word': 'there', 'start': 1185, 'end': 1190}, {'entity': 'en', 'score': 0.9998217, 'index': 286, 'word': 'was', 'start': 1191, 'end': 1194}, {'entity': 'en', 'score': 0.9998242, 'index': 287, 'word': 'ancient', 'start': 1195, 'end': 1202}, {'entity': 'en', 'score': 0.9998467, 'index': 288, 'word': 'civili', 'start': 1203, 'end': 1209}, {'entity': 'en', 'score': 0.9998492, 'index': 289, 'word': '##zation', 'start': 1209, 'end': 1215}, {'entity': 'en', 'score': 0.99985075, 'index': 290, 'word': 'on', 'start': 1216, 'end': 1218}, {'entity': 'ne', 'score': 0.9934476, 'index': 291, 'word': 'Mars', 'start': 1219, 'end': 1223}, {'entity': 'other', 'score': 0.9999279, 'index': 292, 'word': '.', 'start': 1223, 'end': 1224}, {'entity': 'en', 'score': 0.9996929, 'index': 293, 'word': 'We', 'start': 1225, 'end': 1227}, {'entity': 'en', 'score': 0.9997583, 'index': 294, 'word': 'decided', 'start': 1228, 'end': 1235}, {'entity': 'en', 'score': 0.99972254, 'index': 295, 'word': 'to', 'start': 1236, 'end': 1238}, {'entity': 'en', 'score': 0.99963474, 'index': 296, 'word': 'take', 'start': 1239, 'end': 1243}, {'entity': 'en', 'score': 0.99941206, 'index': 297, 'word': 'another', 'start': 1244, 'end': 1251}, {'entity': 'en', 'score': 0.99968994, 'index': 298, 'word': 'shot', 'start': 1252, 'end': 1256}, {'entity': 'en', 'score': 0.9997118, 'index': 299, 'word': 'just', 'start': 1257, 'end': 1261}, {'entity': 'en', 'score': 0.9997631, 'index': 300, 'word': 'to', 'start': 1262, 'end': 1264}, {'entity': 'en', 'score': 0.99978083, 'index': 301, 'word': 'make', 'start': 1265, 'end': 1269}, {'entity': 'en', 'score': 0.9998004, 'index': 302, 'word': 'sure', 'start': 1270, 'end': 1274}, {'entity': 'en', 'score': 0.99981683, 'index': 303, 'word': 'we', 'start': 1275, 'end': 1277}, {'entity': 'en', 'score': 0.99981815, 'index': 304, 'word': 'were', 'start': 1278, 'end': 1282}, {'entity': 'en', 'score': 0.99982005, 'index': 305, 'word': '##n', 'start': 1282, 'end': 1283}, {'entity': 'en', 'score': 0.9997904, 'index': 306, 'word': \"'\", 'start': 1283, 'end': 1284}, {'entity': 'en', 'score': 0.999744, 'index': 307, 'word': 't', 'start': 1284, 'end': 1285}, {'entity': 'en', 'score': 0.9998004, 'index': 308, 'word': 'wrong', 'start': 1286, 'end': 1291}, {'entity': 'other', 'score': 0.99991894, 'index': 309, 'word': ',', 'start': 1291, 'end': 1292}, {'entity': 'en', 'score': 0.9986118, 'index': 310, 'word': 'on', 'start': 1293, 'end': 1295}, {'entity': 'other', 'score': 0.53777504, 'index': 311, 'word': 'April', 'start': 1296, 'end': 1301}, {'entity': 'other', 'score': 0.9973557, 'index': 312, 'word': '5', 'start': 1302, 'end': 1303}, {'entity': 'other', 'score': 0.9998646, 'index': 313, 'word': ',', 'start': 1303, 'end': 1304}, {'entity': 'other', 'score': 0.99943346, 'index': 314, 'word': '1998', 'start': 1305, 'end': 1309}, {'entity': 'other', 'score': 0.9999175, 'index': 315, 'word': '.', 'start': 1309, 'end': 1310}, {'entity': 'ne', 'score': 0.99906963, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'ne', 'score': 0.9983594, 'index': 317, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'ne', 'score': 0.9985464, 'index': 318, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'en', 'score': 0.99965477, 'index': 319, 'word': 'and', 'start': 1325, 'end': 1328}, {'entity': 'en', 'score': 0.9990552, 'index': 320, 'word': 'his', 'start': 1329, 'end': 1332}, {'entity': 'ne', 'score': 0.97999007, 'index': 321, 'word': 'Mars', 'start': 1333, 'end': 1337}, {'entity': 'ne', 'score': 0.6277367, 'index': 322, 'word': 'Or', 'start': 1338, 'end': 1340}, {'entity': 'ne', 'score': 0.5293862, 'index': 323, 'word': '##biter', 'start': 1340, 'end': 1345}, {'entity': 'en', 'score': 0.9995109, 'index': 324, 'word': 'camera', 'start': 1346, 'end': 1352}, {'entity': 'en', 'score': 0.9995858, 'index': 325, 'word': 'team', 'start': 1353, 'end': 1357}, {'entity': 'en', 'score': 0.99904686, 'index': 326, 'word': 'took', 'start': 1358, 'end': 1362}, {'entity': 'en', 'score': 0.99744225, 'index': 327, 'word': 'a', 'start': 1363, 'end': 1364}, {'entity': 'en', 'score': 0.99924374, 'index': 328, 'word': 'picture', 'start': 1365, 'end': 1372}, {'entity': 'en', 'score': 0.99940455, 'index': 329, 'word': 'that', 'start': 1373, 'end': 1377}, {'entity': 'en', 'score': 0.9994288, 'index': 330, 'word': 'was', 'start': 1378, 'end': 1381}, {'entity': 'en', 'score': 0.99890625, 'index': 331, 'word': 'ten', 'start': 1382, 'end': 1385}, {'entity': 'en', 'score': 0.9993057, 'index': 332, 'word': 'times', 'start': 1386, 'end': 1391}, {'entity': 'en', 'score': 0.99956423, 'index': 333, 'word': 'sharp', 'start': 1392, 'end': 1397}, {'entity': 'en', 'score': 0.99929535, 'index': 334, 'word': '##er', 'start': 1397, 'end': 1399}, {'entity': 'en', 'score': 0.9995135, 'index': 335, 'word': 'than', 'start': 1400, 'end': 1404}, {'entity': 'en', 'score': 0.99948967, 'index': 336, 'word': 'the', 'start': 1405, 'end': 1408}, {'entity': 'en', 'score': 0.99922657, 'index': 337, 'word': 'original', 'start': 1409, 'end': 1417}, {'entity': 'en', 'score': 0.54231477, 'index': 338, 'word': 'Viking', 'start': 1418, 'end': 1424}, {'entity': 'en', 'score': 0.9983171, 'index': 339, 'word': 'photos', 'start': 1425, 'end': 1431}, {'entity': 'other', 'score': 0.9999213, 'index': 340, 'word': ',', 'start': 1431, 'end': 1432}, {'entity': 'en', 'score': 0.9995522, 'index': 341, 'word': 'reveal', 'start': 1433, 'end': 1439}, {'entity': 'en', 'score': 0.9996402, 'index': 342, 'word': '##ing', 'start': 1439, 'end': 1442}, {'entity': 'en', 'score': 0.9978428, 'index': 343, 'word': 'a', 'start': 1443, 'end': 1444}, {'entity': 'en', 'score': 0.99874496, 'index': 344, 'word': 'natural', 'start': 1445, 'end': 1452}, {'entity': 'en', 'score': 0.99962974, 'index': 345, 'word': 'land', 'start': 1453, 'end': 1457}, {'entity': 'en', 'score': 0.99966383, 'index': 346, 'word': '##form', 'start': 1457, 'end': 1461}, {'entity': 'other', 'score': 0.99993026, 'index': 347, 'word': ',', 'start': 1461, 'end': 1462}, {'entity': 'en', 'score': 0.99916697, 'index': 348, 'word': 'which', 'start': 1463, 'end': 1468}, {'entity': 'en', 'score': 0.9983835, 'index': 349, 'word': 'meant', 'start': 1469, 'end': 1474}, {'entity': 'en', 'score': 0.9928202, 'index': 350, 'word': 'no', 'start': 1475, 'end': 1477}, {'entity': 'en', 'score': 0.9962263, 'index': 351, 'word': 'alien', 'start': 1478, 'end': 1483}, {'entity': 'en', 'score': 0.9987729, 'index': 352, 'word': 'monument', 'start': 1484, 'end': 1492}, {'entity': 'other', 'score': 0.9999304, 'index': 353, 'word': '.', 'start': 1492, 'end': 1493}, {'entity': 'other', 'score': 0.99991786, 'index': 354, 'word': '\"', 'start': 1494, 'end': 1495}, {'entity': 'en', 'score': 0.99973565, 'index': 355, 'word': 'But', 'start': 1495, 'end': 1498}, {'entity': 'en', 'score': 0.9994904, 'index': 356, 'word': 'that', 'start': 1499, 'end': 1503}, {'entity': 'en', 'score': 0.9995603, 'index': 357, 'word': 'picture', 'start': 1504, 'end': 1511}, {'entity': 'en', 'score': 0.99966073, 'index': 358, 'word': 'wasn', 'start': 1512, 'end': 1516}, {'entity': 'en', 'score': 0.99970895, 'index': 359, 'word': \"'\", 'start': 1516, 'end': 1517}, {'entity': 'en', 'score': 0.99959975, 'index': 360, 'word': 't', 'start': 1517, 'end': 1518}, {'entity': 'en', 'score': 0.9995778, 'index': 361, 'word': 'very', 'start': 1519, 'end': 1523}, {'entity': 'en', 'score': 0.99971443, 'index': 362, 'word': 'clear', 'start': 1524, 'end': 1529}, {'entity': 'en', 'score': 0.9997217, 'index': 363, 'word': 'at', 'start': 1530, 'end': 1532}, {'entity': 'en', 'score': 0.99960893, 'index': 364, 'word': 'all', 'start': 1533, 'end': 1536}, {'entity': 'other', 'score': 0.99993, 'index': 365, 'word': ',', 'start': 1536, 'end': 1537}, {'entity': 'en', 'score': 0.99955565, 'index': 366, 'word': 'which', 'start': 1538, 'end': 1543}, {'entity': 'en', 'score': 0.99938047, 'index': 367, 'word': 'could', 'start': 1544, 'end': 1549}, {'entity': 'en', 'score': 0.99928325, 'index': 368, 'word': 'mean', 'start': 1550, 'end': 1554}, {'entity': 'en', 'score': 0.9982596, 'index': 369, 'word': 'alien', 'start': 1555, 'end': 1560}, {'entity': 'en', 'score': 0.99910384, 'index': 370, 'word': 'marking', 'start': 1561, 'end': 1568}, {'entity': 'en', 'score': 0.99916995, 'index': 371, 'word': '##s', 'start': 1568, 'end': 1569}, {'entity': 'en', 'score': 0.99937785, 'index': 372, 'word': 'were', 'start': 1570, 'end': 1574}, {'entity': 'en', 'score': 0.99948174, 'index': 373, 'word': 'hidden', 'start': 1575, 'end': 1581}, {'entity': 'en', 'score': 0.99941933, 'index': 374, 'word': 'by', 'start': 1582, 'end': 1584}, {'entity': 'en', 'score': 0.99944514, 'index': 375, 'word': 'ha', 'start': 1585, 'end': 1587}, {'entity': 'en', 'score': 0.9994628, 'index': 376, 'word': '##ze', 'start': 1587, 'end': 1589}, {'entity': 'other', 'score': 0.9999194, 'index': 377, 'word': '\"', 'start': 1589, 'end': 1590}, {'entity': 'en', 'score': 0.9996394, 'index': 378, 'word': 'Well', 'start': 1591, 'end': 1595}, {'entity': 'en', 'score': 0.99759346, 'index': 379, 'word': 'no', 'start': 1596, 'end': 1598}, {'entity': 'other', 'score': 0.99993074, 'index': 380, 'word': ',', 'start': 1598, 'end': 1599}, {'entity': 'en', 'score': 0.9995228, 'index': 381, 'word': 'ye', 'start': 1600, 'end': 1602}, {'entity': 'en', 'score': 0.99969923, 'index': 382, 'word': '##s', 'start': 1602, 'end': 1603}, {'entity': 'en', 'score': 0.99970835, 'index': 383, 'word': 'that', 'start': 1604, 'end': 1608}, {'entity': 'en', 'score': 0.999665, 'index': 384, 'word': 'rum', 'start': 1609, 'end': 1612}, {'entity': 'en', 'score': 0.99970514, 'index': 385, 'word': '##or', 'start': 1612, 'end': 1614}, {'entity': 'en', 'score': 0.99943703, 'index': 386, 'word': 'started', 'start': 1615, 'end': 1622}, {'entity': 'other', 'score': 0.9999294, 'index': 387, 'word': ',', 'start': 1622, 'end': 1623}, {'entity': 'en', 'score': 0.99961936, 'index': 388, 'word': 'but', 'start': 1624, 'end': 1627}, {'entity': 'en', 'score': 0.99959284, 'index': 389, 'word': 'to', 'start': 1628, 'end': 1630}, {'entity': 'en', 'score': 0.99967396, 'index': 390, 'word': 'prove', 'start': 1631, 'end': 1636}, {'entity': 'en', 'score': 0.9996499, 'index': 391, 'word': 'them', 'start': 1637, 'end': 1641}, {'entity': 'en', 'score': 0.9996276, 'index': 392, 'word': 'wrong', 'start': 1642, 'end': 1647}, {'entity': 'en', 'score': 0.99944097, 'index': 393, 'word': 'on', 'start': 1648, 'end': 1650}, {'entity': 'en', 'score': 0.61485404, 'index': 394, 'word': 'April', 'start': 1651, 'end': 1656}, {'entity': 'other', 'score': 0.99845207, 'index': 395, 'word': '8', 'start': 1657, 'end': 1658}, {'entity': 'other', 'score': 0.9998248, 'index': 396, 'word': ',', 'start': 1658, 'end': 1659}, {'entity': 'other', 'score': 0.967133, 'index': 397, 'word': '2001', 'start': 1660, 'end': 1664}, {'entity': 'en', 'score': 0.99961036, 'index': 398, 'word': 'we', 'start': 1665, 'end': 1667}, {'entity': 'en', 'score': 0.9996408, 'index': 399, 'word': 'decided', 'start': 1668, 'end': 1675}, {'entity': 'en', 'score': 0.99961096, 'index': 400, 'word': 'to', 'start': 1676, 'end': 1678}, {'entity': 'en', 'score': 0.9994717, 'index': 401, 'word': 'take', 'start': 1679, 'end': 1683}, {'entity': 'en', 'score': 0.99907696, 'index': 402, 'word': 'another', 'start': 1684, 'end': 1691}, {'entity': 'en', 'score': 0.9993956, 'index': 403, 'word': 'picture', 'start': 1692, 'end': 1699}, {'entity': 'other', 'score': 0.9999089, 'index': 404, 'word': ',', 'start': 1699, 'end': 1700}, {'entity': 'en', 'score': 0.9996431, 'index': 405, 'word': 'making', 'start': 1701, 'end': 1707}, {'entity': 'en', 'score': 0.9997148, 'index': 406, 'word': 'sure', 'start': 1708, 'end': 1712}, {'entity': 'en', 'score': 0.9997253, 'index': 407, 'word': 'it', 'start': 1713, 'end': 1715}, {'entity': 'en', 'score': 0.99960643, 'index': 408, 'word': 'was', 'start': 1716, 'end': 1719}, {'entity': 'en', 'score': 0.9989556, 'index': 409, 'word': 'a', 'start': 1720, 'end': 1721}, {'entity': 'en', 'score': 0.99975353, 'index': 410, 'word': 'cloud', 'start': 1722, 'end': 1727}, {'entity': 'en', 'score': 0.999777, 'index': 411, 'word': '##less', 'start': 1727, 'end': 1731}, {'entity': 'en', 'score': 0.9997631, 'index': 412, 'word': 'summer', 'start': 1732, 'end': 1738}, {'entity': 'en', 'score': 0.99969566, 'index': 413, 'word': 'day', 'start': 1739, 'end': 1742}, {'entity': 'other', 'score': 0.9999254, 'index': 414, 'word': '.', 'start': 1742, 'end': 1743}, {'entity': 'ne', 'score': 0.9956275, 'index': 415, 'word': 'Mali', 'start': 1744, 'end': 1748}, {'entity': 'ne', 'score': 0.9934854, 'index': 416, 'word': '##n', 'start': 1748, 'end': 1749}, {'entity': 'en', 'score': 0.9940276, 'index': 417, 'word': \"'\", 'start': 1749, 'end': 1750}, {'entity': 'en', 'score': 0.9994653, 'index': 418, 'word': 's', 'start': 1750, 'end': 1751}, {'entity': 'en', 'score': 0.99972516, 'index': 419, 'word': 'team', 'start': 1752, 'end': 1756}, {'entity': 'en', 'score': 0.9996685, 'index': 420, 'word': 'captured', 'start': 1757, 'end': 1765}, {'entity': 'en', 'score': 0.99941444, 'index': 421, 'word': 'an', 'start': 1766, 'end': 1768}, {'entity': 'en', 'score': 0.99968505, 'index': 422, 'word': 'ama', 'start': 1769, 'end': 1772}, {'entity': 'en', 'score': 0.99972624, 'index': 423, 'word': '##zing', 'start': 1772, 'end': 1776}, {'entity': 'en', 'score': 0.9996338, 'index': 424, 'word': 'photo', 'start': 1777, 'end': 1782}, {'entity': 'en', 'score': 0.9996408, 'index': 425, 'word': 'using', 'start': 1783, 'end': 1788}, {'entity': 'en', 'score': 0.9994491, 'index': 426, 'word': 'the', 'start': 1789, 'end': 1792}, {'entity': 'en', 'score': 0.99961936, 'index': 427, 'word': 'camera', 'start': 1793, 'end': 1799}, {'entity': 'en', 'score': 0.9996668, 'index': 428, 'word': \"'\", 'start': 1799, 'end': 1800}, {'entity': 'en', 'score': 0.9995703, 'index': 429, 'word': 's', 'start': 1800, 'end': 1801}, {'entity': 'en', 'score': 0.99967444, 'index': 430, 'word': 'absolute', 'start': 1802, 'end': 1810}, {'entity': 'en', 'score': 0.99970645, 'index': 431, 'word': 'maximum', 'start': 1811, 'end': 1818}, {'entity': 'en', 'score': 0.9996973, 'index': 432, 'word': 'revolution', 'start': 1819, 'end': 1829}, {'entity': 'other', 'score': 0.99992955, 'index': 433, 'word': '.', 'start': 1829, 'end': 1830}, {'entity': 'en', 'score': 0.9997081, 'index': 434, 'word': 'With', 'start': 1831, 'end': 1835}, {'entity': 'en', 'score': 0.99963665, 'index': 435, 'word': 'this', 'start': 1836, 'end': 1840}, {'entity': 'en', 'score': 0.99965286, 'index': 436, 'word': 'camera', 'start': 1841, 'end': 1847}, {'entity': 'en', 'score': 0.9997528, 'index': 437, 'word': 'you', 'start': 1848, 'end': 1851}, {'entity': 'en', 'score': 0.9997576, 'index': 438, 'word': 'can', 'start': 1852, 'end': 1855}, {'entity': 'en', 'score': 0.9997657, 'index': 439, 'word': 'disc', 'start': 1856, 'end': 1860}, {'entity': 'en', 'score': 0.9997956, 'index': 440, 'word': '##ern', 'start': 1860, 'end': 1863}, {'entity': 'en', 'score': 0.9997335, 'index': 441, 'word': 'things', 'start': 1864, 'end': 1870}, {'entity': 'en', 'score': 0.999653, 'index': 442, 'word': 'in', 'start': 1871, 'end': 1873}, {'entity': 'en', 'score': 0.9991596, 'index': 443, 'word': 'a', 'start': 1874, 'end': 1875}, {'entity': 'en', 'score': 0.99945194, 'index': 444, 'word': 'digital', 'start': 1876, 'end': 1883}, {'entity': 'en', 'score': 0.9995933, 'index': 445, 'word': 'image', 'start': 1884, 'end': 1889}, {'entity': 'other', 'score': 0.9999232, 'index': 446, 'word': ',', 'start': 1889, 'end': 1890}, {'entity': 'other', 'score': 0.99489, 'index': 447, 'word': '3', 'start': 1891, 'end': 1892}, {'entity': 'en', 'score': 0.9991105, 'index': 448, 'word': 'times', 'start': 1893, 'end': 1898}, {'entity': 'en', 'score': 0.9994937, 'index': 449, 'word': 'bigger', 'start': 1899, 'end': 1905}, {'entity': 'en', 'score': 0.9995734, 'index': 450, 'word': 'than', 'start': 1906, 'end': 1910}, {'entity': 'en', 'score': 0.9995715, 'index': 451, 'word': 'the', 'start': 1911, 'end': 1914}, {'entity': 'en', 'score': 0.9996369, 'index': 452, 'word': 'pi', 'start': 1915, 'end': 1917}, {'entity': 'en', 'score': 0.9996636, 'index': 453, 'word': '##xel', 'start': 1917, 'end': 1920}, {'entity': 'en', 'score': 0.99961394, 'index': 454, 'word': 'size', 'start': 1921, 'end': 1925}, {'entity': 'en', 'score': 0.9996786, 'index': 455, 'word': 'which', 'start': 1926, 'end': 1931}, {'entity': 'en', 'score': 0.9996667, 'index': 456, 'word': 'means', 'start': 1932, 'end': 1937}, {'entity': 'en', 'score': 0.9997652, 'index': 457, 'word': 'if', 'start': 1938, 'end': 1940}, {'entity': 'en', 'score': 0.99967515, 'index': 458, 'word': 'there', 'start': 1941, 'end': 1946}, {'entity': 'en', 'score': 0.9996507, 'index': 459, 'word': 'were', 'start': 1947, 'end': 1951}, {'entity': 'en', 'score': 0.99955803, 'index': 460, 'word': 'any', 'start': 1952, 'end': 1955}, {'entity': 'en', 'score': 0.99953544, 'index': 461, 'word': 'signs', 'start': 1956, 'end': 1961}, {'entity': 'en', 'score': 0.99960965, 'index': 462, 'word': 'of', 'start': 1962, 'end': 1964}, {'entity': 'en', 'score': 0.99961805, 'index': 463, 'word': 'life', 'start': 1965, 'end': 1969}, {'entity': 'other', 'score': 0.9999225, 'index': 464, 'word': ',', 'start': 1969, 'end': 1970}, {'entity': 'en', 'score': 0.9997342, 'index': 465, 'word': 'you', 'start': 1971, 'end': 1974}, {'entity': 'en', 'score': 0.99973625, 'index': 466, 'word': 'could', 'start': 1975, 'end': 1980}, {'entity': 'en', 'score': 0.9997397, 'index': 467, 'word': 'easily', 'start': 1981, 'end': 1987}, {'entity': 'en', 'score': 0.99969923, 'index': 468, 'word': 'see', 'start': 1988, 'end': 1991}, {'entity': 'en', 'score': 0.999673, 'index': 469, 'word': 'what', 'start': 1992, 'end': 1996}, {'entity': 'en', 'score': 0.9996773, 'index': 470, 'word': 'they', 'start': 1997, 'end': 2001}, {'entity': 'en', 'score': 0.9995364, 'index': 471, 'word': 'were', 'start': 2002, 'end': 2006}, {'entity': 'other', 'score': 0.99993205, 'index': 472, 'word': '.', 'start': 2006, 'end': 2007}, {'entity': 'en', 'score': 0.99950016, 'index': 473, 'word': 'What', 'start': 2008, 'end': 2012}, {'entity': 'en', 'score': 0.999288, 'index': 474, 'word': 'the', 'start': 2013, 'end': 2016}, {'entity': 'en', 'score': 0.99938726, 'index': 475, 'word': 'picture', 'start': 2017, 'end': 2024}, {'entity': 'en', 'score': 0.9994843, 'index': 476, 'word': 'showed', 'start': 2025, 'end': 2031}, {'entity': 'en', 'score': 0.99938786, 'index': 477, 'word': 'was', 'start': 2032, 'end': 2035}, {'entity': 'en', 'score': 0.99811554, 'index': 478, 'word': 'the', 'start': 2036, 'end': 2039}, {'entity': 'en', 'score': 0.9786213, 'index': 479, 'word': 'but', 'start': 2040, 'end': 2043}, {'entity': 'en', 'score': 0.99004257, 'index': 480, 'word': '##te', 'start': 2043, 'end': 2045}, {'entity': 'en', 'score': 0.99612516, 'index': 481, 'word': 'or', 'start': 2046, 'end': 2048}, {'entity': 'en', 'score': 0.87278074, 'index': 482, 'word': 'mesa', 'start': 2049, 'end': 2053}, {'entity': 'other', 'score': 0.9999268, 'index': 483, 'word': ',', 'start': 2053, 'end': 2054}, {'entity': 'en', 'score': 0.99910945, 'index': 484, 'word': 'which', 'start': 2055, 'end': 2060}, {'entity': 'en', 'score': 0.9985214, 'index': 485, 'word': 'are', 'start': 2061, 'end': 2064}, {'entity': 'en', 'score': 0.9983, 'index': 486, 'word': 'land', 'start': 2065, 'end': 2069}, {'entity': 'en', 'score': 0.9991359, 'index': 487, 'word': '##form', 'start': 2069, 'end': 2073}, {'entity': 'en', 'score': 0.99933463, 'index': 488, 'word': '##s', 'start': 2073, 'end': 2074}, {'entity': 'en', 'score': 0.9994253, 'index': 489, 'word': 'common', 'start': 2075, 'end': 2081}, {'entity': 'en', 'score': 0.9994831, 'index': 490, 'word': 'around', 'start': 2082, 'end': 2088}, {'entity': 'en', 'score': 0.99935335, 'index': 491, 'word': 'the', 'start': 2089, 'end': 2092}, {'entity': 'en', 'score': 0.9982742, 'index': 492, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'en', 'score': 0.99908066, 'index': 493, 'word': 'West', 'start': 2102, 'end': 2106}, {'entity': 'other', 'score': 0.9999201, 'index': 494, 'word': '.', 'start': 2106, 'end': 2107}]\n",
      "{0: 'O', 1: 'ambiguous', 2: 'en', 3: 'fw', 4: 'mixed', 5: 'ne', 6: 'other', 7: 'spa', 8: 'unk'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/codeswitch-spaeng-lid-lince\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"sagorsarker/codeswitch-spaeng-lid-lince\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b7e55e77-d41c-4b9a-b2fd-4ad85c37696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'ambiguous', 2: 'en', 3: 'fw', 4: 'mixed', 5: 'ne', 6: 'other', 7: 'spa', 8: 'unk'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d47015-a58d-4c0b-94b2-05ac3038199a",
   "metadata": {},
   "source": [
    "## 17 jvdzwaan/ocrpostcorrection-task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "234b1d6f-0109-4a66-8353-e6fbf131eae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'jvdzwaan/ocrpostcorrection-task-1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jvdzwaan/ocrpostcorrection-task-1' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[340], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjvdzwaan/ocrpostcorrection-task-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjvdzwaan/ocrpostcorrection-task-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:915\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2275\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2280\u001b[0m     )\n\u001b[0;32m   2282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'jvdzwaan/ocrpostcorrection-task-1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jvdzwaan/ocrpostcorrection-task-1' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jvdzwaan/ocrpostcorrection-task-1\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"jvdzwaan/ocrpostcorrection-task-1\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7babbc0-5f6c-46a4-aea2-65487527ef54",
   "metadata": {},
   "source": [
    "## 18 GEOcite/AuthorParserModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "a043d3cc-9579-4f9c-b52a-9fda322c09fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'B-ORG', 'score': 0.56347305, 'index': 8, 'word': 'nasa', 'start': 16, 'end': 20}, {'entity': 'I-ORG', 'score': 0.64977086, 'index': 25, 'word': 'mars', 'start': 96, 'end': 100}, {'entity': 'I-ORG', 'score': 0.5621034, 'index': 37, 'word': 'mars', 'start': 152, 'end': 156}, {'entity': 'B-ORG', 'score': 0.83589464, 'index': 59, 'word': 'viking', 'start': 240, 'end': 246}, {'entity': 'I-ORG', 'score': 0.93603927, 'index': 60, 'word': '1', 'start': 247, 'end': 248}, {'entity': 'I-ORG', 'score': 0.67972714, 'index': 61, 'word': 'spacecraft', 'start': 249, 'end': 259}, {'entity': 'B-LOC', 'score': 0.47661397, 'index': 97, 'word': 'marti', 'start': 407, 'end': 412}, {'entity': 'I-ORG', 'score': 0.5451927, 'index': 98, 'word': '##an', 'start': 412, 'end': 414}, {'entity': 'I-ORG', 'score': 0.60608196, 'index': 99, 'word': 'mesa', 'start': 415, 'end': 419}, {'entity': 'B-LOC', 'score': 0.8834184, 'index': 103, 'word': 'c', 'start': 435, 'end': 436}, {'entity': 'I-LOC', 'score': 0.7397884, 'index': 104, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'I-LOC', 'score': 0.9083945, 'index': 105, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'B-ORG', 'score': 0.5038258, 'index': 118, 'word': 'egypt', 'start': 496, 'end': 501}, {'entity': 'I-ORG', 'score': 0.64455724, 'index': 119, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'I-ORG', 'score': 0.6576205, 'index': 120, 'word': 'pha', 'start': 505, 'end': 508}, {'entity': 'I-ORG', 'score': 0.6960956, 'index': 121, 'word': '##rao', 'start': 508, 'end': 511}, {'entity': 'I-ORG', 'score': 0.61072016, 'index': 122, 'word': '##h', 'start': 511, 'end': 512}, {'entity': 'B-ORG', 'score': 0.7676731, 'index': 190, 'word': 'nasa', 'start': 801, 'end': 805}, {'entity': 'B-LOC', 'score': 0.37248164, 'index': 201, 'word': 'mars', 'start': 843, 'end': 847}, {'entity': 'I-ORG', 'score': 0.6403719, 'index': 211, 'word': 'mars', 'start': 874, 'end': 878}, {'entity': 'B-ORG', 'score': 0.6982709, 'index': 233, 'word': 'haunted', 'start': 971, 'end': 978}, {'entity': 'I-ORG', 'score': 0.8855793, 'index': 234, 'word': 'gr', 'start': 979, 'end': 981}, {'entity': 'I-ORG', 'score': 0.89122075, 'index': 235, 'word': '##oce', 'start': 981, 'end': 984}, {'entity': 'I-ORG', 'score': 0.91083044, 'index': 236, 'word': '##ry', 'start': 984, 'end': 986}, {'entity': 'I-ORG', 'score': 0.78272146, 'index': 237, 'word': 'store', 'start': 987, 'end': 992}, {'entity': 'I-ORG', 'score': 0.39987472, 'index': 257, 'word': 'mars', 'start': 1087, 'end': 1091}, {'entity': 'B-ORG', 'score': 0.37609065, 'index': 272, 'word': 'defenders', 'start': 1151, 'end': 1160}, {'entity': 'I-ORG', 'score': 0.49420744, 'index': 273, 'word': 'of', 'start': 1161, 'end': 1163}, {'entity': 'I-ORG', 'score': 0.65821886, 'index': 274, 'word': 'the', 'start': 1164, 'end': 1167}, {'entity': 'I-ORG', 'score': 0.626755, 'index': 275, 'word': 'nasa', 'start': 1168, 'end': 1172}, {'entity': 'I-ORG', 'score': 0.6965647, 'index': 281, 'word': 'civilization', 'start': 1203, 'end': 1215}, {'entity': 'I-ORG', 'score': 0.48566574, 'index': 282, 'word': 'on', 'start': 1216, 'end': 1218}, {'entity': 'I-ORG', 'score': 0.70331687, 'index': 283, 'word': 'mars', 'start': 1219, 'end': 1223}, {'entity': 'B-PER', 'score': 0.9669894, 'index': 308, 'word': 'michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.94802105, 'index': 309, 'word': 'mali', 'start': 1319, 'end': 1323}, {'entity': 'I-PER', 'score': 0.9340164, 'index': 310, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'B-ORG', 'score': 0.8970071, 'index': 313, 'word': 'mars', 'start': 1333, 'end': 1337}, {'entity': 'I-ORG', 'score': 0.93871564, 'index': 314, 'word': 'orbite', 'start': 1338, 'end': 1344}, {'entity': 'I-ORG', 'score': 0.9164465, 'index': 315, 'word': '##r', 'start': 1344, 'end': 1345}, {'entity': 'I-ORG', 'score': 0.7809999, 'index': 316, 'word': 'camera', 'start': 1346, 'end': 1352}, {'entity': 'B-ORG', 'score': 0.4608157, 'index': 330, 'word': 'viking', 'start': 1418, 'end': 1424}, {'entity': 'B-ORG', 'score': 0.6714569, 'index': 420, 'word': 'absolute', 'start': 1802, 'end': 1810}, {'entity': 'I-ORG', 'score': 0.76191556, 'index': 421, 'word': 'maximum', 'start': 1811, 'end': 1818}, {'entity': 'I-ORG', 'score': 0.67246604, 'index': 422, 'word': 'revolution', 'start': 1819, 'end': 1829}, {'entity': 'B-LOC', 'score': 0.6010912, 'index': 468, 'word': 'butte', 'start': 2040, 'end': 2045}, {'entity': 'I-ORG', 'score': 0.46326715, 'index': 469, 'word': 'or', 'start': 2046, 'end': 2048}, {'entity': 'I-ORG', 'score': 0.45148414, 'index': 470, 'word': 'mesa', 'start': 2049, 'end': 2053}, {'entity': 'B-LOC', 'score': 0.8956182, 'index': 480, 'word': 'american', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.9263237, 'index': 481, 'word': 'west', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GEOcite/AuthorParserModel\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"GEOcite/AuthorParserModel\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e0956aec-671d-4f55-9c1f-26e23c667920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'I-ORG', 'O', 'I-ORG', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'B-ORG', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'I-ORG', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'I-ORG', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'I-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'I-ORG', 'B-LOC', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'B-LOC', 'I-LOC', 'O']\n",
      "['so', ',', 'if', 'you', \"'\", 're', 'a', 'nasa', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'face', 'on', 'mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'aliens', ',', 'correct', '?', '\"', 'no', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##ppi', '##ng', 'photos', ',', 'when', 'it', 'spotted', 'the', 'shadow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'marti', '##an', 'mesa', ',', 'common', 'around', 'c', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'shadows', 'that', 'made', 'it', 'look', 'like', 'an', 'egypt', '##ion', 'pha', '##rao', '##h', '.', 'very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##em', '##ble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'shadows', '.', 'we', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'nasa', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'mars', '-', '-', 'and', 'it', 'did', '.', 'the', 'face', 'on', 'mars', 'soon', 'became', 'a', 'pop', 'icon', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'haunted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defenders', 'of', 'the', 'nasa', 'budget', 'wish', 'there', 'was', 'ancient', 'civilization', 'on', 'mars', '.', 'we', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'april', '5', ',', '1998', '.', 'michael', 'mali', '##n', 'and', 'his', 'mars', 'orbite', '##r', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharpe', '##r', 'than', 'the', 'original', 'viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'but', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'haz', '##e', '\"', 'well', 'no', ',', 'yes', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'april', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'amazing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'with', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pixel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'what', 'the', 'picture', 'showed', 'was', 'the', 'butte', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'american', 'west', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[\n",
    "    'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    "'O',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "0644d63d-ba52-4d03-9847-28c5de014930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 0.5, 'recall': 1.0, 'f1': 0.6666666666666666, 'number': 2}, 'ORG': {'precision': 0.043478260869565216, 'recall': 0.6666666666666666, 'f1': 0.08163265306122448, 'number': 3}, 'PER': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1': 0.8, 'number': 2}, 'overall_precision': 0.11320754716981132, 'overall_recall': 0.8571428571428571, 'overall_f1': 0.2, 'overall_accuracy': 0.8547717842323651}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ff181-9ecb-45da-acd0-193da4b5ff97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "116981e0-56cb-4094-99e4-e8b99f62597d",
   "metadata": {},
   "source": [
    "## 19 mbruton/spa_en_XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "7220081f-0732-4693-8a92-7b93d17b259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'r0:arg1|tem', 'score': 0.9885584, 'index': 4, 'word': '▁you', 'start': 6, 'end': 10}, {'entity': 'r0:root', 'score': 0.9828293, 'index': 5, 'word': \"'\", 'start': 10, 'end': 11}, {'entity': 'r0:root', 'score': 0.9987388, 'index': 6, 'word': 're', 'start': 11, 'end': 13}, {'entity': 'r0:arg2|atr', 'score': 0.9798707, 'index': 9, 'word': '▁scientist', 'start': 20, 'end': 30}, {'entity': 'r1:arg1|tem', 'score': 0.9950965, 'index': 11, 'word': '▁you', 'start': 31, 'end': 35}, {'entity': 'r1:root', 'score': 0.9981749, 'index': 13, 'word': '▁be', 'start': 42, 'end': 45}, {'entity': 'r1:arg2|atr', 'score': 0.98995215, 'index': 14, 'word': '▁able', 'start': 45, 'end': 50}, {'entity': 'r2:root', 'score': 0.99103266, 'index': 16, 'word': '▁tell', 'start': 53, 'end': 58}, {'entity': 'r2:arg2|ben', 'score': 0.9612953, 'index': 17, 'word': '▁me', 'start': 58, 'end': 61}, {'entity': 'r2:arg1|pat', 'score': 0.98190963, 'index': 20, 'word': '▁story', 'start': 71, 'end': 77}, {'entity': 'r3:arg1|tem', 'score': 0.98912394, 'index': 27, 'word': '▁which', 'start': 101, 'end': 107}, {'entity': 'r3:argM|adv', 'score': 0.60085195, 'index': 28, 'word': '▁obviously', 'start': 107, 'end': 117}, {'entity': 'r3:root', 'score': 0.9985306, 'index': 29, 'word': '▁is', 'start': 117, 'end': 120}, {'entity': 'r3:arg2|atr', 'score': 0.9856076, 'index': 30, 'word': '▁evidence', 'start': 120, 'end': 129}, {'entity': 'r4:root', 'score': 0.55564994, 'index': 32, 'word': '▁there', 'start': 134, 'end': 140}, {'entity': 'r4:root', 'score': 0.9971462, 'index': 33, 'word': '▁is', 'start': 140, 'end': 143}, {'entity': 'r4:arg1|tem', 'score': 0.94116557, 'index': 34, 'word': '▁life', 'start': 143, 'end': 148}, {'entity': 'r4:argM|loc', 'score': 0.54012036, 'index': 35, 'word': '▁on', 'start': 148, 'end': 151}, {'entity': 'r6:arg1|pat', 'score': 0.69593036, 'index': 41, 'word': '▁face', 'start': 170, 'end': 175}, {'entity': 'r6:root', 'score': 0.69639283, 'index': 43, 'word': '▁created', 'start': 179, 'end': 187}, {'entity': 'r6:arg0|agt', 'score': 0.64183134, 'index': 44, 'word': '▁by', 'start': 187, 'end': 190}, {'entity': 'r7:arg0|agt', 'score': 0.55535495, 'index': 61, 'word': 'craft', 'start': 254, 'end': 259}, {'entity': 'r8:root', 'score': 0.47476873, 'index': 63, 'word': '▁circ', 'start': 263, 'end': 268}, {'entity': 'r8:root', 'score': 0.47575742, 'index': 64, 'word': 'ling', 'start': 268, 'end': 272}, {'entity': 'r8:arg1|pat', 'score': 0.31078094, 'index': 66, 'word': '▁planet', 'start': 276, 'end': 283}, {'entity': 'r8:root', 'score': 0.31837395, 'index': 68, 'word': '▁sna', 'start': 284, 'end': 288}, {'entity': 'r9:root', 'score': 0.31875014, 'index': 69, 'word': 'pping', 'start': 288, 'end': 293}, {'entity': 'r8:arg1|pat', 'score': 0.286856, 'index': 70, 'word': '▁photos', 'start': 293, 'end': 300}, {'entity': 'r8:arg0|agt', 'score': 0.21294405, 'index': 73, 'word': '▁it', 'start': 306, 'end': 309}, {'entity': 'r9:root', 'score': 0.3105045, 'index': 74, 'word': '▁spot', 'start': 309, 'end': 314}, {'entity': 'r9:root', 'score': 0.31755808, 'index': 75, 'word': 'ted', 'start': 314, 'end': 317}, {'entity': 'r9:arg1|pat', 'score': 0.22570874, 'index': 79, 'word': '▁like', 'start': 329, 'end': 334}, {'entity': 'r9:arg1|pat', 'score': 0.2306108, 'index': 80, 'word': 'ness', 'start': 334, 'end': 338}, {'entity': 'r9:arg0|agt', 'score': 0.19357839, 'index': 87, 'word': '▁scientist', 'start': 358, 'end': 368}, {'entity': 'r9:root', 'score': 0.2762144, 'index': 89, 'word': '▁figure', 'start': 369, 'end': 376}, {'entity': 'r9:root', 'score': 0.22951078, 'index': 90, 'word': 'd', 'start': 376, 'end': 377}, {'entity': 'r8:arg1|tem', 'score': 0.078046024, 'index': 93, 'word': '▁it', 'start': 386, 'end': 389}, {'entity': 'r9:root', 'score': 0.15313852, 'index': 94, 'word': '▁was', 'start': 389, 'end': 393}, {'entity': 'r9:arg1|pat', 'score': 0.045638904, 'index': 99, 'word': '▁mesa', 'start': 414, 'end': 419}, {'entity': 'r9:arg1|tem', 'score': 0.058356848, 'index': 109, 'word': '▁one', 'start': 453, 'end': 457}, {'entity': 'r9:root', 'score': 0.1325599, 'index': 110, 'word': '▁had', 'start': 457, 'end': 461}, {'entity': 'r9:arg1|pat', 'score': 0.06691175, 'index': 111, 'word': '▁shadow', 'start': 461, 'end': 468}, {'entity': 'r9:arg0|agt', 'score': 0.06208066, 'index': 113, 'word': '▁that', 'start': 469, 'end': 474}, {'entity': 'r9:arg1|pat', 'score': 0.0632269, 'index': 115, 'word': '▁it', 'start': 479, 'end': 482}, {'entity': 'r9:root', 'score': 0.099622406, 'index': 116, 'word': '▁look', 'start': 482, 'end': 487}, {'entity': 'r9:arg1|pat', 'score': 0.034427807, 'index': 117, 'word': '▁like', 'start': 487, 'end': 492}, {'entity': 'r8:argM|tmp', 'score': 0.06514092, 'index': 128, 'word': '▁later', 'start': 527, 'end': 533}, {'entity': 'r9:arg0|agt', 'score': 0.09965178, 'index': 130, 'word': '▁we', 'start': 534, 'end': 537}, {'entity': 'r11:root', 'score': 0.08552331, 'index': 131, 'word': '▁reveal', 'start': 537, 'end': 544}, {'entity': 'r11:root', 'score': 0.07387665, 'index': 132, 'word': 'ed', 'start': 544, 'end': 546}, {'entity': 'r9:arg1|pat', 'score': 0.12956654, 'index': 134, 'word': '▁image', 'start': 550, 'end': 556}, {'entity': 'r8:argM|adv', 'score': 0.038444836, 'index': 135, 'word': '▁for', 'start': 556, 'end': 560}, {'entity': 'r9:arg0|agt', 'score': 0.047709465, 'index': 141, 'word': '▁we', 'start': 576, 'end': 579}, {'entity': 'r10:root', 'score': 0.077698395, 'index': 142, 'word': '▁made', 'start': 579, 'end': 584}, {'entity': 'r9:arg1|pat', 'score': 0.04974987, 'index': 147, 'word': '▁it', 'start': 602, 'end': 605}, {'entity': 'r11:root', 'score': 0.073394544, 'index': 148, 'word': '▁was', 'start': 605, 'end': 609}, {'entity': 'r9:arg1|pat', 'score': 0.04125579, 'index': 151, 'word': '▁rock', 'start': 616, 'end': 621}, {'entity': 'r9:arg1|pat', 'score': 0.039437402, 'index': 152, 'word': '▁formation', 'start': 621, 'end': 631}, {'entity': 'r8:arg1|tem', 'score': 0.044781037, 'index': 153, 'word': '▁that', 'start': 631, 'end': 636}, {'entity': 'r8:argM|adv', 'score': 0.039434977, 'index': 154, 'word': '▁just', 'start': 636, 'end': 641}, {'entity': 'r11:root', 'score': 0.0661228, 'index': 155, 'word': '▁rese', 'start': 641, 'end': 646}, {'entity': 'r11:root', 'score': 0.0739401, 'index': 156, 'word': 'mble', 'start': 646, 'end': 650}, {'entity': 'r11:root', 'score': 0.06486134, 'index': 157, 'word': 'd', 'start': 650, 'end': 651}, {'entity': 'r9:arg1|pat', 'score': 0.046511702, 'index': 160, 'word': '▁head', 'start': 659, 'end': 664}, {'entity': 'r11:root', 'score': 0.0790022, 'index': 169, 'word': '▁for', 'start': 692, 'end': 696}, {'entity': 'r10:root', 'score': 0.077606075, 'index': 170, 'word': 'med', 'start': 696, 'end': 699}, {'entity': 'r9:arg0|agt', 'score': 0.040013608, 'index': 171, 'word': '▁by', 'start': 699, 'end': 702}, {'entity': 'r9:arg0|agt', 'score': 0.08678443, 'index': 175, 'word': '▁We', 'start': 711, 'end': 714}, {'entity': 'r9:root', 'score': 0.10425882, 'index': 177, 'word': '▁announced', 'start': 719, 'end': 729}, {'entity': 'r9:arg1|pat', 'score': 0.13532344, 'index': 178, 'word': '▁it', 'start': 729, 'end': 732}, {'entity': 'r9:arg0|agt', 'score': 0.03753009, 'index': 180, 'word': '▁we', 'start': 740, 'end': 743}, {'entity': 'r9:root', 'score': 0.07860999, 'index': 181, 'word': '▁thought', 'start': 743, 'end': 751}, {'entity': 'r9:arg1|pat', 'score': 0.058518294, 'index': 182, 'word': '▁it', 'start': 751, 'end': 754}, {'entity': 'r9:root', 'score': 0.15334934, 'index': 184, 'word': '▁be', 'start': 760, 'end': 763}, {'entity': 'r8:arg2|atr', 'score': 0.077581845, 'index': 187, 'word': '▁way', 'start': 770, 'end': 774}, {'entity': 'r9:root', 'score': 0.1817208, 'index': 189, 'word': '▁engage', 'start': 777, 'end': 784}, {'entity': 'r8:arg1|tem', 'score': 0.06929828, 'index': 191, 'word': '▁public', 'start': 788, 'end': 795}, {'entity': 'r8:arg1|tem', 'score': 0.07084324, 'index': 209, 'word': '▁it', 'start': 853, 'end': 856}, {'entity': 'r9:root', 'score': 0.097537614, 'index': 210, 'word': '▁did', 'start': 856, 'end': 860}, {'entity': 'r8:arg1|tem', 'score': 0.22996224, 'index': 213, 'word': '▁face', 'start': 865, 'end': 870}, {'entity': 'r8:argM|tmp', 'score': 0.15224062, 'index': 216, 'word': '▁soon', 'start': 878, 'end': 883}, {'entity': 'r9:root', 'score': 0.14052552, 'index': 217, 'word': '▁became', 'start': 883, 'end': 890}, {'entity': 'r7:arg2|atr', 'score': 0.13227668, 'index': 220, 'word': '▁icon', 'start': 896, 'end': 901}, {'entity': 'r2:root', 'score': 0.34378532, 'index': 222, 'word': '▁shot', 'start': 902, 'end': 907}, {'entity': 'r4:argM|loc', 'score': 0.28516474, 'index': 223, 'word': '▁in', 'start': 907, 'end': 910}, {'entity': 'r3:root', 'score': 0.45903492, 'index': 226, 'word': '▁appeared', 'start': 918, 'end': 927}, {'entity': 'r4:arg2|loc', 'score': 0.18714885, 'index': 227, 'word': '▁in', 'start': 927, 'end': 930}, {'entity': 'r3:argM|tmp', 'score': 0.3332829, 'index': 248, 'word': '▁for', 'start': 1007, 'end': 1011}, {'entity': 'r4:arg0|agt', 'score': 0.4323846, 'index': 253, 'word': '▁people', 'start': 1026, 'end': 1033}, {'entity': 'r4:root', 'score': 0.6460269, 'index': 254, 'word': '▁thought', 'start': 1033, 'end': 1041}, {'entity': 'r5:arg1|tem', 'score': 0.7946849, 'index': 257, 'word': '▁land', 'start': 1053, 'end': 1058}, {'entity': 'r5:arg1|tem', 'score': 0.7655751, 'index': 258, 'word': 'form', 'start': 1058, 'end': 1062}, {'entity': 'r5:root', 'score': 0.87633103, 'index': 259, 'word': '▁was', 'start': 1062, 'end': 1066}, {'entity': 'r5:arg2|atr', 'score': 0.82787794, 'index': 260, 'word': '▁evidence', 'start': 1066, 'end': 1075}, {'entity': 'r5:arg0|agt', 'score': 0.4799223, 'index': 269, 'word': '▁scientist', 'start': 1104, 'end': 1114}, {'entity': 'r5:root', 'score': 0.8739596, 'index': 271, 'word': '▁wanted', 'start': 1115, 'end': 1122}, {'entity': 'r5:root', 'score': 0.4174833, 'index': 273, 'word': '▁hi', 'start': 1125, 'end': 1128}, {'entity': 'r6:arg1|pat', 'score': 0.72568583, 'index': 275, 'word': '▁it', 'start': 1130, 'end': 1133}, {'entity': 'r5:argM|adv', 'score': 0.48375428, 'index': 278, 'word': '▁really', 'start': 1138, 'end': 1145}, {'entity': 'r5:arg0|agt', 'score': 0.5601293, 'index': 281, 'word': '▁defender', 'start': 1150, 'end': 1159}, {'entity': 'r4:root', 'score': 0.5749663, 'index': 287, 'word': '▁wish', 'start': 1179, 'end': 1184}, {'entity': 'r5:root', 'score': 0.5627814, 'index': 289, 'word': '▁was', 'start': 1190, 'end': 1194}, {'entity': 'r5:arg1|tem', 'score': 0.7166488, 'index': 292, 'word': '▁civiliza', 'start': 1202, 'end': 1211}, {'entity': 'r5:arg1|tem', 'score': 0.45804867, 'index': 293, 'word': 'tion', 'start': 1211, 'end': 1215}, {'entity': 'r5:arg0|agt', 'score': 0.82322997, 'index': 297, 'word': '▁We', 'start': 1224, 'end': 1227}, {'entity': 'r5:root', 'score': 0.84057826, 'index': 298, 'word': '▁decided', 'start': 1227, 'end': 1235}, {'entity': 'r5:root', 'score': 0.49023774, 'index': 300, 'word': '▁take', 'start': 1238, 'end': 1243}, {'entity': 'r6:arg1|pat', 'score': 0.4983117, 'index': 302, 'word': '▁shot', 'start': 1251, 'end': 1256}, {'entity': 'r6:arg1|tem', 'score': 0.7063215, 'index': 307, 'word': '▁we', 'start': 1274, 'end': 1277}, {'entity': 'r6:root', 'score': 0.58319706, 'index': 308, 'word': '▁were', 'start': 1277, 'end': 1282}, {'entity': 'r6:arg2|atr', 'score': 0.6607443, 'index': 312, 'word': '▁wrong', 'start': 1285, 'end': 1291}, {'entity': 'r6:argM|tmp', 'score': 0.4239783, 'index': 314, 'word': '▁on', 'start': 1292, 'end': 1295}, {'entity': 'r7:arg0|agt', 'score': 0.61083174, 'index': 319, 'word': '▁Michael', 'start': 1310, 'end': 1318}, {'entity': 'r7:arg0|agt', 'score': 0.43220523, 'index': 320, 'word': '▁Malin', 'start': 1318, 'end': 1324}, {'entity': 'r7:root', 'score': 0.568414, 'index': 329, 'word': '▁took', 'start': 1357, 'end': 1362}, {'entity': 'r6:arg1|pat', 'score': 0.70329344, 'index': 331, 'word': '▁picture', 'start': 1364, 'end': 1372}, {'entity': 'r7:arg1|tem', 'score': 0.4258404, 'index': 332, 'word': '▁that', 'start': 1372, 'end': 1377}, {'entity': 'r7:root', 'score': 0.72852856, 'index': 333, 'word': '▁was', 'start': 1377, 'end': 1381}, {'entity': 'r7:arg2|atr', 'score': 0.4354887, 'index': 336, 'word': '▁sharp', 'start': 1391, 'end': 1397}, {'entity': 'r7:arg2|atr', 'score': 0.3854383, 'index': 337, 'word': 'er', 'start': 1397, 'end': 1399}, {'entity': 'r8:root', 'score': 0.4831441, 'index': 344, 'word': '▁reveal', 'start': 1432, 'end': 1439}, {'entity': 'r8:root', 'score': 0.3593413, 'index': 345, 'word': 'ing', 'start': 1439, 'end': 1442}, {'entity': 'r9:arg1|pat', 'score': 0.2637543, 'index': 348, 'word': '▁land', 'start': 1452, 'end': 1457}, {'entity': 'r8:arg1|tem', 'score': 0.20474246, 'index': 351, 'word': '▁which', 'start': 1462, 'end': 1468}, {'entity': 'r9:root', 'score': 0.3001293, 'index': 352, 'word': '▁meant', 'start': 1468, 'end': 1474}, {'entity': 'r9:arg1|pat', 'score': 0.060182273, 'index': 355, 'word': '▁monument', 'start': 1483, 'end': 1492}, {'entity': 'r8:arg1|tem', 'score': 0.17025372, 'index': 360, 'word': '▁picture', 'start': 1503, 'end': 1511}, {'entity': 'r9:root', 'score': 0.30658206, 'index': 361, 'word': '▁wasn', 'start': 1511, 'end': 1516}, {'entity': 'r8:arg2|atr', 'score': 0.1777854, 'index': 365, 'word': '▁clear', 'start': 1523, 'end': 1529}, {'entity': 'r9:arg0|agt', 'score': 0.04626326, 'index': 369, 'word': '▁which', 'start': 1537, 'end': 1543}, {'entity': 'r9:root', 'score': 0.13540097, 'index': 371, 'word': '▁mean', 'start': 1549, 'end': 1554}, {'entity': 'r9:arg1|pat', 'score': 0.12729116, 'index': 373, 'word': '▁mark', 'start': 1560, 'end': 1565}, {'entity': 'r9:arg1|pat', 'score': 0.11717077, 'index': 374, 'word': 'ings', 'start': 1565, 'end': 1569}, {'entity': 'r9:root', 'score': 0.15416586, 'index': 376, 'word': '▁hidden', 'start': 1574, 'end': 1581}, {'entity': 'r9:arg0|agt', 'score': 0.0479266, 'index': 377, 'word': '▁by', 'start': 1581, 'end': 1584}, {'entity': 'r8:arg1|tem', 'score': 0.06482706, 'index': 386, 'word': '▁rumor', 'start': 1608, 'end': 1614}, {'entity': 'r9:root', 'score': 0.09473133, 'index': 387, 'word': '▁started', 'start': 1614, 'end': 1622}, {'entity': 'r9:root', 'score': 0.08225819, 'index': 391, 'word': '▁prove', 'start': 1630, 'end': 1636}, {'entity': 'r9:arg1|pat', 'score': 0.11032233, 'index': 392, 'word': '▁them', 'start': 1636, 'end': 1641}, {'entity': 'r9:arg1|pat', 'score': 0.039644323, 'index': 393, 'word': '▁wrong', 'start': 1641, 'end': 1647}, {'entity': 'r8:argM|tmp', 'score': 0.058614645, 'index': 394, 'word': '▁on', 'start': 1647, 'end': 1650}, {'entity': 'r9:arg0|agt', 'score': 0.09158646, 'index': 399, 'word': '▁we', 'start': 1664, 'end': 1667}, {'entity': 'r9:root', 'score': 0.097704664, 'index': 400, 'word': '▁decided', 'start': 1667, 'end': 1675}, {'entity': 'r9:root', 'score': 0.06539654, 'index': 402, 'word': '▁take', 'start': 1678, 'end': 1683}, {'entity': 'r9:arg1|pat', 'score': 0.103520945, 'index': 404, 'word': '▁picture', 'start': 1691, 'end': 1699}, {'entity': 'r9:arg1|pat', 'score': 0.051061057, 'index': 408, 'word': '▁it', 'start': 1712, 'end': 1715}, {'entity': 'r10:root', 'score': 0.08168564, 'index': 409, 'word': '▁was', 'start': 1715, 'end': 1719}, {'entity': 'r9:arg1|pat', 'score': 0.03991396, 'index': 414, 'word': '▁day', 'start': 1738, 'end': 1742}, {'entity': 'r9:arg0|agt', 'score': 0.07429483, 'index': 416, 'word': '▁Malin', 'start': 1743, 'end': 1749}, {'entity': 'r9:arg0|agt', 'score': 0.0536705, 'index': 419, 'word': '▁team', 'start': 1751, 'end': 1756}, {'entity': 'r10:root', 'score': 0.08358262, 'index': 420, 'word': '▁capture', 'start': 1756, 'end': 1764}, {'entity': 'r10:root', 'score': 0.079094924, 'index': 421, 'word': 'd', 'start': 1764, 'end': 1765}, {'entity': 'r9:arg1|pat', 'score': 0.11371465, 'index': 424, 'word': '▁photo', 'start': 1776, 'end': 1782}, {'entity': 'r11:root', 'score': 0.0549004, 'index': 425, 'word': '▁using', 'start': 1782, 'end': 1788}, {'entity': 'r9:arg1|pat', 'score': 0.062476702, 'index': 427, 'word': '▁camera', 'start': 1792, 'end': 1799}, {'entity': 'r9:arg1|pat', 'score': 0.023913436, 'index': 434, 'word': '▁With', 'start': 1830, 'end': 1835}, {'entity': 'r9:arg0|agt', 'score': 0.04540858, 'index': 437, 'word': '▁you', 'start': 1847, 'end': 1851}, {'entity': 'r9:root', 'score': 0.1428813, 'index': 439, 'word': '▁discern', 'start': 1855, 'end': 1863}, {'entity': 'r9:arg1|pat', 'score': 0.12133165, 'index': 440, 'word': '▁things', 'start': 1863, 'end': 1870}, {'entity': 'r8:argM|loc', 'score': 0.03498705, 'index': 441, 'word': '▁in', 'start': 1870, 'end': 1873}, {'entity': 'r9:arg1|pat', 'score': 0.03346169, 'index': 453, 'word': '▁which', 'start': 1925, 'end': 1931}, {'entity': 'r9:root', 'score': 0.12779085, 'index': 454, 'word': '▁means', 'start': 1931, 'end': 1937}, {'entity': 'r9:root', 'score': 0.057673983, 'index': 456, 'word': '▁there', 'start': 1940, 'end': 1946}, {'entity': 'r9:root', 'score': 0.14751895, 'index': 457, 'word': '▁were', 'start': 1946, 'end': 1951}, {'entity': 'r9:arg1|pat', 'score': 0.07470571, 'index': 459, 'word': '▁sign', 'start': 1955, 'end': 1960}, {'entity': 'r9:arg0|agt', 'score': 0.054813575, 'index': 464, 'word': '▁you', 'start': 1970, 'end': 1974}, {'entity': 'r8:argM|adv', 'score': 0.036606755, 'index': 466, 'word': '▁easily', 'start': 1980, 'end': 1987}, {'entity': 'r9:root', 'score': 0.17583221, 'index': 467, 'word': '▁see', 'start': 1987, 'end': 1991}, {'entity': 'r9:arg2|atr', 'score': 0.052657988, 'index': 468, 'word': '▁what', 'start': 1991, 'end': 1996}, {'entity': 'r8:arg1|tem', 'score': 0.08003322, 'index': 469, 'word': '▁they', 'start': 1996, 'end': 2001}, {'entity': 'r9:root', 'score': 0.13172248, 'index': 470, 'word': '▁were', 'start': 2001, 'end': 2006}, {'entity': 'r9:arg1|pat', 'score': 0.10661528, 'index': 472, 'word': '▁What', 'start': 2007, 'end': 2012}, {'entity': 'r9:arg0|agt', 'score': 0.059802476, 'index': 474, 'word': '▁picture', 'start': 2016, 'end': 2024}, {'entity': 'r9:root', 'score': 0.14850138, 'index': 475, 'word': '▁showed', 'start': 2024, 'end': 2031}, {'entity': 'r9:root', 'score': 0.23177616, 'index': 476, 'word': '▁was', 'start': 2031, 'end': 2035}, {'entity': 'r8:arg2|atr', 'score': 0.21636702, 'index': 478, 'word': '▁but', 'start': 2039, 'end': 2043}, {'entity': 'r7:arg2|atr', 'score': 0.2325799, 'index': 479, 'word': 'te', 'start': 2043, 'end': 2045}, {'entity': 'r7:arg1|tem', 'score': 0.26516578, 'index': 483, 'word': '▁which', 'start': 2054, 'end': 2060}, {'entity': 'r7:root', 'score': 0.20337786, 'index': 484, 'word': '▁are', 'start': 2060, 'end': 2064}, {'entity': 'r7:arg2|atr', 'score': 0.2973422, 'index': 485, 'word': '▁land', 'start': 2064, 'end': 2069}, {'entity': 'r7:arg2|atr', 'score': 0.31532818, 'index': 486, 'word': 'form', 'start': 2069, 'end': 2073}, {'entity': 'r7:arg2|atr', 'score': 0.2744848, 'index': 487, 'word': 's', 'start': 2073, 'end': 2074}]\n",
      "{0: 'O', 1: 'r0:arg0|agt', 2: 'r0:arg0|cau', 3: 'r0:arg0|exp', 4: 'r0:arg0|pat', 5: 'r0:arg0|src', 6: 'r0:arg1|ext', 7: 'r0:arg1|loc', 8: 'r0:arg1|pat', 9: 'r0:arg1|tem', 10: 'r0:arg2|atr', 11: 'r0:arg2|ben', 12: 'r0:arg2|efi', 13: 'r0:arg2|exp', 14: 'r0:arg2|ext', 15: 'r0:arg2|ins', 16: 'r0:arg2|loc', 17: 'r0:arg2|tem', 18: 'r0:arg3|ben', 19: 'r0:arg3|ein', 20: 'r0:arg3|exp', 21: 'r0:arg3|fin', 22: 'r0:arg3|ins', 23: 'r0:arg3|loc', 24: 'r0:arg3|ori', 25: 'r0:arg4|des', 26: 'r0:arg4|efi', 27: 'r0:argM|LOC', 28: 'r0:argM|adv', 29: 'r0:argM|atr', 30: 'r0:argM|cau', 31: 'r0:argM|ext', 32: 'r0:argM|fin', 33: 'r0:argM|ins', 34: 'r0:argM|loc', 35: 'r0:argM|mnr', 36: 'r0:argM|tmp', 37: 'r0:root', 38: 'r10:arg0|agt', 39: 'r10:arg1|pat', 40: 'r10:arg1|tem', 41: 'r10:arg2|atr', 42: 'r10:arg2|ben', 43: 'r10:arg2|efi', 44: 'r10:arg2|loc', 45: 'r10:arg3|ben', 46: 'r10:arg4|des', 47: 'r10:argM|adv', 48: 'r10:argM|atr', 49: 'r10:argM|fin', 50: 'r10:argM|loc', 51: 'r10:argM|tmp', 52: 'r10:root', 53: 'r11:arg0|agt', 54: 'r11:arg0|cau', 55: 'r11:arg1|pat', 56: 'r11:arg1|tem', 57: 'r11:arg2|atr', 58: 'r11:arg2|ben', 59: 'r11:arg2|loc', 60: 'r11:arg4|des', 61: 'r11:argM|adv', 62: 'r11:argM|loc', 63: 'r11:argM|mnr', 64: 'r11:argM|tmp', 65: 'r11:root', 66: 'r12:arg0|agt', 67: 'r12:arg0|cau', 68: 'r12:arg1|pat', 69: 'r12:arg1|tem', 70: 'r12:arg2|atr', 71: 'r12:argM|adv', 72: 'r12:argM|cau', 73: 'r12:argM|loc', 74: 'r12:argM|tmp', 75: 'r12:root', 76: 'r13:arg0|agt', 77: 'r13:arg0|cau', 78: 'r13:arg1|pat', 79: 'r13:arg1|tem', 80: 'r13:arg2|atr', 81: 'r13:arg2|ben', 82: 'r13:argM|adv', 83: 'r13:argM|atr', 84: 'r13:argM|loc', 85: 'r13:root', 86: 'r14:arg0|agt', 87: 'r14:arg1|pat', 88: 'r14:arg2|ben', 89: 'r14:argM|adv', 90: 'r14:argM|loc', 91: 'r14:argM|mnr', 92: 'r14:root', 93: 'r15:arg0|cau', 94: 'r15:arg1|tem', 95: 'r15:arg2|atr', 96: 'r15:arg3|ben', 97: 'r15:root', 98: 'r16:arg0|agt', 99: 'r16:arg0|cau', 100: 'r16:arg1|pat', 101: 'r16:arg1|tem', 102: 'r16:argM|loc', 103: 'r16:argM|tmp', 104: 'r16:root', 105: 'r1:arg0|agt', 106: 'r1:arg0|cau', 107: 'r1:arg0|exp', 108: 'r1:arg0|src', 109: 'r1:arg1|ext', 110: 'r1:arg1|loc', 111: 'r1:arg1|pat', 112: 'r1:arg1|tem', 113: 'r1:arg2|atr', 114: 'r1:arg2|ben', 115: 'r1:arg2|efi', 116: 'r1:arg2|exp', 117: 'r1:arg2|ext', 118: 'r1:arg2|ins', 119: 'r1:arg2|loc', 120: 'r1:arg3|atr', 121: 'r1:arg3|ben', 122: 'r1:arg3|des', 123: 'r1:arg3|ein', 124: 'r1:arg3|exp', 125: 'r1:arg3|fin', 126: 'r1:arg3|ins', 127: 'r1:arg3|ori', 128: 'r1:arg4|des', 129: 'r1:arg4|efi', 130: 'r1:argM|adv', 131: 'r1:argM|atr', 132: 'r1:argM|cau', 133: 'r1:argM|ext', 134: 'r1:argM|fin', 135: 'r1:argM|ins', 136: 'r1:argM|loc', 137: 'r1:argM|mnr', 138: 'r1:argM|tmp', 139: 'r1:root', 140: 'r2:arg0|agt', 141: 'r2:arg0|cau', 142: 'r2:arg0|exp', 143: 'r2:arg0|src', 144: 'r2:arg1|ext', 145: 'r2:arg1|loc', 146: 'r2:arg1|pat', 147: 'r2:arg1|tem', 148: 'r2:arg2|atr', 149: 'r2:arg2|ben', 150: 'r2:arg2|efi', 151: 'r2:arg2|exp', 152: 'r2:arg2|ext', 153: 'r2:arg2|ins', 154: 'r2:arg2|loc', 155: 'r2:arg3|atr', 156: 'r2:arg3|ben', 157: 'r2:arg3|ein', 158: 'r2:arg3|exp', 159: 'r2:arg3|fin', 160: 'r2:arg3|loc', 161: 'r2:arg3|ori', 162: 'r2:arg4|des', 163: 'r2:arg4|efi', 164: 'r2:argM|adv', 165: 'r2:argM|atr', 166: 'r2:argM|cau', 167: 'r2:argM|ext', 168: 'r2:argM|fin', 169: 'r2:argM|ins', 170: 'r2:argM|loc', 171: 'r2:argM|mnr', 172: 'r2:argM|tmp', 173: 'r2:root', 174: 'r3:arg0|agt', 175: 'r3:arg0|cau', 176: 'r3:arg0|exp', 177: 'r3:arg0|src', 178: 'r3:arg1|ext', 179: 'r3:arg1|loc', 180: 'r3:arg1|pat', 181: 'r3:arg1|tem', 182: 'r3:arg2|atr', 183: 'r3:arg2|ben', 184: 'r3:arg2|efi', 185: 'r3:arg2|exp', 186: 'r3:arg2|ext', 187: 'r3:arg2|ins', 188: 'r3:arg2|loc', 189: 'r3:arg2|tem', 190: 'r3:arg3|ben', 191: 'r3:arg3|ein', 192: 'r3:arg3|fin', 193: 'r3:arg3|loc', 194: 'r3:arg3|ori', 195: 'r3:arg4|des', 196: 'r3:arg4|efi', 197: 'r3:argM|adv', 198: 'r3:argM|atr', 199: 'r3:argM|cau', 200: 'r3:argM|ext', 201: 'r3:argM|fin', 202: 'r3:argM|ins', 203: 'r3:argM|loc', 204: 'r3:argM|mnr', 205: 'r3:argM|tmp', 206: 'r3:root', 207: 'r4:arg0|agt', 208: 'r4:arg0|cau', 209: 'r4:arg0|exp', 210: 'r4:arg0|src', 211: 'r4:arg1|ext', 212: 'r4:arg1|loc', 213: 'r4:arg1|pat', 214: 'r4:arg1|tem', 215: 'r4:arg2|atr', 216: 'r4:arg2|ben', 217: 'r4:arg2|efi', 218: 'r4:arg2|exp', 219: 'r4:arg2|ext', 220: 'r4:arg2|ins', 221: 'r4:arg2|loc', 222: 'r4:arg3|ben', 223: 'r4:arg3|ein', 224: 'r4:arg3|exp', 225: 'r4:arg3|fin', 226: 'r4:arg3|ori', 227: 'r4:arg4|des', 228: 'r4:arg4|efi', 229: 'r4:argM|adv', 230: 'r4:argM|atr', 231: 'r4:argM|cau', 232: 'r4:argM|ext', 233: 'r4:argM|fin', 234: 'r4:argM|ins', 235: 'r4:argM|loc', 236: 'r4:argM|mnr', 237: 'r4:argM|tmp', 238: 'r4:root', 239: 'r5:arg0|agt', 240: 'r5:arg0|cau', 241: 'r5:arg1|ext', 242: 'r5:arg1|loc', 243: 'r5:arg1|pat', 244: 'r5:arg1|tem', 245: 'r5:arg2|atr', 246: 'r5:arg2|ben', 247: 'r5:arg2|efi', 248: 'r5:arg2|exp', 249: 'r5:arg2|ext', 250: 'r5:arg2|loc', 251: 'r5:arg3|ben', 252: 'r5:arg3|ein', 253: 'r5:arg3|fin', 254: 'r5:arg3|ins', 255: 'r5:arg3|ori', 256: 'r5:arg4|des', 257: 'r5:arg4|efi', 258: 'r5:argM|adv', 259: 'r5:argM|atr', 260: 'r5:argM|cau', 261: 'r5:argM|ext', 262: 'r5:argM|fin', 263: 'r5:argM|loc', 264: 'r5:argM|mnr', 265: 'r5:argM|tmp', 266: 'r5:root', 267: 'r6:arg0|agt', 268: 'r6:arg0|cau', 269: 'r6:arg1|loc', 270: 'r6:arg1|pat', 271: 'r6:arg1|tem', 272: 'r6:arg2|atr', 273: 'r6:arg2|ben', 274: 'r6:arg2|efi', 275: 'r6:arg2|exp', 276: 'r6:arg2|ext', 277: 'r6:arg2|loc', 278: 'r6:arg3|ben', 279: 'r6:arg3|ori', 280: 'r6:arg4|des', 281: 'r6:argM|adv', 282: 'r6:argM|atr', 283: 'r6:argM|cau', 284: 'r6:argM|ext', 285: 'r6:argM|fin', 286: 'r6:argM|loc', 287: 'r6:argM|mnr', 288: 'r6:argM|tmp', 289: 'r6:root', 290: 'r7:arg0|agt', 291: 'r7:arg0|cau', 292: 'r7:arg1|loc', 293: 'r7:arg1|pat', 294: 'r7:arg1|tem', 295: 'r7:arg2|atr', 296: 'r7:arg2|ben', 297: 'r7:arg2|efi', 298: 'r7:arg2|loc', 299: 'r7:arg3|ben', 300: 'r7:arg3|exp', 301: 'r7:arg3|fin', 302: 'r7:arg3|ori', 303: 'r7:arg4|des', 304: 'r7:argM|adv', 305: 'r7:argM|atr', 306: 'r7:argM|cau', 307: 'r7:argM|fin', 308: 'r7:argM|ins', 309: 'r7:argM|loc', 310: 'r7:argM|mnr', 311: 'r7:argM|tmp', 312: 'r7:root', 313: 'r8:arg0|agt', 314: 'r8:arg0|cau', 315: 'r8:arg0|src', 316: 'r8:arg1|pat', 317: 'r8:arg1|tem', 318: 'r8:arg2|atr', 319: 'r8:arg2|ben', 320: 'r8:arg2|ext', 321: 'r8:arg2|loc', 322: 'r8:arg3|ori', 323: 'r8:arg4|des', 324: 'r8:argM|adv', 325: 'r8:argM|cau', 326: 'r8:argM|ext', 327: 'r8:argM|fin', 328: 'r8:argM|loc', 329: 'r8:argM|mnr', 330: 'r8:argM|tmp', 331: 'r8:root', 332: 'r9:arg0|agt', 333: 'r9:arg0|cau', 334: 'r9:arg1|pat', 335: 'r9:arg1|tem', 336: 'r9:arg2|atr', 337: 'r9:arg2|ben', 338: 'r9:arg2|ins', 339: 'r9:arg2|loc', 340: 'r9:arg4|des', 341: 'r9:argM|adv', 342: 'r9:argM|cau', 343: 'r9:argM|fin', 344: 'r9:argM|loc', 345: 'r9:argM|mnr', 346: 'r9:argM|tmp', 347: 'r9:root'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mbruton/spa_en_XLM-R\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"mbruton/spa_en_XLM-R\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "172bc82e-129d-40c1-9f33-0a80eaa9237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b229b-c017-4db6-bdc9-94ef7210508e",
   "metadata": {},
   "source": [
    "## 20 mbruton/gal_enptsp_mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "61034c1e-ab67-4221-9aac-bdd54263f6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'r0:root', 'score': 0.69288, 'index': 6, 'word': 're', 'start': 11, 'end': 13}, {'entity': 'r1:root', 'score': 0.8305716, 'index': 16, 'word': 'tell', 'start': 54, 'end': 58}, {'entity': 'r2:arg1', 'score': 0.65180844, 'index': 20, 'word': 'story', 'start': 72, 'end': 77}, {'entity': 'r5:root', 'score': 0.4588822, 'index': 44, 'word': 'created', 'start': 180, 'end': 187}, {'entity': 'r6:root', 'score': 0.49542937, 'index': 64, 'word': 'ci', 'start': 264, 'end': 266}, {'entity': 'r6:root', 'score': 0.45653778, 'index': 65, 'word': '##rc', 'start': 266, 'end': 268}, {'entity': 'r6:root', 'score': 0.31713662, 'index': 66, 'word': '##ling', 'start': 268, 'end': 272}, {'entity': 'r6:arg1', 'score': 0.37775016, 'index': 68, 'word': 'planet', 'start': 277, 'end': 283}, {'entity': 'r6:root', 'score': 0.41948485, 'index': 70, 'word': 'sna', 'start': 285, 'end': 288}, {'entity': 'r6:root', 'score': 0.28788853, 'index': 71, 'word': '##pping', 'start': 288, 'end': 293}, {'entity': 'r6:arg1', 'score': 0.3409012, 'index': 72, 'word': 'photos', 'start': 294, 'end': 300}, {'entity': 'r8:root', 'score': 0.29355028, 'index': 76, 'word': 'spotted', 'start': 310, 'end': 317}, {'entity': 'r8:root', 'score': 0.27520663, 'index': 90, 'word': 'figure', 'start': 370, 'end': 376}, {'entity': 'r8:root', 'score': 0.1575869, 'index': 170, 'word': 'formed', 'start': 693, 'end': 699}, {'entity': 'r8:root', 'score': 0.17861637, 'index': 178, 'word': 'announced', 'start': 720, 'end': 729}, {'entity': 'r8:root', 'score': 0.17304042, 'index': 227, 'word': 'appeared', 'start': 919, 'end': 927}, {'entity': 'r5:root', 'score': 0.31188446, 'index': 253, 'word': 'thought', 'start': 1034, 'end': 1041}, {'entity': 'r5:root', 'score': 0.5253225, 'index': 271, 'word': 'hide', 'start': 1126, 'end': 1130}, {'entity': 'r6:arg1', 'score': 0.34368348, 'index': 272, 'word': 'it', 'start': 1131, 'end': 1133}, {'entity': 'r2:root', 'score': 0.6164163, 'index': 284, 'word': 'wish', 'start': 1180, 'end': 1184}, {'entity': 'r4:root', 'score': 0.31295392, 'index': 296, 'word': 'take', 'start': 1239, 'end': 1243}, {'entity': 'r2:root', 'score': 0.26798648, 'index': 301, 'word': 'make', 'start': 1265, 'end': 1269}, {'entity': 'r8:root', 'score': 0.10455666, 'index': 326, 'word': 'took', 'start': 1358, 'end': 1362}, {'entity': 'r7:arg1', 'score': 0.13289611, 'index': 328, 'word': 'picture', 'start': 1365, 'end': 1372}, {'entity': 'r8:root', 'score': 0.08957504, 'index': 341, 'word': 'reveal', 'start': 1433, 'end': 1439}]\n",
      "{0: 'O', 1: 'r0:arg0', 2: 'r0:arg1', 3: 'r0:arg2', 4: 'r0:root', 5: 'r10:arg0', 6: 'r10:arg1', 7: 'r10:root', 8: 'r11:arg0', 9: 'r11:root', 10: 'r12:arg1', 11: 'r12:root', 12: 'r13:arg1', 13: 'r13:root', 14: 'r1:arg0', 15: 'r1:arg1', 16: 'r1:arg2', 17: 'r1:root', 18: 'r2:arg0', 19: 'r2:arg1', 20: 'r2:arg2', 21: 'r2:root', 22: 'r3:arg0', 23: 'r3:arg1', 24: 'r3:arg2', 25: 'r3:root', 26: 'r4:arg0', 27: 'r4:arg1', 28: 'r4:arg2', 29: 'r4:root', 30: 'r5:arg0', 31: 'r5:arg1', 32: 'r5:arg2', 33: 'r5:root', 34: 'r6:arg0', 35: 'r6:arg1', 36: 'r6:arg2', 37: 'r6:root', 38: 'r7:arg0', 39: 'r7:arg1', 40: 'r7:arg2', 41: 'r7:root', 42: 'r8:arg0', 43: 'r8:arg1', 44: 'r8:arg2', 45: 'r8:root', 46: 'r9:arg0', 47: 'r9:arg1', 48: 'r9:arg2', 49: 'r9:root'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mbruton/gal_enptsp_mBERT\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"mbruton/gal_enptsp_mBERT\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf510a-a810-4cd6-9e03-675e31a660eb",
   "metadata": {},
   "source": [
    "## 21 benjamin/wtp-bert-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "4e3dff5d-8b2c-4f45-a475-04f7b11e2c41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `bert-char` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bert-char'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[346], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-bert-tiny\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-bert-tiny\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `bert-char` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-bert-tiny\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-bert-tiny\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d370f-6909-47ab-8f71-6ae001b06067",
   "metadata": {},
   "source": [
    "## 22 benjamin/wtp-canine-s-1l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "3dd00e55-9d25-4119-b4f0-ff1d64666c52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'la-canine'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[347], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-1l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-1l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-canine-s-1l\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-canine-s-1l\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68ad13-321b-443e-8a71-703ee218e1d2",
   "metadata": {},
   "source": [
    "## 23 benjamin/wtp-canine-s-6l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "ee272836-552e-4335-86b5-6ef64c83294b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'la-canine'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[348], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-6l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-6l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-canine-s-6l\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-canine-s-6l\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5fa36-7761-47b3-a4d1-110ad004db06",
   "metadata": {},
   "source": [
    "## 24 benjamin/wtp-canine-s-9l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "1759bc6d-9a6a-4466-8226-3b8508170373",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'la-canine'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[349], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-9l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-9l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-canine-s-9l\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-canine-s-9l\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2509820-70e1-49ce-959c-74430c8ace62",
   "metadata": {},
   "source": [
    "## 25 benjamin/wtp-canine-s-1l-no-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "36801845-c8c2-49b4-9157-3f915be02912",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'la-canine'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[350], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-1l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-1l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-canine-s-1l-no-adapters\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-canine-s-1l-no-adapters\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8cf71-eb95-490f-85f8-81a8a77a7471",
   "metadata": {},
   "source": [
    "## 26 benjamin/wtp-canine-s-6l-no-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "a077b70f-2cb9-4735-a476-b06831039d23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'la-canine'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[351], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-6l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-6l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-canine-s-6l-no-adapters\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-canine-s-6l-no-adapters\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f962e5f-b928-4ad4-9c77-2f042e8c53d6",
   "metadata": {},
   "source": [
    "## 27 benjamin/wtp-canine-s-12l-no-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "bce862a8-9ec5-4a82-bf78-d75851e9bf0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'la-canine'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[352], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-12l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-12l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-canine-s-12l-no-adapters\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-canine-s-12l-no-adapters\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afb232-2a61-45c6-ae7a-d9bef25fb3d3",
   "metadata": {},
   "source": [
    "## 28 Posos/ClinicalNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c11476c7-d694-49a6-bd2b-7e300e81f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Posos/ClinicalNER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Posos/ClinicalNER\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2e211-d26e-4b3b-b395-6817fef1adba",
   "metadata": {},
   "source": [
    "## 29 numind/NuNER-multilingual-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "777410bc-d134-4439-9f3c-8840fa5a532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at numind/NuNER-multilingual-v0.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_1', 'score': 0.71984005, 'index': 1, 'word': 'So', 'start': 0, 'end': 2}, {'entity': 'LABEL_1', 'score': 0.73801607, 'index': 2, 'word': ',', 'start': 2, 'end': 3}, {'entity': 'LABEL_1', 'score': 0.7332495, 'index': 3, 'word': 'if', 'start': 4, 'end': 6}, {'entity': 'LABEL_1', 'score': 0.72144157, 'index': 4, 'word': 'you', 'start': 7, 'end': 10}, {'entity': 'LABEL_1', 'score': 0.68311286, 'index': 5, 'word': \"'\", 'start': 10, 'end': 11}, {'entity': 'LABEL_1', 'score': 0.7472436, 'index': 6, 'word': 're', 'start': 11, 'end': 13}, {'entity': 'LABEL_1', 'score': 0.7369742, 'index': 7, 'word': 'a', 'start': 14, 'end': 15}, {'entity': 'LABEL_1', 'score': 0.79003894, 'index': 8, 'word': 'NASA', 'start': 16, 'end': 20}, {'entity': 'LABEL_1', 'score': 0.81120235, 'index': 9, 'word': 'scientist', 'start': 21, 'end': 30}, {'entity': 'LABEL_1', 'score': 0.70407075, 'index': 10, 'word': ',', 'start': 30, 'end': 31}, {'entity': 'LABEL_1', 'score': 0.6875325, 'index': 11, 'word': 'you', 'start': 32, 'end': 35}, {'entity': 'LABEL_1', 'score': 0.6866596, 'index': 12, 'word': 'should', 'start': 36, 'end': 42}, {'entity': 'LABEL_1', 'score': 0.64229923, 'index': 13, 'word': 'be', 'start': 43, 'end': 45}, {'entity': 'LABEL_1', 'score': 0.6947452, 'index': 14, 'word': 'able', 'start': 46, 'end': 50}, {'entity': 'LABEL_1', 'score': 0.6886766, 'index': 15, 'word': 'to', 'start': 51, 'end': 53}, {'entity': 'LABEL_1', 'score': 0.75225806, 'index': 16, 'word': 'tell', 'start': 54, 'end': 58}, {'entity': 'LABEL_1', 'score': 0.7414947, 'index': 17, 'word': 'me', 'start': 59, 'end': 61}, {'entity': 'LABEL_1', 'score': 0.7612589, 'index': 18, 'word': 'the', 'start': 62, 'end': 65}, {'entity': 'LABEL_1', 'score': 0.75991243, 'index': 19, 'word': 'whole', 'start': 66, 'end': 71}, {'entity': 'LABEL_1', 'score': 0.7680968, 'index': 20, 'word': 'story', 'start': 72, 'end': 77}, {'entity': 'LABEL_1', 'score': 0.7661695, 'index': 21, 'word': 'about', 'start': 78, 'end': 83}, {'entity': 'LABEL_1', 'score': 0.7923602, 'index': 22, 'word': 'the', 'start': 84, 'end': 87}, {'entity': 'LABEL_1', 'score': 0.8179057, 'index': 23, 'word': 'Face', 'start': 88, 'end': 92}, {'entity': 'LABEL_1', 'score': 0.83616984, 'index': 24, 'word': 'On', 'start': 93, 'end': 95}, {'entity': 'LABEL_1', 'score': 0.77161944, 'index': 25, 'word': 'Mars', 'start': 96, 'end': 100}, {'entity': 'LABEL_1', 'score': 0.77334476, 'index': 26, 'word': ',', 'start': 100, 'end': 101}, {'entity': 'LABEL_1', 'score': 0.73123896, 'index': 27, 'word': 'which', 'start': 102, 'end': 107}, {'entity': 'LABEL_1', 'score': 0.7242855, 'index': 28, 'word': 'obvious', 'start': 108, 'end': 115}, {'entity': 'LABEL_1', 'score': 0.70659983, 'index': 29, 'word': '##ly', 'start': 115, 'end': 117}, {'entity': 'LABEL_1', 'score': 0.70492053, 'index': 30, 'word': 'is', 'start': 118, 'end': 120}, {'entity': 'LABEL_1', 'score': 0.6999787, 'index': 31, 'word': 'evidence', 'start': 121, 'end': 129}, {'entity': 'LABEL_1', 'score': 0.70931506, 'index': 32, 'word': 'that', 'start': 130, 'end': 134}, {'entity': 'LABEL_1', 'score': 0.6945473, 'index': 33, 'word': 'there', 'start': 135, 'end': 140}, {'entity': 'LABEL_1', 'score': 0.7170606, 'index': 34, 'word': 'is', 'start': 141, 'end': 143}, {'entity': 'LABEL_1', 'score': 0.7405042, 'index': 35, 'word': 'life', 'start': 144, 'end': 148}, {'entity': 'LABEL_1', 'score': 0.7860015, 'index': 36, 'word': 'on', 'start': 149, 'end': 151}, {'entity': 'LABEL_1', 'score': 0.7395303, 'index': 37, 'word': 'Mars', 'start': 152, 'end': 156}, {'entity': 'LABEL_1', 'score': 0.74499786, 'index': 38, 'word': ',', 'start': 156, 'end': 157}, {'entity': 'LABEL_1', 'score': 0.6618726, 'index': 39, 'word': 'and', 'start': 158, 'end': 161}, {'entity': 'LABEL_1', 'score': 0.70406127, 'index': 40, 'word': 'that', 'start': 162, 'end': 166}, {'entity': 'LABEL_1', 'score': 0.7992562, 'index': 41, 'word': 'the', 'start': 167, 'end': 170}, {'entity': 'LABEL_1', 'score': 0.8085065, 'index': 42, 'word': 'face', 'start': 171, 'end': 175}, {'entity': 'LABEL_1', 'score': 0.7634861, 'index': 43, 'word': 'was', 'start': 176, 'end': 179}, {'entity': 'LABEL_1', 'score': 0.7589782, 'index': 44, 'word': 'created', 'start': 180, 'end': 187}, {'entity': 'LABEL_1', 'score': 0.70278364, 'index': 45, 'word': 'by', 'start': 188, 'end': 190}, {'entity': 'LABEL_1', 'score': 0.6042582, 'index': 46, 'word': 'alien', 'start': 191, 'end': 196}, {'entity': 'LABEL_1', 'score': 0.578272, 'index': 47, 'word': '##s', 'start': 196, 'end': 197}, {'entity': 'LABEL_1', 'score': 0.6508598, 'index': 48, 'word': ',', 'start': 197, 'end': 198}, {'entity': 'LABEL_1', 'score': 0.65776867, 'index': 49, 'word': 'correct', 'start': 199, 'end': 206}, {'entity': 'LABEL_1', 'score': 0.66189235, 'index': 50, 'word': '?', 'start': 206, 'end': 207}, {'entity': 'LABEL_1', 'score': 0.70917964, 'index': 51, 'word': '\"', 'start': 207, 'end': 208}, {'entity': 'LABEL_1', 'score': 0.73407716, 'index': 52, 'word': 'No', 'start': 209, 'end': 211}, {'entity': 'LABEL_1', 'score': 0.7614063, 'index': 53, 'word': ',', 'start': 211, 'end': 212}, {'entity': 'LABEL_1', 'score': 0.80251765, 'index': 54, 'word': 'twenty', 'start': 213, 'end': 219}, {'entity': 'LABEL_1', 'score': 0.792191, 'index': 55, 'word': 'five', 'start': 220, 'end': 224}, {'entity': 'LABEL_1', 'score': 0.78709924, 'index': 56, 'word': 'years', 'start': 225, 'end': 230}, {'entity': 'LABEL_1', 'score': 0.79996437, 'index': 57, 'word': 'ago', 'start': 231, 'end': 234}, {'entity': 'LABEL_1', 'score': 0.76745087, 'index': 58, 'word': ',', 'start': 234, 'end': 235}, {'entity': 'LABEL_1', 'score': 0.7806239, 'index': 59, 'word': 'our', 'start': 236, 'end': 239}, {'entity': 'LABEL_1', 'score': 0.73189795, 'index': 60, 'word': 'Viking', 'start': 240, 'end': 246}, {'entity': 'LABEL_1', 'score': 0.7565487, 'index': 61, 'word': '1', 'start': 247, 'end': 248}, {'entity': 'LABEL_1', 'score': 0.76608074, 'index': 62, 'word': 'spacecraft', 'start': 249, 'end': 259}, {'entity': 'LABEL_1', 'score': 0.7808929, 'index': 63, 'word': 'was', 'start': 260, 'end': 263}, {'entity': 'LABEL_1', 'score': 0.84131616, 'index': 64, 'word': 'ci', 'start': 264, 'end': 266}, {'entity': 'LABEL_1', 'score': 0.8425548, 'index': 65, 'word': '##rc', 'start': 266, 'end': 268}, {'entity': 'LABEL_1', 'score': 0.8575051, 'index': 66, 'word': '##ling', 'start': 268, 'end': 272}, {'entity': 'LABEL_1', 'score': 0.81599665, 'index': 67, 'word': 'the', 'start': 273, 'end': 276}, {'entity': 'LABEL_1', 'score': 0.7990209, 'index': 68, 'word': 'planet', 'start': 277, 'end': 283}, {'entity': 'LABEL_1', 'score': 0.7858126, 'index': 69, 'word': ',', 'start': 283, 'end': 284}, {'entity': 'LABEL_1', 'score': 0.83325493, 'index': 70, 'word': 'sna', 'start': 285, 'end': 288}, {'entity': 'LABEL_1', 'score': 0.8228549, 'index': 71, 'word': '##pping', 'start': 288, 'end': 293}, {'entity': 'LABEL_1', 'score': 0.8006405, 'index': 72, 'word': 'photos', 'start': 294, 'end': 300}, {'entity': 'LABEL_1', 'score': 0.7927411, 'index': 73, 'word': ',', 'start': 300, 'end': 301}, {'entity': 'LABEL_1', 'score': 0.7979515, 'index': 74, 'word': 'when', 'start': 302, 'end': 306}, {'entity': 'LABEL_1', 'score': 0.7468175, 'index': 75, 'word': 'it', 'start': 307, 'end': 309}, {'entity': 'LABEL_1', 'score': 0.8362597, 'index': 76, 'word': 'spotted', 'start': 310, 'end': 317}, {'entity': 'LABEL_1', 'score': 0.7179883, 'index': 77, 'word': 'the', 'start': 318, 'end': 321}, {'entity': 'LABEL_1', 'score': 0.73556095, 'index': 78, 'word': 'sh', 'start': 322, 'end': 324}, {'entity': 'LABEL_1', 'score': 0.729346, 'index': 79, 'word': '##adow', 'start': 324, 'end': 328}, {'entity': 'LABEL_1', 'score': 0.7376273, 'index': 80, 'word': '##y', 'start': 328, 'end': 329}, {'entity': 'LABEL_1', 'score': 0.73925066, 'index': 81, 'word': 'like', 'start': 330, 'end': 334}, {'entity': 'LABEL_1', 'score': 0.71821326, 'index': 82, 'word': '##ness', 'start': 334, 'end': 338}, {'entity': 'LABEL_1', 'score': 0.7665671, 'index': 83, 'word': 'of', 'start': 339, 'end': 341}, {'entity': 'LABEL_1', 'score': 0.7226397, 'index': 84, 'word': 'a', 'start': 342, 'end': 343}, {'entity': 'LABEL_1', 'score': 0.6861323, 'index': 85, 'word': 'human', 'start': 344, 'end': 349}, {'entity': 'LABEL_1', 'score': 0.7599484, 'index': 86, 'word': 'face', 'start': 350, 'end': 354}, {'entity': 'LABEL_1', 'score': 0.69882786, 'index': 87, 'word': '.', 'start': 354, 'end': 355}, {'entity': 'LABEL_1', 'score': 0.75464, 'index': 88, 'word': 'Us', 'start': 356, 'end': 358}, {'entity': 'LABEL_1', 'score': 0.7767833, 'index': 89, 'word': 'scientists', 'start': 359, 'end': 369}, {'entity': 'LABEL_1', 'score': 0.7329959, 'index': 90, 'word': 'figure', 'start': 370, 'end': 376}, {'entity': 'LABEL_1', 'score': 0.712735, 'index': 91, 'word': '##d', 'start': 376, 'end': 377}, {'entity': 'LABEL_1', 'score': 0.7192607, 'index': 92, 'word': 'out', 'start': 378, 'end': 381}, {'entity': 'LABEL_1', 'score': 0.68940145, 'index': 93, 'word': 'that', 'start': 382, 'end': 386}, {'entity': 'LABEL_1', 'score': 0.7401924, 'index': 94, 'word': 'it', 'start': 387, 'end': 389}, {'entity': 'LABEL_1', 'score': 0.6560321, 'index': 95, 'word': 'was', 'start': 390, 'end': 393}, {'entity': 'LABEL_1', 'score': 0.6377453, 'index': 96, 'word': 'just', 'start': 394, 'end': 398}, {'entity': 'LABEL_1', 'score': 0.6284793, 'index': 97, 'word': 'another', 'start': 399, 'end': 406}, {'entity': 'LABEL_1', 'score': 0.68464476, 'index': 98, 'word': 'Mart', 'start': 407, 'end': 411}, {'entity': 'LABEL_1', 'score': 0.66553444, 'index': 99, 'word': '##ian', 'start': 411, 'end': 414}, {'entity': 'LABEL_1', 'score': 0.6893722, 'index': 100, 'word': 'mesa', 'start': 415, 'end': 419}, {'entity': 'LABEL_1', 'score': 0.6662047, 'index': 101, 'word': ',', 'start': 419, 'end': 420}, {'entity': 'LABEL_1', 'score': 0.6929259, 'index': 102, 'word': 'common', 'start': 421, 'end': 427}, {'entity': 'LABEL_1', 'score': 0.66761, 'index': 103, 'word': 'around', 'start': 428, 'end': 434}, {'entity': 'LABEL_1', 'score': 0.6142734, 'index': 104, 'word': 'C', 'start': 435, 'end': 436}, {'entity': 'LABEL_1', 'score': 0.6142866, 'index': 105, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'LABEL_1', 'score': 0.61786187, 'index': 106, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'LABEL_1', 'score': 0.6594537, 'index': 107, 'word': ',', 'start': 442, 'end': 443}, {'entity': 'LABEL_1', 'score': 0.6563783, 'index': 108, 'word': 'only', 'start': 444, 'end': 448}, {'entity': 'LABEL_1', 'score': 0.69069314, 'index': 109, 'word': 'this', 'start': 449, 'end': 453}, {'entity': 'LABEL_1', 'score': 0.6983758, 'index': 110, 'word': 'one', 'start': 454, 'end': 457}, {'entity': 'LABEL_1', 'score': 0.7214084, 'index': 111, 'word': 'had', 'start': 458, 'end': 461}, {'entity': 'LABEL_1', 'score': 0.7586172, 'index': 112, 'word': 'sh', 'start': 462, 'end': 464}, {'entity': 'LABEL_1', 'score': 0.75072455, 'index': 113, 'word': '##adow', 'start': 464, 'end': 468}, {'entity': 'LABEL_1', 'score': 0.7650202, 'index': 114, 'word': '##s', 'start': 468, 'end': 469}, {'entity': 'LABEL_1', 'score': 0.739691, 'index': 115, 'word': 'that', 'start': 470, 'end': 474}, {'entity': 'LABEL_1', 'score': 0.742022, 'index': 116, 'word': 'made', 'start': 475, 'end': 479}, {'entity': 'LABEL_1', 'score': 0.7720496, 'index': 117, 'word': 'it', 'start': 480, 'end': 482}, {'entity': 'LABEL_1', 'score': 0.7664237, 'index': 118, 'word': 'look', 'start': 483, 'end': 487}, {'entity': 'LABEL_1', 'score': 0.7771936, 'index': 119, 'word': 'like', 'start': 488, 'end': 492}, {'entity': 'LABEL_1', 'score': 0.6811989, 'index': 120, 'word': 'an', 'start': 493, 'end': 495}, {'entity': 'LABEL_1', 'score': 0.6281717, 'index': 121, 'word': 'Egypt', 'start': 496, 'end': 501}, {'entity': 'LABEL_1', 'score': 0.61691815, 'index': 122, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'LABEL_1', 'score': 0.6027525, 'index': 123, 'word': 'Ph', 'start': 505, 'end': 507}, {'entity': 'LABEL_1', 'score': 0.6525575, 'index': 124, 'word': '##ara', 'start': 507, 'end': 510}, {'entity': 'LABEL_1', 'score': 0.62562203, 'index': 125, 'word': '##oh', 'start': 510, 'end': 512}, {'entity': 'LABEL_1', 'score': 0.74775475, 'index': 126, 'word': '.', 'start': 512, 'end': 513}, {'entity': 'LABEL_1', 'score': 0.72957546, 'index': 127, 'word': 'Very', 'start': 514, 'end': 518}, {'entity': 'LABEL_1', 'score': 0.78199077, 'index': 128, 'word': 'few', 'start': 519, 'end': 522}, {'entity': 'LABEL_1', 'score': 0.8058096, 'index': 129, 'word': 'days', 'start': 523, 'end': 527}, {'entity': 'LABEL_1', 'score': 0.7955371, 'index': 130, 'word': 'later', 'start': 528, 'end': 533}, {'entity': 'LABEL_1', 'score': 0.7160164, 'index': 131, 'word': ',', 'start': 533, 'end': 534}, {'entity': 'LABEL_1', 'score': 0.6962384, 'index': 132, 'word': 'we', 'start': 535, 'end': 537}, {'entity': 'LABEL_1', 'score': 0.79035527, 'index': 133, 'word': 'revealed', 'start': 538, 'end': 546}, {'entity': 'LABEL_1', 'score': 0.79964536, 'index': 134, 'word': 'the', 'start': 547, 'end': 550}, {'entity': 'LABEL_1', 'score': 0.8037711, 'index': 135, 'word': 'image', 'start': 551, 'end': 556}, {'entity': 'LABEL_1', 'score': 0.70425034, 'index': 136, 'word': 'for', 'start': 557, 'end': 560}, {'entity': 'LABEL_1', 'score': 0.656156, 'index': 137, 'word': 'all', 'start': 561, 'end': 564}, {'entity': 'LABEL_1', 'score': 0.6592189, 'index': 138, 'word': 'to', 'start': 565, 'end': 567}, {'entity': 'LABEL_1', 'score': 0.7974449, 'index': 139, 'word': 'see', 'start': 568, 'end': 571}, {'entity': 'LABEL_1', 'score': 0.74104273, 'index': 140, 'word': ',', 'start': 571, 'end': 572}, {'entity': 'LABEL_1', 'score': 0.67319137, 'index': 141, 'word': 'and', 'start': 573, 'end': 576}, {'entity': 'LABEL_1', 'score': 0.68734366, 'index': 142, 'word': 'we', 'start': 577, 'end': 579}, {'entity': 'LABEL_1', 'score': 0.72160256, 'index': 143, 'word': 'made', 'start': 580, 'end': 584}, {'entity': 'LABEL_1', 'score': 0.6952543, 'index': 144, 'word': 'sure', 'start': 585, 'end': 589}, {'entity': 'LABEL_1', 'score': 0.69323736, 'index': 145, 'word': 'to', 'start': 590, 'end': 592}, {'entity': 'LABEL_1', 'score': 0.71838343, 'index': 146, 'word': 'note', 'start': 593, 'end': 597}, {'entity': 'LABEL_1', 'score': 0.7105794, 'index': 147, 'word': 'that', 'start': 598, 'end': 602}, {'entity': 'LABEL_1', 'score': 0.7815477, 'index': 148, 'word': 'it', 'start': 603, 'end': 605}, {'entity': 'LABEL_1', 'score': 0.70479774, 'index': 149, 'word': 'was', 'start': 606, 'end': 609}, {'entity': 'LABEL_1', 'score': 0.6992195, 'index': 150, 'word': 'a', 'start': 610, 'end': 611}, {'entity': 'LABEL_1', 'score': 0.70020753, 'index': 151, 'word': 'huge', 'start': 612, 'end': 616}, {'entity': 'LABEL_1', 'score': 0.71117467, 'index': 152, 'word': 'rock', 'start': 617, 'end': 621}, {'entity': 'LABEL_1', 'score': 0.7202643, 'index': 153, 'word': 'formation', 'start': 622, 'end': 631}, {'entity': 'LABEL_1', 'score': 0.7003255, 'index': 154, 'word': 'that', 'start': 632, 'end': 636}, {'entity': 'LABEL_1', 'score': 0.7014738, 'index': 155, 'word': 'just', 'start': 637, 'end': 641}, {'entity': 'LABEL_1', 'score': 0.7438691, 'index': 156, 'word': 'res', 'start': 642, 'end': 645}, {'entity': 'LABEL_1', 'score': 0.77008, 'index': 157, 'word': '##emble', 'start': 645, 'end': 650}, {'entity': 'LABEL_1', 'score': 0.73790956, 'index': 158, 'word': '##d', 'start': 650, 'end': 651}, {'entity': 'LABEL_1', 'score': 0.7106866, 'index': 159, 'word': 'a', 'start': 652, 'end': 653}, {'entity': 'LABEL_1', 'score': 0.6789753, 'index': 160, 'word': 'human', 'start': 654, 'end': 659}, {'entity': 'LABEL_1', 'score': 0.7522357, 'index': 161, 'word': 'head', 'start': 660, 'end': 664}, {'entity': 'LABEL_1', 'score': 0.7565729, 'index': 162, 'word': 'and', 'start': 665, 'end': 668}, {'entity': 'LABEL_1', 'score': 0.7692702, 'index': 163, 'word': 'face', 'start': 669, 'end': 673}, {'entity': 'LABEL_1', 'score': 0.70920914, 'index': 164, 'word': ',', 'start': 673, 'end': 674}, {'entity': 'LABEL_1', 'score': 0.66177475, 'index': 165, 'word': 'but', 'start': 675, 'end': 678}, {'entity': 'LABEL_1', 'score': 0.70336664, 'index': 166, 'word': 'all', 'start': 679, 'end': 682}, {'entity': 'LABEL_1', 'score': 0.7623628, 'index': 167, 'word': 'of', 'start': 683, 'end': 685}, {'entity': 'LABEL_1', 'score': 0.75627846, 'index': 168, 'word': 'it', 'start': 686, 'end': 688}, {'entity': 'LABEL_1', 'score': 0.70650285, 'index': 169, 'word': 'was', 'start': 689, 'end': 692}, {'entity': 'LABEL_1', 'score': 0.728847, 'index': 170, 'word': 'formed', 'start': 693, 'end': 699}, {'entity': 'LABEL_1', 'score': 0.7043545, 'index': 171, 'word': 'by', 'start': 700, 'end': 702}, {'entity': 'LABEL_1', 'score': 0.76492995, 'index': 172, 'word': 'sh', 'start': 703, 'end': 705}, {'entity': 'LABEL_1', 'score': 0.75064677, 'index': 173, 'word': '##adow', 'start': 705, 'end': 709}, {'entity': 'LABEL_1', 'score': 0.7592806, 'index': 174, 'word': '##s', 'start': 709, 'end': 710}, {'entity': 'LABEL_1', 'score': 0.7250259, 'index': 175, 'word': '.', 'start': 710, 'end': 711}, {'entity': 'LABEL_1', 'score': 0.69586295, 'index': 176, 'word': 'We', 'start': 712, 'end': 714}, {'entity': 'LABEL_1', 'score': 0.67534536, 'index': 177, 'word': 'only', 'start': 715, 'end': 719}, {'entity': 'LABEL_1', 'score': 0.80312073, 'index': 178, 'word': 'announced', 'start': 720, 'end': 729}, {'entity': 'LABEL_1', 'score': 0.79136634, 'index': 179, 'word': 'it', 'start': 730, 'end': 732}, {'entity': 'LABEL_1', 'score': 0.6562886, 'index': 180, 'word': 'because', 'start': 733, 'end': 740}, {'entity': 'LABEL_1', 'score': 0.69648355, 'index': 181, 'word': 'we', 'start': 741, 'end': 743}, {'entity': 'LABEL_1', 'score': 0.65375775, 'index': 182, 'word': 'thought', 'start': 744, 'end': 751}, {'entity': 'LABEL_1', 'score': 0.6537003, 'index': 183, 'word': 'it', 'start': 752, 'end': 754}, {'entity': 'LABEL_1', 'score': 0.5866837, 'index': 184, 'word': 'would', 'start': 755, 'end': 760}, {'entity': 'LABEL_1', 'score': 0.58098775, 'index': 185, 'word': 'be', 'start': 761, 'end': 763}, {'entity': 'LABEL_1', 'score': 0.5318321, 'index': 186, 'word': 'a', 'start': 764, 'end': 765}, {'entity': 'LABEL_1', 'score': 0.55339235, 'index': 187, 'word': 'good', 'start': 766, 'end': 770}, {'entity': 'LABEL_1', 'score': 0.5413213, 'index': 188, 'word': 'way', 'start': 771, 'end': 774}, {'entity': 'LABEL_1', 'score': 0.6115697, 'index': 189, 'word': 'to', 'start': 775, 'end': 777}, {'entity': 'LABEL_1', 'score': 0.64011514, 'index': 190, 'word': 'engage', 'start': 778, 'end': 784}, {'entity': 'LABEL_1', 'score': 0.69212765, 'index': 191, 'word': 'the', 'start': 785, 'end': 788}, {'entity': 'LABEL_1', 'score': 0.6641921, 'index': 192, 'word': 'public', 'start': 789, 'end': 795}, {'entity': 'LABEL_1', 'score': 0.6442652, 'index': 193, 'word': 'with', 'start': 796, 'end': 800}, {'entity': 'LABEL_1', 'score': 0.7596163, 'index': 194, 'word': 'NASA', 'start': 801, 'end': 805}, {'entity': 'LABEL_1', 'score': 0.7713378, 'index': 195, 'word': \"'\", 'start': 805, 'end': 806}, {'entity': 'LABEL_1', 'score': 0.7573875, 'index': 196, 'word': 's', 'start': 806, 'end': 807}, {'entity': 'LABEL_1', 'score': 0.7930233, 'index': 197, 'word': 'findings', 'start': 808, 'end': 816}, {'entity': 'LABEL_1', 'score': 0.67292166, 'index': 198, 'word': ',', 'start': 816, 'end': 817}, {'entity': 'LABEL_1', 'score': 0.64137864, 'index': 199, 'word': 'and', 'start': 818, 'end': 821}, {'entity': 'LABEL_1', 'score': 0.6967897, 'index': 200, 'word': 'at', 'start': 822, 'end': 824}, {'entity': 'LABEL_1', 'score': 0.72243834, 'index': 201, 'word': '##rra', 'start': 824, 'end': 827}, {'entity': 'LABEL_1', 'score': 0.6921733, 'index': 202, 'word': '##ct', 'start': 827, 'end': 829}, {'entity': 'LABEL_1', 'score': 0.6736925, 'index': 203, 'word': 'attention', 'start': 830, 'end': 839}, {'entity': 'LABEL_1', 'score': 0.7459911, 'index': 204, 'word': 'to', 'start': 840, 'end': 842}, {'entity': 'LABEL_1', 'score': 0.72581196, 'index': 205, 'word': 'Mars', 'start': 843, 'end': 847}, {'entity': 'LABEL_1', 'score': 0.6681045, 'index': 206, 'word': '-', 'start': 847, 'end': 848}, {'entity': 'LABEL_1', 'score': 0.6670952, 'index': 207, 'word': '-', 'start': 848, 'end': 849}, {'entity': 'LABEL_1', 'score': 0.6767578, 'index': 208, 'word': 'and', 'start': 850, 'end': 853}, {'entity': 'LABEL_1', 'score': 0.72920245, 'index': 209, 'word': 'it', 'start': 854, 'end': 856}, {'entity': 'LABEL_1', 'score': 0.7114156, 'index': 210, 'word': 'did', 'start': 857, 'end': 860}, {'entity': 'LABEL_1', 'score': 0.7417399, 'index': 211, 'word': '.', 'start': 860, 'end': 861}, {'entity': 'LABEL_1', 'score': 0.80909854, 'index': 212, 'word': 'The', 'start': 862, 'end': 865}, {'entity': 'LABEL_1', 'score': 0.81195474, 'index': 213, 'word': 'face', 'start': 866, 'end': 870}, {'entity': 'LABEL_1', 'score': 0.82393944, 'index': 214, 'word': 'on', 'start': 871, 'end': 873}, {'entity': 'LABEL_1', 'score': 0.7426605, 'index': 215, 'word': 'Mars', 'start': 874, 'end': 878}, {'entity': 'LABEL_1', 'score': 0.67656016, 'index': 216, 'word': 'soon', 'start': 879, 'end': 883}, {'entity': 'LABEL_1', 'score': 0.64128137, 'index': 217, 'word': 'became', 'start': 884, 'end': 890}, {'entity': 'LABEL_1', 'score': 0.5706787, 'index': 218, 'word': 'a', 'start': 891, 'end': 892}, {'entity': 'LABEL_1', 'score': 0.607071, 'index': 219, 'word': 'pop', 'start': 893, 'end': 896}, {'entity': 'LABEL_1', 'score': 0.6233586, 'index': 220, 'word': 'i', 'start': 897, 'end': 898}, {'entity': 'LABEL_1', 'score': 0.6048701, 'index': 221, 'word': '##con', 'start': 898, 'end': 901}, {'entity': 'LABEL_1', 'score': 0.66074026, 'index': 222, 'word': ';', 'start': 901, 'end': 902}, {'entity': 'LABEL_1', 'score': 0.8274189, 'index': 223, 'word': 'shot', 'start': 903, 'end': 907}, {'entity': 'LABEL_1', 'score': 0.830896, 'index': 224, 'word': 'in', 'start': 908, 'end': 910}, {'entity': 'LABEL_1', 'score': 0.763844, 'index': 225, 'word': 'movies', 'start': 911, 'end': 917}, {'entity': 'LABEL_1', 'score': 0.77244467, 'index': 226, 'word': ',', 'start': 917, 'end': 918}, {'entity': 'LABEL_1', 'score': 0.8021798, 'index': 227, 'word': 'appeared', 'start': 919, 'end': 927}, {'entity': 'LABEL_1', 'score': 0.83794945, 'index': 228, 'word': 'in', 'start': 928, 'end': 930}, {'entity': 'LABEL_1', 'score': 0.7797067, 'index': 229, 'word': 'books', 'start': 931, 'end': 936}, {'entity': 'LABEL_1', 'score': 0.8155486, 'index': 230, 'word': ',', 'start': 936, 'end': 937}, {'entity': 'LABEL_1', 'score': 0.79162115, 'index': 231, 'word': 'magazines', 'start': 938, 'end': 947}, {'entity': 'LABEL_1', 'score': 0.80562055, 'index': 232, 'word': ',', 'start': 947, 'end': 948}, {'entity': 'LABEL_1', 'score': 0.79249084, 'index': 233, 'word': 'radio', 'start': 949, 'end': 954}, {'entity': 'LABEL_1', 'score': 0.7957106, 'index': 234, 'word': 'talk', 'start': 955, 'end': 959}, {'entity': 'LABEL_1', 'score': 0.783369, 'index': 235, 'word': 'shows', 'start': 960, 'end': 965}, {'entity': 'LABEL_1', 'score': 0.75296974, 'index': 236, 'word': ',', 'start': 965, 'end': 966}, {'entity': 'LABEL_1', 'score': 0.7316475, 'index': 237, 'word': 'and', 'start': 967, 'end': 970}, {'entity': 'LABEL_1', 'score': 0.7308964, 'index': 238, 'word': 'hau', 'start': 971, 'end': 974}, {'entity': 'LABEL_1', 'score': 0.7282335, 'index': 239, 'word': '##nted', 'start': 974, 'end': 978}, {'entity': 'LABEL_1', 'score': 0.70369375, 'index': 240, 'word': 'gr', 'start': 979, 'end': 981}, {'entity': 'LABEL_1', 'score': 0.7000393, 'index': 241, 'word': '##oce', 'start': 981, 'end': 984}, {'entity': 'LABEL_1', 'score': 0.7300214, 'index': 242, 'word': '##ry', 'start': 984, 'end': 986}, {'entity': 'LABEL_1', 'score': 0.743193, 'index': 243, 'word': 'store', 'start': 987, 'end': 992}, {'entity': 'LABEL_1', 'score': 0.7444752, 'index': 244, 'word': 'check', 'start': 993, 'end': 998}, {'entity': 'LABEL_1', 'score': 0.7417262, 'index': 245, 'word': '##out', 'start': 998, 'end': 1001}, {'entity': 'LABEL_1', 'score': 0.7392207, 'index': 246, 'word': 'lines', 'start': 1002, 'end': 1007}, {'entity': 'LABEL_1', 'score': 0.6868295, 'index': 247, 'word': 'for', 'start': 1008, 'end': 1011}, {'entity': 'LABEL_1', 'score': 0.7700206, 'index': 248, 'word': '25', 'start': 1012, 'end': 1014}, {'entity': 'LABEL_1', 'score': 0.7531801, 'index': 249, 'word': 'years', 'start': 1015, 'end': 1020}, {'entity': 'LABEL_1', 'score': 0.74636894, 'index': 250, 'word': '.', 'start': 1020, 'end': 1021}, {'entity': 'LABEL_1', 'score': 0.6291768, 'index': 251, 'word': 'Some', 'start': 1022, 'end': 1026}, {'entity': 'LABEL_1', 'score': 0.6802287, 'index': 252, 'word': 'people', 'start': 1027, 'end': 1033}, {'entity': 'LABEL_1', 'score': 0.6918312, 'index': 253, 'word': 'thought', 'start': 1034, 'end': 1041}, {'entity': 'LABEL_1', 'score': 0.7356861, 'index': 254, 'word': 'the', 'start': 1042, 'end': 1045}, {'entity': 'LABEL_1', 'score': 0.72291964, 'index': 255, 'word': 'natural', 'start': 1046, 'end': 1053}, {'entity': 'LABEL_1', 'score': 0.7440703, 'index': 256, 'word': 'land', 'start': 1054, 'end': 1058}, {'entity': 'LABEL_1', 'score': 0.7435822, 'index': 257, 'word': '##form', 'start': 1058, 'end': 1062}, {'entity': 'LABEL_1', 'score': 0.70303893, 'index': 258, 'word': 'was', 'start': 1063, 'end': 1066}, {'entity': 'LABEL_1', 'score': 0.71025074, 'index': 259, 'word': 'evidence', 'start': 1067, 'end': 1075}, {'entity': 'LABEL_1', 'score': 0.73211503, 'index': 260, 'word': 'of', 'start': 1076, 'end': 1078}, {'entity': 'LABEL_1', 'score': 0.7357745, 'index': 261, 'word': 'life', 'start': 1079, 'end': 1083}, {'entity': 'LABEL_1', 'score': 0.775545, 'index': 262, 'word': 'on', 'start': 1084, 'end': 1086}, {'entity': 'LABEL_1', 'score': 0.7389958, 'index': 263, 'word': 'Mars', 'start': 1087, 'end': 1091}, {'entity': 'LABEL_1', 'score': 0.6944283, 'index': 264, 'word': ',', 'start': 1091, 'end': 1092}, {'entity': 'LABEL_1', 'score': 0.6648144, 'index': 265, 'word': 'and', 'start': 1093, 'end': 1096}, {'entity': 'LABEL_1', 'score': 0.71309364, 'index': 266, 'word': 'that', 'start': 1097, 'end': 1101}, {'entity': 'LABEL_1', 'score': 0.7978385, 'index': 267, 'word': 'us', 'start': 1102, 'end': 1104}, {'entity': 'LABEL_1', 'score': 0.82260644, 'index': 268, 'word': 'scientists', 'start': 1105, 'end': 1115}, {'entity': 'LABEL_1', 'score': 0.67090344, 'index': 269, 'word': 'wanted', 'start': 1116, 'end': 1122}, {'entity': 'LABEL_1', 'score': 0.70524585, 'index': 270, 'word': 'to', 'start': 1123, 'end': 1125}, {'entity': 'LABEL_1', 'score': 0.74001384, 'index': 271, 'word': 'hide', 'start': 1126, 'end': 1130}, {'entity': 'LABEL_1', 'score': 0.6811347, 'index': 272, 'word': 'it', 'start': 1131, 'end': 1133}, {'entity': 'LABEL_1', 'score': 0.64233613, 'index': 273, 'word': ',', 'start': 1133, 'end': 1134}, {'entity': 'LABEL_1', 'score': 0.6602537, 'index': 274, 'word': 'but', 'start': 1135, 'end': 1138}, {'entity': 'LABEL_1', 'score': 0.6728735, 'index': 275, 'word': 'really', 'start': 1139, 'end': 1145}, {'entity': 'LABEL_1', 'score': 0.6881698, 'index': 276, 'word': ',', 'start': 1145, 'end': 1146}, {'entity': 'LABEL_1', 'score': 0.75322527, 'index': 277, 'word': 'the', 'start': 1147, 'end': 1150}, {'entity': 'LABEL_1', 'score': 0.76803297, 'index': 278, 'word': 'defender', 'start': 1151, 'end': 1159}, {'entity': 'LABEL_1', 'score': 0.7756065, 'index': 279, 'word': '##s', 'start': 1159, 'end': 1160}, {'entity': 'LABEL_1', 'score': 0.76783335, 'index': 280, 'word': 'of', 'start': 1161, 'end': 1163}, {'entity': 'LABEL_1', 'score': 0.7693949, 'index': 281, 'word': 'the', 'start': 1164, 'end': 1167}, {'entity': 'LABEL_1', 'score': 0.7982697, 'index': 282, 'word': 'NASA', 'start': 1168, 'end': 1172}, {'entity': 'LABEL_1', 'score': 0.81478804, 'index': 283, 'word': 'budget', 'start': 1173, 'end': 1179}, {'entity': 'LABEL_1', 'score': 0.7281754, 'index': 284, 'word': 'wish', 'start': 1180, 'end': 1184}, {'entity': 'LABEL_1', 'score': 0.6352392, 'index': 285, 'word': 'there', 'start': 1185, 'end': 1190}, {'entity': 'LABEL_1', 'score': 0.6453447, 'index': 286, 'word': 'was', 'start': 1191, 'end': 1194}, {'entity': 'LABEL_1', 'score': 0.6552948, 'index': 287, 'word': 'ancient', 'start': 1195, 'end': 1202}, {'entity': 'LABEL_1', 'score': 0.6077917, 'index': 288, 'word': 'civili', 'start': 1203, 'end': 1209}, {'entity': 'LABEL_1', 'score': 0.6093392, 'index': 289, 'word': '##zation', 'start': 1209, 'end': 1215}, {'entity': 'LABEL_1', 'score': 0.7073531, 'index': 290, 'word': 'on', 'start': 1216, 'end': 1218}, {'entity': 'LABEL_1', 'score': 0.71317667, 'index': 291, 'word': 'Mars', 'start': 1219, 'end': 1223}, {'entity': 'LABEL_1', 'score': 0.74446726, 'index': 292, 'word': '.', 'start': 1223, 'end': 1224}, {'entity': 'LABEL_1', 'score': 0.70695424, 'index': 293, 'word': 'We', 'start': 1225, 'end': 1227}, {'entity': 'LABEL_1', 'score': 0.78181815, 'index': 294, 'word': 'decided', 'start': 1228, 'end': 1235}, {'entity': 'LABEL_1', 'score': 0.79681146, 'index': 295, 'word': 'to', 'start': 1236, 'end': 1238}, {'entity': 'LABEL_1', 'score': 0.83711326, 'index': 296, 'word': 'take', 'start': 1239, 'end': 1243}, {'entity': 'LABEL_1', 'score': 0.8054449, 'index': 297, 'word': 'another', 'start': 1244, 'end': 1251}, {'entity': 'LABEL_1', 'score': 0.83750856, 'index': 298, 'word': 'shot', 'start': 1252, 'end': 1256}, {'entity': 'LABEL_1', 'score': 0.7796991, 'index': 299, 'word': 'just', 'start': 1257, 'end': 1261}, {'entity': 'LABEL_1', 'score': 0.68823475, 'index': 300, 'word': 'to', 'start': 1262, 'end': 1264}, {'entity': 'LABEL_1', 'score': 0.73818845, 'index': 301, 'word': 'make', 'start': 1265, 'end': 1269}, {'entity': 'LABEL_1', 'score': 0.7159554, 'index': 302, 'word': 'sure', 'start': 1270, 'end': 1274}, {'entity': 'LABEL_1', 'score': 0.708212, 'index': 303, 'word': 'we', 'start': 1275, 'end': 1277}, {'entity': 'LABEL_1', 'score': 0.6885756, 'index': 304, 'word': 'were', 'start': 1278, 'end': 1282}, {'entity': 'LABEL_1', 'score': 0.6930481, 'index': 305, 'word': '##n', 'start': 1282, 'end': 1283}, {'entity': 'LABEL_1', 'score': 0.6597411, 'index': 306, 'word': \"'\", 'start': 1283, 'end': 1284}, {'entity': 'LABEL_1', 'score': 0.7179377, 'index': 307, 'word': 't', 'start': 1284, 'end': 1285}, {'entity': 'LABEL_1', 'score': 0.7015423, 'index': 308, 'word': 'wrong', 'start': 1286, 'end': 1291}, {'entity': 'LABEL_1', 'score': 0.77257836, 'index': 309, 'word': ',', 'start': 1291, 'end': 1292}, {'entity': 'LABEL_1', 'score': 0.8276968, 'index': 310, 'word': 'on', 'start': 1293, 'end': 1295}, {'entity': 'LABEL_1', 'score': 0.7789852, 'index': 311, 'word': 'April', 'start': 1296, 'end': 1301}, {'entity': 'LABEL_1', 'score': 0.7939822, 'index': 312, 'word': '5', 'start': 1302, 'end': 1303}, {'entity': 'LABEL_1', 'score': 0.7708835, 'index': 313, 'word': ',', 'start': 1303, 'end': 1304}, {'entity': 'LABEL_1', 'score': 0.80051655, 'index': 314, 'word': '1998', 'start': 1305, 'end': 1309}, {'entity': 'LABEL_1', 'score': 0.77317923, 'index': 315, 'word': '.', 'start': 1309, 'end': 1310}, {'entity': 'LABEL_1', 'score': 0.7907752, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'LABEL_1', 'score': 0.7598501, 'index': 317, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'LABEL_1', 'score': 0.7807553, 'index': 318, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'LABEL_1', 'score': 0.8002719, 'index': 319, 'word': 'and', 'start': 1325, 'end': 1328}, {'entity': 'LABEL_1', 'score': 0.78151566, 'index': 320, 'word': 'his', 'start': 1329, 'end': 1332}, {'entity': 'LABEL_1', 'score': 0.70913815, 'index': 321, 'word': 'Mars', 'start': 1333, 'end': 1337}, {'entity': 'LABEL_1', 'score': 0.75237215, 'index': 322, 'word': 'Or', 'start': 1338, 'end': 1340}, {'entity': 'LABEL_1', 'score': 0.7532377, 'index': 323, 'word': '##biter', 'start': 1340, 'end': 1345}, {'entity': 'LABEL_1', 'score': 0.83377236, 'index': 324, 'word': 'camera', 'start': 1346, 'end': 1352}, {'entity': 'LABEL_1', 'score': 0.8265592, 'index': 325, 'word': 'team', 'start': 1353, 'end': 1357}, {'entity': 'LABEL_1', 'score': 0.8360809, 'index': 326, 'word': 'took', 'start': 1358, 'end': 1362}, {'entity': 'LABEL_1', 'score': 0.7798363, 'index': 327, 'word': 'a', 'start': 1363, 'end': 1364}, {'entity': 'LABEL_1', 'score': 0.7865757, 'index': 328, 'word': 'picture', 'start': 1365, 'end': 1372}, {'entity': 'LABEL_1', 'score': 0.7181756, 'index': 329, 'word': 'that', 'start': 1373, 'end': 1377}, {'entity': 'LABEL_1', 'score': 0.68633896, 'index': 330, 'word': 'was', 'start': 1378, 'end': 1381}, {'entity': 'LABEL_1', 'score': 0.733147, 'index': 331, 'word': 'ten', 'start': 1382, 'end': 1385}, {'entity': 'LABEL_1', 'score': 0.7399326, 'index': 332, 'word': 'times', 'start': 1386, 'end': 1391}, {'entity': 'LABEL_1', 'score': 0.72486407, 'index': 333, 'word': 'sharp', 'start': 1392, 'end': 1397}, {'entity': 'LABEL_1', 'score': 0.71444404, 'index': 334, 'word': '##er', 'start': 1397, 'end': 1399}, {'entity': 'LABEL_1', 'score': 0.7588349, 'index': 335, 'word': 'than', 'start': 1400, 'end': 1404}, {'entity': 'LABEL_1', 'score': 0.74737614, 'index': 336, 'word': 'the', 'start': 1405, 'end': 1408}, {'entity': 'LABEL_1', 'score': 0.7683341, 'index': 337, 'word': 'original', 'start': 1409, 'end': 1417}, {'entity': 'LABEL_1', 'score': 0.6583878, 'index': 338, 'word': 'Viking', 'start': 1418, 'end': 1424}, {'entity': 'LABEL_1', 'score': 0.76486826, 'index': 339, 'word': 'photos', 'start': 1425, 'end': 1431}, {'entity': 'LABEL_1', 'score': 0.72986007, 'index': 340, 'word': ',', 'start': 1431, 'end': 1432}, {'entity': 'LABEL_1', 'score': 0.78450584, 'index': 341, 'word': 'reveal', 'start': 1433, 'end': 1439}, {'entity': 'LABEL_1', 'score': 0.7687387, 'index': 342, 'word': '##ing', 'start': 1439, 'end': 1442}, {'entity': 'LABEL_1', 'score': 0.6958834, 'index': 343, 'word': 'a', 'start': 1443, 'end': 1444}, {'entity': 'LABEL_1', 'score': 0.68702126, 'index': 344, 'word': 'natural', 'start': 1445, 'end': 1452}, {'entity': 'LABEL_1', 'score': 0.7168807, 'index': 345, 'word': 'land', 'start': 1453, 'end': 1457}, {'entity': 'LABEL_1', 'score': 0.7284, 'index': 346, 'word': '##form', 'start': 1457, 'end': 1461}, {'entity': 'LABEL_1', 'score': 0.67773956, 'index': 347, 'word': ',', 'start': 1461, 'end': 1462}, {'entity': 'LABEL_1', 'score': 0.6827419, 'index': 348, 'word': 'which', 'start': 1463, 'end': 1468}, {'entity': 'LABEL_1', 'score': 0.68175316, 'index': 349, 'word': 'meant', 'start': 1469, 'end': 1474}, {'entity': 'LABEL_1', 'score': 0.7066642, 'index': 350, 'word': 'no', 'start': 1475, 'end': 1477}, {'entity': 'LABEL_1', 'score': 0.58663476, 'index': 351, 'word': 'alien', 'start': 1478, 'end': 1483}, {'entity': 'LABEL_1', 'score': 0.66721547, 'index': 352, 'word': 'monument', 'start': 1484, 'end': 1492}, {'entity': 'LABEL_1', 'score': 0.74949974, 'index': 353, 'word': '.', 'start': 1492, 'end': 1493}, {'entity': 'LABEL_1', 'score': 0.6512251, 'index': 354, 'word': '\"', 'start': 1494, 'end': 1495}, {'entity': 'LABEL_1', 'score': 0.66526794, 'index': 355, 'word': 'But', 'start': 1495, 'end': 1498}, {'entity': 'LABEL_1', 'score': 0.7629965, 'index': 356, 'word': 'that', 'start': 1499, 'end': 1503}, {'entity': 'LABEL_1', 'score': 0.80264395, 'index': 357, 'word': 'picture', 'start': 1504, 'end': 1511}, {'entity': 'LABEL_1', 'score': 0.69550437, 'index': 358, 'word': 'wasn', 'start': 1512, 'end': 1516}, {'entity': 'LABEL_1', 'score': 0.6697725, 'index': 359, 'word': \"'\", 'start': 1516, 'end': 1517}, {'entity': 'LABEL_1', 'score': 0.7466537, 'index': 360, 'word': 't', 'start': 1517, 'end': 1518}, {'entity': 'LABEL_1', 'score': 0.71118444, 'index': 361, 'word': 'very', 'start': 1519, 'end': 1523}, {'entity': 'LABEL_1', 'score': 0.72078806, 'index': 362, 'word': 'clear', 'start': 1524, 'end': 1529}, {'entity': 'LABEL_1', 'score': 0.69659925, 'index': 363, 'word': 'at', 'start': 1530, 'end': 1532}, {'entity': 'LABEL_1', 'score': 0.71104306, 'index': 364, 'word': 'all', 'start': 1533, 'end': 1536}, {'entity': 'LABEL_1', 'score': 0.6549209, 'index': 365, 'word': ',', 'start': 1536, 'end': 1537}, {'entity': 'LABEL_1', 'score': 0.7029999, 'index': 366, 'word': 'which', 'start': 1538, 'end': 1543}, {'entity': 'LABEL_1', 'score': 0.65546757, 'index': 367, 'word': 'could', 'start': 1544, 'end': 1549}, {'entity': 'LABEL_1', 'score': 0.6893235, 'index': 368, 'word': 'mean', 'start': 1550, 'end': 1554}, {'entity': 'LABEL_1', 'score': 0.5247262, 'index': 369, 'word': 'alien', 'start': 1555, 'end': 1560}, {'entity': 'LABEL_1', 'score': 0.59424454, 'index': 370, 'word': 'marking', 'start': 1561, 'end': 1568}, {'entity': 'LABEL_1', 'score': 0.5835969, 'index': 371, 'word': '##s', 'start': 1568, 'end': 1569}, {'entity': 'LABEL_1', 'score': 0.5980385, 'index': 372, 'word': 'were', 'start': 1570, 'end': 1574}, {'entity': 'LABEL_1', 'score': 0.5945397, 'index': 373, 'word': 'hidden', 'start': 1575, 'end': 1581}, {'entity': 'LABEL_1', 'score': 0.65447074, 'index': 374, 'word': 'by', 'start': 1582, 'end': 1584}, {'entity': 'LABEL_1', 'score': 0.7307404, 'index': 375, 'word': 'ha', 'start': 1585, 'end': 1587}, {'entity': 'LABEL_1', 'score': 0.709337, 'index': 376, 'word': '##ze', 'start': 1587, 'end': 1589}, {'entity': 'LABEL_1', 'score': 0.67702574, 'index': 377, 'word': '\"', 'start': 1589, 'end': 1590}, {'entity': 'LABEL_1', 'score': 0.73029155, 'index': 378, 'word': 'Well', 'start': 1591, 'end': 1595}, {'entity': 'LABEL_1', 'score': 0.6972946, 'index': 379, 'word': 'no', 'start': 1596, 'end': 1598}, {'entity': 'LABEL_1', 'score': 0.7082973, 'index': 380, 'word': ',', 'start': 1598, 'end': 1599}, {'entity': 'LABEL_1', 'score': 0.73839986, 'index': 381, 'word': 'ye', 'start': 1600, 'end': 1602}, {'entity': 'LABEL_1', 'score': 0.7333391, 'index': 382, 'word': '##s', 'start': 1602, 'end': 1603}, {'entity': 'LABEL_1', 'score': 0.7253444, 'index': 383, 'word': 'that', 'start': 1604, 'end': 1608}, {'entity': 'LABEL_1', 'score': 0.702405, 'index': 384, 'word': 'rum', 'start': 1609, 'end': 1612}, {'entity': 'LABEL_1', 'score': 0.7143475, 'index': 385, 'word': '##or', 'start': 1612, 'end': 1614}, {'entity': 'LABEL_1', 'score': 0.7574995, 'index': 386, 'word': 'started', 'start': 1615, 'end': 1622}, {'entity': 'LABEL_1', 'score': 0.6915992, 'index': 387, 'word': ',', 'start': 1622, 'end': 1623}, {'entity': 'LABEL_1', 'score': 0.7290222, 'index': 388, 'word': 'but', 'start': 1624, 'end': 1627}, {'entity': 'LABEL_1', 'score': 0.7465644, 'index': 389, 'word': 'to', 'start': 1628, 'end': 1630}, {'entity': 'LABEL_1', 'score': 0.74033535, 'index': 390, 'word': 'prove', 'start': 1631, 'end': 1636}, {'entity': 'LABEL_1', 'score': 0.6651234, 'index': 391, 'word': 'them', 'start': 1637, 'end': 1641}, {'entity': 'LABEL_1', 'score': 0.70885557, 'index': 392, 'word': 'wrong', 'start': 1642, 'end': 1647}, {'entity': 'LABEL_1', 'score': 0.8038346, 'index': 393, 'word': 'on', 'start': 1648, 'end': 1650}, {'entity': 'LABEL_1', 'score': 0.7705838, 'index': 394, 'word': 'April', 'start': 1651, 'end': 1656}, {'entity': 'LABEL_1', 'score': 0.8015028, 'index': 395, 'word': '8', 'start': 1657, 'end': 1658}, {'entity': 'LABEL_1', 'score': 0.7531794, 'index': 396, 'word': ',', 'start': 1658, 'end': 1659}, {'entity': 'LABEL_1', 'score': 0.7946574, 'index': 397, 'word': '2001', 'start': 1660, 'end': 1664}, {'entity': 'LABEL_1', 'score': 0.7253527, 'index': 398, 'word': 'we', 'start': 1665, 'end': 1667}, {'entity': 'LABEL_1', 'score': 0.7871118, 'index': 399, 'word': 'decided', 'start': 1668, 'end': 1675}, {'entity': 'LABEL_1', 'score': 0.7854386, 'index': 400, 'word': 'to', 'start': 1676, 'end': 1678}, {'entity': 'LABEL_1', 'score': 0.82490003, 'index': 401, 'word': 'take', 'start': 1679, 'end': 1683}, {'entity': 'LABEL_1', 'score': 0.7989956, 'index': 402, 'word': 'another', 'start': 1684, 'end': 1691}, {'entity': 'LABEL_1', 'score': 0.82106984, 'index': 403, 'word': 'picture', 'start': 1692, 'end': 1699}, {'entity': 'LABEL_1', 'score': 0.7991998, 'index': 404, 'word': ',', 'start': 1699, 'end': 1700}, {'entity': 'LABEL_1', 'score': 0.77249146, 'index': 405, 'word': 'making', 'start': 1701, 'end': 1707}, {'entity': 'LABEL_1', 'score': 0.7597969, 'index': 406, 'word': 'sure', 'start': 1708, 'end': 1712}, {'entity': 'LABEL_1', 'score': 0.8081106, 'index': 407, 'word': 'it', 'start': 1713, 'end': 1715}, {'entity': 'LABEL_1', 'score': 0.8034739, 'index': 408, 'word': 'was', 'start': 1716, 'end': 1719}, {'entity': 'LABEL_1', 'score': 0.81105435, 'index': 409, 'word': 'a', 'start': 1720, 'end': 1721}, {'entity': 'LABEL_1', 'score': 0.8072989, 'index': 410, 'word': 'cloud', 'start': 1722, 'end': 1727}, {'entity': 'LABEL_1', 'score': 0.8248416, 'index': 411, 'word': '##less', 'start': 1727, 'end': 1731}, {'entity': 'LABEL_1', 'score': 0.80674416, 'index': 412, 'word': 'summer', 'start': 1732, 'end': 1738}, {'entity': 'LABEL_1', 'score': 0.82033736, 'index': 413, 'word': 'day', 'start': 1739, 'end': 1742}, {'entity': 'LABEL_1', 'score': 0.7262438, 'index': 414, 'word': '.', 'start': 1742, 'end': 1743}, {'entity': 'LABEL_1', 'score': 0.76844937, 'index': 415, 'word': 'Mali', 'start': 1744, 'end': 1748}, {'entity': 'LABEL_1', 'score': 0.7816564, 'index': 416, 'word': '##n', 'start': 1748, 'end': 1749}, {'entity': 'LABEL_1', 'score': 0.77130514, 'index': 417, 'word': \"'\", 'start': 1749, 'end': 1750}, {'entity': 'LABEL_1', 'score': 0.7552653, 'index': 418, 'word': 's', 'start': 1750, 'end': 1751}, {'entity': 'LABEL_1', 'score': 0.769777, 'index': 419, 'word': 'team', 'start': 1752, 'end': 1756}, {'entity': 'LABEL_1', 'score': 0.825163, 'index': 420, 'word': 'captured', 'start': 1757, 'end': 1765}, {'entity': 'LABEL_1', 'score': 0.7647301, 'index': 421, 'word': 'an', 'start': 1766, 'end': 1768}, {'entity': 'LABEL_1', 'score': 0.65662813, 'index': 422, 'word': 'ama', 'start': 1769, 'end': 1772}, {'entity': 'LABEL_1', 'score': 0.6743174, 'index': 423, 'word': '##zing', 'start': 1772, 'end': 1776}, {'entity': 'LABEL_1', 'score': 0.7817416, 'index': 424, 'word': 'photo', 'start': 1777, 'end': 1782}, {'entity': 'LABEL_1', 'score': 0.7573933, 'index': 425, 'word': 'using', 'start': 1783, 'end': 1788}, {'entity': 'LABEL_1', 'score': 0.7618541, 'index': 426, 'word': 'the', 'start': 1789, 'end': 1792}, {'entity': 'LABEL_1', 'score': 0.7836661, 'index': 427, 'word': 'camera', 'start': 1793, 'end': 1799}, {'entity': 'LABEL_1', 'score': 0.7738333, 'index': 428, 'word': \"'\", 'start': 1799, 'end': 1800}, {'entity': 'LABEL_1', 'score': 0.7555313, 'index': 429, 'word': 's', 'start': 1800, 'end': 1801}, {'entity': 'LABEL_1', 'score': 0.74457055, 'index': 430, 'word': 'absolute', 'start': 1802, 'end': 1810}, {'entity': 'LABEL_1', 'score': 0.753864, 'index': 431, 'word': 'maximum', 'start': 1811, 'end': 1818}, {'entity': 'LABEL_1', 'score': 0.77354956, 'index': 432, 'word': 'revolution', 'start': 1819, 'end': 1829}, {'entity': 'LABEL_1', 'score': 0.75317514, 'index': 433, 'word': '.', 'start': 1829, 'end': 1830}, {'entity': 'LABEL_1', 'score': 0.7622576, 'index': 434, 'word': 'With', 'start': 1831, 'end': 1835}, {'entity': 'LABEL_1', 'score': 0.74248993, 'index': 435, 'word': 'this', 'start': 1836, 'end': 1840}, {'entity': 'LABEL_1', 'score': 0.79989207, 'index': 436, 'word': 'camera', 'start': 1841, 'end': 1847}, {'entity': 'LABEL_1', 'score': 0.703232, 'index': 437, 'word': 'you', 'start': 1848, 'end': 1851}, {'entity': 'LABEL_1', 'score': 0.70588446, 'index': 438, 'word': 'can', 'start': 1852, 'end': 1855}, {'entity': 'LABEL_1', 'score': 0.77229, 'index': 439, 'word': 'disc', 'start': 1856, 'end': 1860}, {'entity': 'LABEL_1', 'score': 0.7842013, 'index': 440, 'word': '##ern', 'start': 1860, 'end': 1863}, {'entity': 'LABEL_1', 'score': 0.7368206, 'index': 441, 'word': 'things', 'start': 1864, 'end': 1870}, {'entity': 'LABEL_1', 'score': 0.7523249, 'index': 442, 'word': 'in', 'start': 1871, 'end': 1873}, {'entity': 'LABEL_1', 'score': 0.75886136, 'index': 443, 'word': 'a', 'start': 1874, 'end': 1875}, {'entity': 'LABEL_1', 'score': 0.79303557, 'index': 444, 'word': 'digital', 'start': 1876, 'end': 1883}, {'entity': 'LABEL_1', 'score': 0.81027055, 'index': 445, 'word': 'image', 'start': 1884, 'end': 1889}, {'entity': 'LABEL_1', 'score': 0.7587734, 'index': 446, 'word': ',', 'start': 1889, 'end': 1890}, {'entity': 'LABEL_1', 'score': 0.8017615, 'index': 447, 'word': '3', 'start': 1891, 'end': 1892}, {'entity': 'LABEL_1', 'score': 0.79854894, 'index': 448, 'word': 'times', 'start': 1893, 'end': 1898}, {'entity': 'LABEL_1', 'score': 0.7753849, 'index': 449, 'word': 'bigger', 'start': 1899, 'end': 1905}, {'entity': 'LABEL_1', 'score': 0.7946766, 'index': 450, 'word': 'than', 'start': 1906, 'end': 1910}, {'entity': 'LABEL_1', 'score': 0.7816375, 'index': 451, 'word': 'the', 'start': 1911, 'end': 1914}, {'entity': 'LABEL_1', 'score': 0.81315005, 'index': 452, 'word': 'pi', 'start': 1915, 'end': 1917}, {'entity': 'LABEL_1', 'score': 0.79145354, 'index': 453, 'word': '##xel', 'start': 1917, 'end': 1920}, {'entity': 'LABEL_1', 'score': 0.78540695, 'index': 454, 'word': 'size', 'start': 1921, 'end': 1925}, {'entity': 'LABEL_1', 'score': 0.73578054, 'index': 455, 'word': 'which', 'start': 1926, 'end': 1931}, {'entity': 'LABEL_1', 'score': 0.75317943, 'index': 456, 'word': 'means', 'start': 1932, 'end': 1937}, {'entity': 'LABEL_1', 'score': 0.68044114, 'index': 457, 'word': 'if', 'start': 1938, 'end': 1940}, {'entity': 'LABEL_1', 'score': 0.65632993, 'index': 458, 'word': 'there', 'start': 1941, 'end': 1946}, {'entity': 'LABEL_1', 'score': 0.6696365, 'index': 459, 'word': 'were', 'start': 1947, 'end': 1951}, {'entity': 'LABEL_1', 'score': 0.6908212, 'index': 460, 'word': 'any', 'start': 1952, 'end': 1955}, {'entity': 'LABEL_1', 'score': 0.6708151, 'index': 461, 'word': 'signs', 'start': 1956, 'end': 1961}, {'entity': 'LABEL_1', 'score': 0.70746076, 'index': 462, 'word': 'of', 'start': 1962, 'end': 1964}, {'entity': 'LABEL_1', 'score': 0.7460417, 'index': 463, 'word': 'life', 'start': 1965, 'end': 1969}, {'entity': 'LABEL_1', 'score': 0.6650858, 'index': 464, 'word': ',', 'start': 1969, 'end': 1970}, {'entity': 'LABEL_1', 'score': 0.65826124, 'index': 465, 'word': 'you', 'start': 1971, 'end': 1974}, {'entity': 'LABEL_1', 'score': 0.60152525, 'index': 466, 'word': 'could', 'start': 1975, 'end': 1980}, {'entity': 'LABEL_1', 'score': 0.6337636, 'index': 467, 'word': 'easily', 'start': 1981, 'end': 1987}, {'entity': 'LABEL_1', 'score': 0.7974535, 'index': 468, 'word': 'see', 'start': 1988, 'end': 1991}, {'entity': 'LABEL_1', 'score': 0.67936397, 'index': 469, 'word': 'what', 'start': 1992, 'end': 1996}, {'entity': 'LABEL_1', 'score': 0.6276276, 'index': 470, 'word': 'they', 'start': 1997, 'end': 2001}, {'entity': 'LABEL_1', 'score': 0.66260046, 'index': 471, 'word': 'were', 'start': 2002, 'end': 2006}, {'entity': 'LABEL_1', 'score': 0.65728337, 'index': 472, 'word': '.', 'start': 2006, 'end': 2007}, {'entity': 'LABEL_1', 'score': 0.7592064, 'index': 473, 'word': 'What', 'start': 2008, 'end': 2012}, {'entity': 'LABEL_1', 'score': 0.7772832, 'index': 474, 'word': 'the', 'start': 2013, 'end': 2016}, {'entity': 'LABEL_1', 'score': 0.8069773, 'index': 475, 'word': 'picture', 'start': 2017, 'end': 2024}, {'entity': 'LABEL_1', 'score': 0.8326009, 'index': 476, 'word': 'showed', 'start': 2025, 'end': 2031}, {'entity': 'LABEL_1', 'score': 0.68662155, 'index': 477, 'word': 'was', 'start': 2032, 'end': 2035}, {'entity': 'LABEL_1', 'score': 0.6621673, 'index': 478, 'word': 'the', 'start': 2036, 'end': 2039}, {'entity': 'LABEL_1', 'score': 0.6492267, 'index': 479, 'word': 'but', 'start': 2040, 'end': 2043}, {'entity': 'LABEL_1', 'score': 0.6707638, 'index': 480, 'word': '##te', 'start': 2043, 'end': 2045}, {'entity': 'LABEL_1', 'score': 0.69464356, 'index': 481, 'word': 'or', 'start': 2046, 'end': 2048}, {'entity': 'LABEL_1', 'score': 0.6542466, 'index': 482, 'word': 'mesa', 'start': 2049, 'end': 2053}, {'entity': 'LABEL_1', 'score': 0.6557618, 'index': 483, 'word': ',', 'start': 2053, 'end': 2054}, {'entity': 'LABEL_1', 'score': 0.6405406, 'index': 484, 'word': 'which', 'start': 2055, 'end': 2060}, {'entity': 'LABEL_1', 'score': 0.62847257, 'index': 485, 'word': 'are', 'start': 2061, 'end': 2064}, {'entity': 'LABEL_1', 'score': 0.6821395, 'index': 486, 'word': 'land', 'start': 2065, 'end': 2069}, {'entity': 'LABEL_1', 'score': 0.7125307, 'index': 487, 'word': '##form', 'start': 2069, 'end': 2073}, {'entity': 'LABEL_1', 'score': 0.6966245, 'index': 488, 'word': '##s', 'start': 2073, 'end': 2074}, {'entity': 'LABEL_1', 'score': 0.6840304, 'index': 489, 'word': 'common', 'start': 2075, 'end': 2081}, {'entity': 'LABEL_1', 'score': 0.70108384, 'index': 490, 'word': 'around', 'start': 2082, 'end': 2088}, {'entity': 'LABEL_1', 'score': 0.6881684, 'index': 491, 'word': 'the', 'start': 2089, 'end': 2092}, {'entity': 'LABEL_1', 'score': 0.6812218, 'index': 492, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'LABEL_1', 'score': 0.6562686, 'index': 493, 'word': 'West', 'start': 2102, 'end': 2106}, {'entity': 'LABEL_1', 'score': 0.66924584, 'index': 494, 'word': '.', 'start': 2106, 'end': 2107}]\n",
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"numind/NuNER-multilingual-v0.1\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"numind/NuNER-multilingual-v0.1\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "2888a75f-856e-486c-b945-637d2d5b00d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e4378-8c3b-4848-b7d5-8e370ae393ee",
   "metadata": {},
   "source": [
    "## 30 orgcatorg/bert-base-multilingual-cased-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "c780e0af-aa32-44a2-b519-dea67c5b5788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
      "[{'entity': 'B-ORG', 'score': 0.99468493, 'index': 60, 'word': 'Viking', 'start': 240, 'end': 246}, {'entity': 'I-ORG', 'score': 0.99335337, 'index': 61, 'word': '1', 'start': 247, 'end': 248}, {'entity': 'B-LOC', 'score': 0.9839153, 'index': 104, 'word': 'C', 'start': 435, 'end': 436}, {'entity': 'B-LOC', 'score': 0.9820802, 'index': 105, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'B-LOC', 'score': 0.9102487, 'index': 106, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'B-PER', 'score': 0.8671783, 'index': 121, 'word': 'Egypt', 'start': 496, 'end': 501}, {'entity': 'B-PER', 'score': 0.7123952, 'index': 122, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'I-PER', 'score': 0.94853634, 'index': 123, 'word': 'Ph', 'start': 505, 'end': 507}, {'entity': 'I-PER', 'score': 0.96879715, 'index': 124, 'word': '##ara', 'start': 507, 'end': 510}, {'entity': 'I-PER', 'score': 0.9677627, 'index': 125, 'word': '##oh', 'start': 510, 'end': 512}, {'entity': 'B-PER', 'score': 0.9969103, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.99837106, 'index': 317, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'I-PER', 'score': 0.9981609, 'index': 318, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'B-ORG', 'score': 0.9981694, 'index': 321, 'word': 'Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-ORG', 'score': 0.99742895, 'index': 322, 'word': 'Or', 'start': 1338, 'end': 1340}, {'entity': 'I-ORG', 'score': 0.9966016, 'index': 323, 'word': '##biter', 'start': 1340, 'end': 1345}, {'entity': 'B-LOC', 'score': 0.9417412, 'index': 492, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.96071965, 'index': 493, 'word': 'West', 'start': 2102, 'end': 2106}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"orgcatorg/bert-base-multilingual-cased-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"orgcatorg/bert-base-multilingual-cased-ner\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "salida = nlp(text)\n",
    "salida\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)\n",
    "\n",
    "print(salida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "bb202997-d5f2-4ef4-9eac-0045a1277e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8952458e-9e41-4cf0-bec0-15b66116fbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=['O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-LOC',\n",
    " 'B-LOC',\n",
    " 'B-LOC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    "'O',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "637ca46c-c43e-4605-b56c-d2b57bd1f62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 4}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'PER': {'precision': 0.4, 'recall': 1.0, 'f1': 0.5714285714285715, 'number': 2}, 'overall_precision': 0.3333333333333333, 'overall_recall': 0.8571428571428571, 'overall_f1': 0.48, 'overall_accuracy': 0.9615384615384616}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01668401-aae4-4edb-beca-5158043923da",
   "metadata": {},
   "source": [
    "## 31 orgcatorg/xlm-roberta-base-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "7218c33d-66a7-4aee-a1d6-aab2d0f53b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-ORG', 'score': 0.9307573, 'index': 8, 'word': '▁NASA', 'start': 16, 'end': 20}, {'entity': 'I-ORG', 'score': 0.55268466, 'index': 9, 'word': '▁scientist', 'start': 21, 'end': 30}, {'entity': 'B-ORG', 'score': 0.8743138, 'index': 58, 'word': '▁Viking', 'start': 240, 'end': 246}, {'entity': 'I-ORG', 'score': 0.8924012, 'index': 59, 'word': '▁1', 'start': 247, 'end': 248}, {'entity': 'I-ORG', 'score': 0.8749563, 'index': 60, 'word': '▁space', 'start': 249, 'end': 254}, {'entity': 'I-ORG', 'score': 0.83837587, 'index': 61, 'word': 'craft', 'start': 254, 'end': 259}, {'entity': 'B-ORG', 'score': 0.31897616, 'index': 97, 'word': '▁Marti', 'start': 407, 'end': 412}, {'entity': 'B-LOC', 'score': 0.66254807, 'index': 103, 'word': '▁Cy', 'start': 435, 'end': 437}, {'entity': 'B-LOC', 'score': 0.6462039, 'index': 104, 'word': 'do', 'start': 437, 'end': 439}, {'entity': 'B-LOC', 'score': 0.6156382, 'index': 105, 'word': 'nia', 'start': 439, 'end': 442}, {'entity': 'B-ORG', 'score': 0.5166316, 'index': 119, 'word': '▁Egypt', 'start': 496, 'end': 501}, {'entity': 'B-ORG', 'score': 0.5068441, 'index': 120, 'word': 'ion', 'start': 501, 'end': 504}, {'entity': 'I-ORG', 'score': 0.51523536, 'index': 121, 'word': '▁Phar', 'start': 505, 'end': 509}, {'entity': 'I-ORG', 'score': 0.5124219, 'index': 122, 'word': 'a', 'start': 509, 'end': 510}, {'entity': 'I-ORG', 'score': 0.54218507, 'index': 123, 'word': 'oh', 'start': 510, 'end': 512}, {'entity': 'B-PER', 'score': 0.9100919, 'index': 319, 'word': '▁Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.89008635, 'index': 320, 'word': '▁Malin', 'start': 1319, 'end': 1324}, {'entity': 'B-ORG', 'score': 0.8640533, 'index': 323, 'word': '▁Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-ORG', 'score': 0.913112, 'index': 324, 'word': '▁Or', 'start': 1338, 'end': 1340}, {'entity': 'I-ORG', 'score': 0.88857234, 'index': 325, 'word': 'bit', 'start': 1340, 'end': 1343}, {'entity': 'I-ORG', 'score': 0.85235375, 'index': 326, 'word': 'er', 'start': 1343, 'end': 1345}, {'entity': 'B-LOC', 'score': 0.50641507, 'index': 478, 'word': '▁but', 'start': 2040, 'end': 2043}, {'entity': 'B-LOC', 'score': 0.8740346, 'index': 491, 'word': '▁American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.78719395, 'index': 492, 'word': '▁West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"orgcatorg/xlm-roberta-base-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"orgcatorg/xlm-roberta-base-ner\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "print(ner_results)\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c54964d5-041e-4fc7-989f-07f45279adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[ 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b685eb28-1d00-440a-bd9d-fa124b85185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 0.25, 'recall': 1.0, 'f1': 0.4, 'number': 1}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'PER': {'precision': 0.4, 'recall': 1.0, 'f1': 0.5714285714285715, 'number': 2}, 'overall_precision': 0.16666666666666666, 'overall_recall': 0.5, 'overall_f1': 0.25, 'overall_accuracy': 0.951417004048583}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c388f1-41aa-451a-928e-41acfe12639d",
   "metadata": {},
   "source": [
    "## 32 orgcatorg/EntityCS-39-PEP_MS_MLM-xlmr-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "34af1035-7a09-485b-9663-057506e508ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-ORG', 'score': 0.9218857, 'index': 8, 'word': '▁NASA', 'start': 16, 'end': 20}, {'entity': 'I-ORG', 'score': 0.9510029, 'index': 9, 'word': '▁scientist', 'start': 21, 'end': 30}, {'entity': 'I-ORG', 'score': 0.8217206, 'index': 25, 'word': '▁Mars', 'start': 96, 'end': 100}, {'entity': 'B-ORG', 'score': 0.96086377, 'index': 58, 'word': '▁Viking', 'start': 240, 'end': 246}, {'entity': 'I-ORG', 'score': 0.9884774, 'index': 59, 'word': '▁1', 'start': 247, 'end': 248}, {'entity': 'I-ORG', 'score': 0.9854343, 'index': 60, 'word': '▁space', 'start': 249, 'end': 254}, {'entity': 'I-ORG', 'score': 0.97897524, 'index': 61, 'word': 'craft', 'start': 254, 'end': 259}, {'entity': 'B-ORG', 'score': 0.8928362, 'index': 97, 'word': '▁Marti', 'start': 407, 'end': 412}, {'entity': 'B-ORG', 'score': 0.83326113, 'index': 98, 'word': 'an', 'start': 412, 'end': 414}, {'entity': 'I-ORG', 'score': 0.8212351, 'index': 99, 'word': '▁mesa', 'start': 415, 'end': 419}, {'entity': 'B-ORG', 'score': 0.86985016, 'index': 119, 'word': '▁Egypt', 'start': 496, 'end': 501}, {'entity': 'B-ORG', 'score': 0.87044686, 'index': 120, 'word': 'ion', 'start': 501, 'end': 504}, {'entity': 'I-ORG', 'score': 0.70206004, 'index': 121, 'word': '▁Phar', 'start': 505, 'end': 509}, {'entity': 'I-ORG', 'score': 0.8018863, 'index': 122, 'word': 'a', 'start': 509, 'end': 510}, {'entity': 'I-ORG', 'score': 0.8662057, 'index': 123, 'word': 'oh', 'start': 510, 'end': 512}, {'entity': 'B-PER', 'score': 0.8813573, 'index': 319, 'word': '▁Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.90332484, 'index': 320, 'word': '▁Malin', 'start': 1319, 'end': 1324}, {'entity': 'I-ORG', 'score': 0.8650216, 'index': 324, 'word': '▁Or', 'start': 1338, 'end': 1340}, {'entity': 'I-ORG', 'score': 0.79992837, 'index': 325, 'word': 'bit', 'start': 1340, 'end': 1343}, {'entity': 'I-ORG', 'score': 0.8160805, 'index': 326, 'word': 'er', 'start': 1343, 'end': 1345}]\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"orgcatorg/EntityCS-39-PEP_MS_MLM-xlmr-base\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"orgcatorg/EntityCS-39-PEP_MS_MLM-xlmr-base\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "print(ner_results)\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "8f0dab43-9fd8-4a0f-a3b4-c936f4ab9d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)\n",
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[ 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "8bb5d3fd-065b-4453-b4c3-a7aac9bd4869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 0.25, 'recall': 1.0, 'f1': 0.4, 'number': 1}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'PER': {'precision': 0.4, 'recall': 1.0, 'f1': 0.5714285714285715, 'number': 2}, 'overall_precision': 0.16666666666666666, 'overall_recall': 0.5, 'overall_f1': 0.25, 'overall_accuracy': 0.951417004048583}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b343a33-2ae7-441a-8ff2-7b10dcd5611d",
   "metadata": {},
   "source": [
    "## 33 igorsterner/xlmr-multilingual-sentence-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "3ae7c05d-6f10-4666-92dc-6a3483c40afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': '|',\n",
       "  'score': 0.9995815,\n",
       "  'index': 49,\n",
       "  'word': '?\"',\n",
       "  'start': 206,\n",
       "  'end': 208},\n",
       " {'entity': '|',\n",
       "  'score': 0.9974347,\n",
       "  'index': 85,\n",
       "  'word': '.',\n",
       "  'start': 354,\n",
       "  'end': 355},\n",
       " {'entity': '|',\n",
       "  'score': 0.9996898,\n",
       "  'index': 124,\n",
       "  'word': '.',\n",
       "  'start': 512,\n",
       "  'end': 513},\n",
       " {'entity': '|',\n",
       "  'score': 0.9997476,\n",
       "  'index': 174,\n",
       "  'word': '.',\n",
       "  'start': 710,\n",
       "  'end': 711},\n",
       " {'entity': '|',\n",
       "  'score': 0.9997371,\n",
       "  'index': 211,\n",
       "  'word': '.',\n",
       "  'start': 860,\n",
       "  'end': 861},\n",
       " {'entity': '|',\n",
       "  'score': 0.9997018,\n",
       "  'index': 251,\n",
       "  'word': '.',\n",
       "  'start': 1020,\n",
       "  'end': 1021},\n",
       " {'entity': '|',\n",
       "  'score': 0.9995659,\n",
       "  'index': 296,\n",
       "  'word': '.',\n",
       "  'start': 1223,\n",
       "  'end': 1224},\n",
       " {'entity': '|',\n",
       "  'score': 0.9988242,\n",
       "  'index': 356,\n",
       "  'word': '.',\n",
       "  'start': 1492,\n",
       "  'end': 1493},\n",
       " {'entity': '|',\n",
       "  'score': 0.9983346,\n",
       "  'index': 380,\n",
       "  'word': '\"',\n",
       "  'start': 1589,\n",
       "  'end': 1590},\n",
       " {'entity': '|',\n",
       "  'score': 0.999102,\n",
       "  'index': 415,\n",
       "  'word': '.',\n",
       "  'start': 1742,\n",
       "  'end': 1743},\n",
       " {'entity': '|',\n",
       "  'score': 0.99834883,\n",
       "  'index': 433,\n",
       "  'word': '.',\n",
       "  'start': 1829,\n",
       "  'end': 1830},\n",
       " {'entity': '|',\n",
       "  'score': 0.99967754,\n",
       "  'index': 471,\n",
       "  'word': '.',\n",
       "  'start': 2006,\n",
       "  'end': 2007},\n",
       " {'entity': '|',\n",
       "  'score': 0.99990845,\n",
       "  'index': 493,\n",
       "  'word': '.',\n",
       "  'start': 2106,\n",
       "  'end': 2107}]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"igorsterner/xlmr-multilingual-sentence-segmentation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"igorsterner/xlmr-multilingual-sentence-segmentation\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32e7cb9f-a799-4227-a642-ef9af8602b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity\n",
      "|    13\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "entity  word\n",
       "|                1\n",
       "        .       11\n",
       "        ?        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbccdc4-eafd-485d-bdd0-500d606e6429",
   "metadata": {},
   "source": [
    "## 34 mukowaty/punctuate-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "997199dd-2e7f-43ee-8dc4-b5189ef1da47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: '.', 2: ',', 3: '?', 4: '-', 5: ':'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mukowaty/punctuate-16\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"mukowaty/punctuate-16\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67c25d-82fc-494d-95c6-3812228d0d7d",
   "metadata": {},
   "source": [
    "## 35 HiTZ/mbert-argmining-abstrct-multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "6e8a7262-9e25-4363-bae7-49d477b96175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HiTZ/mbert-argmining-abstrct-multilingual\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"HiTZ/mbert-argmining-abstrct-multilingual\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12a382-1d1b-440c-b968-d9b307d1b391",
   "metadata": {},
   "source": [
    "## 36 benjamin/wtp-canine-s-12l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "462a5e9d-56f2-449e-b80c-28e56d8f4000",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'la-canine'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[376], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-12l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-12l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-canine-s-12l\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-canine-s-12l\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0794928-e326-4f21-9a11-4d9cfa04a373",
   "metadata": {},
   "source": [
    "## 37 benjamin/wtp-canine-s-3l-no-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "e57f841c-e383-430c-8afd-a954e4bd2483",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'la-canine'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[377], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-3l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-3l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-canine-s-3l-no-adapters\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-canine-s-3l-no-adapters\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8fe70-ec5a-43dc-b4a2-aead0b235f75",
   "metadata": {},
   "source": [
    "## 38 benjamin/wtp-canine-s-9l-no-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "c63de375-015d-4f70-a9f4-bdfa18a6a3cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:989\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 989\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:691\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    692\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'la-canine'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[378], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-9l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenjamin/wtp-canine-s-9l-no-adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:991\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `la-canine` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"benjamin/wtp-canine-s-9l-no-adapters\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"benjamin/wtp-canine-s-9l-no-adapters\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b75f15-739f-45a4-8211-b094fd2b9104",
   "metadata": {},
   "source": [
    "## 39 msislam/code-mixed-language-detection-XLMRoberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "e72aafcc-9c45-47c4-bd61-bf0941e523c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'I-DE', 1: 'I-EN', 2: 'I-ES', 3: 'I-FR'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"msislam/code-mixed-language-detection-XLMRoberta\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"msislam/code-mixed-language-detection-XLMRoberta\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c588a1c-faa2-4955-bc83-9bdf2bdb3692",
   "metadata": {},
   "source": [
    "## 40 DunnBC22/bert-base-multilingual-cased-fine_tuned-ner-WikiNeural_Multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "21949283-c47a-4c93-8965-98ce4b11d09a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'B-LOC'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[385], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDunnBC22/bert-base-cased-finetuned-WikiNeural\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDunnBC22/bert-base-cased-finetuned-WikiNeural\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m      7\u001b[0m salida \u001b[38;5;241m=\u001b[39m nlp(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:524\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    522\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 524\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    525\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    526\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    527\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    528\u001b[0m     code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[0;32m    529\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    532\u001b[0m )\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:996\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:772\u001b[0m, in \u001b[0;36mPretrainedConfig.from_dict\u001b[1;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[0;32m    770\u001b[0m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 772\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruned_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    775\u001b[0m     config\u001b[38;5;241m.\u001b[39mpruned_heads \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpruned_heads\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\configuration_bert.py:119\u001b[0m, in \u001b[0;36mBertConfig.__init__\u001b[1;34m(self, vocab_size, hidden_size, num_hidden_layers, num_attention_heads, intermediate_size, hidden_act, hidden_dropout_prob, attention_probs_dropout_prob, max_position_embeddings, type_vocab_size, initializer_range, layer_norm_eps, pad_token_id, position_embedding_type, use_cache, classifier_dropout, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    101\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30522\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    118\u001b[0m ):\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m vocab_size\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m hidden_size\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:309\u001b[0m, in \u001b[0;36mPretrainedConfig.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2label) \u001b[38;5;241m!=\u001b[39m num_labels:\n\u001b[0;32m    305\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    306\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed along `num_labels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` with an incompatible id to label map: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The number of labels wil be overwritten to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m         )\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2label \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2label\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;66;03m# Keys are always strings in JSON so convert ids to int here.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:309\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2label) \u001b[38;5;241m!=\u001b[39m num_labels:\n\u001b[0;32m    305\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    306\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed along `num_labels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` with an incompatible id to label map: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The number of labels wil be overwritten to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m         )\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2label \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2label\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;66;03m# Keys are always strings in JSON so convert ids to int here.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'B-LOC'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DunnBC22/bert-base-cased-finetuned-WikiNeural\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"DunnBC22/bert-base-cased-finetuned-WikiNeural\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "salida = nlp(text)\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d4edf-44f2-4d33-aaef-74e2351995ae",
   "metadata": {},
   "source": [
    "## 41 rollerhafeezh-amikom/xlm-roberta-base-ner-silvanus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "31b2cf11-39a1-43cf-9679-76eaef9b5428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-LOC', 'score': 0.9711031, 'index': 103, 'word': '▁Cy', 'start': 435, 'end': 437}, {'entity': 'I-LOC', 'score': 0.97973377, 'index': 104, 'word': 'do', 'start': 437, 'end': 439}, {'entity': 'I-LOC', 'score': 0.98118365, 'index': 105, 'word': 'nia', 'start': 439, 'end': 442}, {'entity': 'B-DAT', 'score': 0.99709177, 'index': 315, 'word': '▁April', 'start': 1296, 'end': 1301}, {'entity': 'I-DAT', 'score': 0.9621371, 'index': 316, 'word': '▁5', 'start': 1302, 'end': 1303}, {'entity': 'I-DAT', 'score': 0.9620826, 'index': 317, 'word': ',', 'start': 1303, 'end': 1304}, {'entity': 'I-DAT', 'score': 0.98092973, 'index': 318, 'word': '▁1998.', 'start': 1305, 'end': 1310}, {'entity': 'B-DAT', 'score': 0.9965664, 'index': 395, 'word': '▁April', 'start': 1651, 'end': 1656}, {'entity': 'I-DAT', 'score': 0.9623601, 'index': 396, 'word': '▁8', 'start': 1657, 'end': 1658}, {'entity': 'I-DAT', 'score': 0.9603569, 'index': 397, 'word': ',', 'start': 1658, 'end': 1659}, {'entity': 'I-DAT', 'score': 0.9859262, 'index': 398, 'word': '▁2001', 'start': 1660, 'end': 1664}, {'entity': 'B-LOC', 'score': 0.9732382, 'index': 491, 'word': '▁American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.6832874, 'index': 492, 'word': '▁West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-LOC', 2: 'I-LOC', 3: 'B-DAT', 4: 'I-DAT', 5: 'B-TIM', 6: 'I-TIM'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rollerhafeezh-amikom/xlm-roberta-base-ner-silvanus\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"rollerhafeezh-amikom/xlm-roberta-base-ner-silvanus\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "salida = nlp(text)\n",
    "salida\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "print(ner_results)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "e7cc7649-0234-4fb3-9802-e5909f30a3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'I-DAT', 'O', 'I-DAT', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[ 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    "'B-DAT',\n",
    "'I-DAT',\n",
    "'I-DAT',\n",
    "'I-DAT',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    "'B-DAT',\n",
    "'I-DAT',\n",
    "'I-DAT',\n",
    "'I-DAT',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "c13f0e6a-289b-4b1d-aac6-b73527e1009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DAT': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2}, 'LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.9129554655870445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1d357-0650-4946-8703-0fe86143eb17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 42 orgcatorg/distilbert-base-multilingual-cased-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "b5293a8a-c29f-4090-9db2-b7f92b4842f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-LOC', 'score': 0.9980641, 'index': 104, 'word': 'C', 'start': 435, 'end': 436}, {'entity': 'B-LOC', 'score': 0.99705327, 'index': 105, 'word': '##yd', 'start': 436, 'end': 438}, {'entity': 'B-LOC', 'score': 0.9987865, 'index': 106, 'word': '##onia', 'start': 438, 'end': 442}, {'entity': 'B-PER', 'score': 0.66370535, 'index': 121, 'word': 'Egypt', 'start': 496, 'end': 501}, {'entity': 'B-PER', 'score': 0.6953692, 'index': 122, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'I-PER', 'score': 0.6620473, 'index': 123, 'word': 'Ph', 'start': 505, 'end': 507}, {'entity': 'I-PER', 'score': 0.7284523, 'index': 124, 'word': '##ara', 'start': 507, 'end': 510}, {'entity': 'I-PER', 'score': 0.704528, 'index': 125, 'word': '##oh', 'start': 510, 'end': 512}, {'entity': 'B-PER', 'score': 0.9904669, 'index': 316, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.9859772, 'index': 317, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'I-PER', 'score': 0.9845413, 'index': 318, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'B-LOC', 'score': 0.6711369, 'index': 492, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.49512243, 'index': 493, 'word': 'West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"orgcatorg/distilbert-base-multilingual-cased-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"orgcatorg/distilbert-base-multilingual-cased-ner\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "salida = nlp(text)\n",
    "print(salida)\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word']: entidad['entity'] for entidad in entidades}\n",
    "    labels = [entidad_map.get(token, 'O') for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "\n",
    "\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "63543f53-7959-4cae-9090-926897ead7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=[\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'B-LOC',\n",
    " 'B-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "46a11308-c538-4864-8fb2-a9d8dbf05a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 1.0, 'recall': 0.8, 'f1': 0.888888888888889, 'number': 5}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'PER': {'precision': 0.2, 'recall': 1.0, 'f1': 0.33333333333333337, 'number': 1}, 'overall_precision': 0.5555555555555556, 'overall_recall': 0.5555555555555556, 'overall_f1': 0.5555555555555556, 'overall_accuracy': 0.9777327935222672}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846da5e7-bd48-4d24-9c48-4da51f22fadb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 43 orgcatorg/xlm-v-base-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "94d18491-a366-4197-b0c5-de86e41ee576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-ORG', 'score': 0.8529484, 'index': 8, 'word': '▁NASA', 'start': 15, 'end': 20}, {'entity': 'B-ORG', 'score': 0.67219996, 'index': 23, 'word': '▁Face', 'start': 87, 'end': 92}, {'entity': 'I-ORG', 'score': 0.9593816, 'index': 24, 'word': '▁On', 'start': 92, 'end': 95}, {'entity': 'I-ORG', 'score': 0.98047435, 'index': 25, 'word': '▁Mars', 'start': 95, 'end': 100}, {'entity': 'B-ORG', 'score': 0.97809476, 'index': 57, 'word': '▁Viking', 'start': 239, 'end': 246}, {'entity': 'I-ORG', 'score': 0.97545755, 'index': 58, 'word': '▁1', 'start': 246, 'end': 248}, {'entity': 'I-ORG', 'score': 0.5685065, 'index': 59, 'word': '▁space', 'start': 248, 'end': 254}, {'entity': 'B-LOC', 'score': 0.7317715, 'index': 93, 'word': '▁Marti', 'start': 406, 'end': 412}, {'entity': 'B-LOC', 'score': 0.77504426, 'index': 94, 'word': 'an', 'start': 412, 'end': 414}, {'entity': 'B-LOC', 'score': 0.73297995, 'index': 99, 'word': '▁Cy', 'start': 434, 'end': 437}, {'entity': 'B-LOC', 'score': 0.9430252, 'index': 113, 'word': '▁Egypti', 'start': 495, 'end': 502}, {'entity': 'B-LOC', 'score': 0.9459621, 'index': 114, 'word': 'on', 'start': 502, 'end': 504}, {'entity': 'I-LOC', 'score': 0.98246944, 'index': 115, 'word': '▁Pharao', 'start': 504, 'end': 511}, {'entity': 'I-LOC', 'score': 0.9633557, 'index': 116, 'word': 'h', 'start': 511, 'end': 512}, {'entity': 'B-PER', 'score': 0.9796036, 'index': 294, 'word': '▁Michael', 'start': 1310, 'end': 1318}, {'entity': 'I-PER', 'score': 0.98091847, 'index': 295, 'word': '▁Malin', 'start': 1318, 'end': 1324}, {'entity': 'B-ORG', 'score': 0.9358236, 'index': 298, 'word': '▁Mars', 'start': 1332, 'end': 1337}, {'entity': 'I-ORG', 'score': 0.90482205, 'index': 299, 'word': '▁Orbit', 'start': 1337, 'end': 1343}, {'entity': 'I-ORG', 'score': 0.70797783, 'index': 300, 'word': 'er', 'start': 1343, 'end': 1345}, {'entity': 'B-LOC', 'score': 0.73849326, 'index': 448, 'word': '▁but', 'start': 2039, 'end': 2043}, {'entity': 'B-LOC', 'score': 0.6988277, 'index': 449, 'word': 'te', 'start': 2043, 'end': 2045}, {'entity': 'B-LOC', 'score': 0.5565135, 'index': 460, 'word': '▁American', 'start': 2092, 'end': 2101}]\n",
      "entidad_map {'NASA': 'B-ORG', 'Face': 'B-ORG', 'On': 'I-ORG', 'Mars': 'B-ORG', 'Viking': 'B-ORG', '1': 'I-ORG', 'space': 'I-ORG', 'Marti': 'B-LOC', 'an': 'B-LOC', 'Cy': 'B-LOC', 'Egypti': 'B-LOC', 'on': 'B-LOC', 'Pharao': 'I-LOC', 'h': 'I-LOC', 'Michael': 'B-PER', 'Malin': 'I-PER', 'Orbit': 'I-ORG', 'er': 'I-ORG', 'but': 'B-LOC', 'te': 'B-LOC', 'American': 'B-LOC'}\n",
      "tokens ['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n",
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"orgcatorg/xlm-v-base-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"orgcatorg/xlm-v-base-ner\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "salida = nlp(text)\n",
    "print(salida)\n",
    "\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word'].replace('▁',''): entidad['entity'] for entidad in entidades}\n",
    "    print('entidad_map',entidad_map)\n",
    "    print('tokens',tokens)\n",
    "    labels = [entidad_map.get(token, 'O') if token != '' else '' for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels_ = mapeo(salida, tokens)\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "da937121-ff84-4ee6-9cdc-a624047999ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obvious', '##ly', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'alien', '##s', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'ci', '##rc', '##ling', 'the', 'planet', ',', 'sna', '##pping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'sh', '##adow', '##y', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figure', '##d', 'out', 'that', 'it', 'was', 'just', 'another', 'Mart', '##ian', 'mesa', ',', 'common', 'around', 'C', '##yd', '##onia', ',', 'only', 'this', 'one', 'had', 'sh', '##adow', '##s', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'res', '##emble', '##d', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'sh', '##adow', '##s', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'i', '##con', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'hau', '##nted', 'gr', '##oce', '##ry', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defender', '##s', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civili', '##zation', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'were', '##n', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##biter', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'reveal', '##ing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'marking', '##s', 'were', 'hidden', 'by', 'ha', '##ze', '\"', 'Well', 'no', ',', 'ye', '##s', 'that', 'rum', '##or', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'ama', '##zing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'pi', '##xel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'but', '##te', 'or', 'mesa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels_)\n",
    "print(tokens)\n",
    "etiquetas_referencia=['O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'B-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'I-PER',\n",
    " 'I-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "6aa77478-f2b9-4140-b09c-edecbaa434bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.9696356275303644}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2733d-93b9-447b-91d4-efae62f4d2b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 44 dejanseo/LinkBERT-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "7c2c315e-cf4c-453e-afcf-816049cfc16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea5c4f6028c4aeaad6b37a8462c8674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NW\\.cache\\huggingface\\hub\\models--dejanseo--LinkBERT-XL. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd73cfadde341f8964fc3d1aa4f02a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae2f339226947aea241bc1861b84251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff959098b144c6495e00334a4fac56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d19faf2d164654b9d38725e82dd437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dejanseo/LinkBERT-XL\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dejanseo/LinkBERT-XL\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "2a49f48a-a917-4756-ad26-cc4cf7f68a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96937981-4d0d-404c-ae4c-aefe77910390",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 45 HiTZ/mbert-argmining-abstrct-en-es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "8e292687-4b7c-40ab-9add-32f7190694c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HiTZ/mbert-argmining-abstrct-en-es\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"HiTZ/mbert-argmining-abstrct-en-es\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "salida = nlp(text)\n",
    "print(salida)\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word'].replace('▁',''): entidad['entity'] for entidad in entidades}\n",
    "    #print('entidad_map',entidad_map)\n",
    "    #print('tokens',tokens)\n",
    "    labels = [entidad_map.get(token, 'O') if token != '' else '' for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "1638402b-de39-4766-8775-4512fef31db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-Claim', 1: 'B-Premise', 2: 'I-Claim', 3: 'I-Premise', 4: 'O'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bcc2a-74f5-4168-bd46-81080ded913e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 46 HiTZ/mdeberta-expl-extraction-multi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "0b463753-3823-4b84-b214-530b4b2d82db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at HiTZ/mdeberta-expl-extraction-multi and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HiTZ/mdeberta-expl-extraction-multi\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"HiTZ/mdeberta-expl-extraction-multi\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68cccb-bc9e-415c-babb-3dd2b2ff72cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 47 rollerhafeezh-amikom/xlm-roberta-base-ner-augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "34fc2413-4e56-430f-bb99-16869761b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'B-LOC', 'score': 0.9935009, 'index': 103, 'word': '▁Cy', 'start': 435, 'end': 437}, {'entity': 'I-LOC', 'score': 0.9510317, 'index': 104, 'word': 'do', 'start': 437, 'end': 439}, {'entity': 'I-LOC', 'score': 0.96693367, 'index': 105, 'word': 'nia', 'start': 439, 'end': 442}, {'entity': 'B-LOC', 'score': 0.7514709, 'index': 264, 'word': '▁Mars', 'start': 1087, 'end': 1091}, {'entity': 'B-DAT', 'score': 0.9943066, 'index': 315, 'word': '▁April', 'start': 1296, 'end': 1301}, {'entity': 'I-DAT', 'score': 0.99545693, 'index': 316, 'word': '▁5', 'start': 1302, 'end': 1303}, {'entity': 'I-DAT', 'score': 0.9937779, 'index': 317, 'word': ',', 'start': 1303, 'end': 1304}, {'entity': 'I-DAT', 'score': 0.99656725, 'index': 318, 'word': '▁1998.', 'start': 1305, 'end': 1310}, {'entity': 'B-DAT', 'score': 0.9939989, 'index': 395, 'word': '▁April', 'start': 1651, 'end': 1656}, {'entity': 'I-DAT', 'score': 0.99549806, 'index': 396, 'word': '▁8', 'start': 1657, 'end': 1658}, {'entity': 'I-DAT', 'score': 0.994966, 'index': 397, 'word': ',', 'start': 1658, 'end': 1659}, {'entity': 'I-DAT', 'score': 0.99757344, 'index': 398, 'word': '▁2001', 'start': 1660, 'end': 1664}, {'entity': 'B-LOC', 'score': 0.77366734, 'index': 491, 'word': '▁American', 'start': 2093, 'end': 2101}, {'entity': 'I-LOC', 'score': 0.5598569, 'index': 492, 'word': '▁West', 'start': 2102, 'end': 2106}]\n",
      "{0: 'O', 1: 'B-LOC', 2: 'I-LOC', 3: 'B-DAT', 4: 'I-DAT', 5: 'B-TIM', 6: 'I-TIM'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rollerhafeezh-amikom/xlm-roberta-base-ner-augmentation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"rollerhafeezh-amikom/xlm-roberta-base-ner-augmentation\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word'].replace('▁','').replace('#',''): entidad['entity'] for entidad in entidades}\n",
    "    #print('entidad_map',entidad_map)\n",
    "    #print('tokens',tokens)\n",
    "    labels = [entidad_map.get(token.replace('▁','').replace('#',''), 'O') if token.replace('▁','').replace('#','') != '' else '' for token in tokens]\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "f5ae9b8f-c5ff-40ba-9956-1aa511d14d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'B-DAT', 'I-DAT', 'I-DAT', 'I-DAT', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DAT', 'I-DAT', 'I-DAT', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "['▁So', ',', '▁if', '▁you', \"'\", 're', '▁a', '▁NASA', '▁scientist', ',', '▁you', '▁should', '▁be', '▁able', '▁to', '▁tell', '▁me', '▁the', '▁whole', '▁story', '▁about', '▁the', '▁Face', '▁On', '▁Mars', ',', '▁which', '▁obviously', '▁is', '▁evidence', '▁that', '▁there', '▁is', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁the', '▁face', '▁was', '▁created', '▁by', '▁alien', 's', ',', '▁correct', '?\"', '▁No', ',', '▁twenty', '▁five', '▁years', '▁ago', ',', '▁our', '▁Viking', '▁1', '▁space', 'craft', '▁was', '▁circ', 'ling', '▁the', '▁planet', ',', '▁sna', 'pping', '▁photos', ',', '▁when', '▁it', '▁spot', 'ted', '▁the', '▁shadow', 'y', '▁like', 'ness', '▁of', '▁a', '▁human', '▁face', '.', '▁Us', '▁scientist', 's', '▁figure', 'd', '▁out', '▁that', '▁it', '▁was', '▁just', '▁another', '▁Marti', 'an', '▁mesa', ',', '▁common', '▁around', '▁Cy', 'do', 'nia', ',', '▁only', '▁this', '▁one', '▁had', '▁shadow', 's', '▁that', '▁made', '▁it', '▁look', '▁like', '▁an', '▁Egypt', 'ion', '▁Phar', 'a', 'oh', '.', '▁Very', '▁few', '▁days', '▁later', ',', '▁we', '▁reveal', 'ed', '▁the', '▁image', '▁for', '▁all', '▁to', '▁see', ',', '▁and', '▁we', '▁made', '▁sure', '▁to', '▁note', '▁that', '▁it', '▁was', '▁a', '▁huge', '▁rock', '▁formation', '▁that', '▁just', '▁rese', 'mble', 'd', '▁a', '▁human', '▁head', '▁and', '▁face', ',', '▁but', '▁all', '▁of', '▁it', '▁was', '▁for', 'med', '▁by', '▁shadow', 's', '.', '▁We', '▁only', '▁announced', '▁it', '▁because', '▁we', '▁thought', '▁it', '▁would', '▁be', '▁a', '▁good', '▁way', '▁to', '▁engage', '▁the', '▁public', '▁with', '▁NASA', \"'\", 's', '▁finding', 's', ',', '▁and', '▁at', 'rra', 'ct', '▁attention', '▁to', '▁Mars', '-', '-', '▁and', '▁it', '▁did', '.', '▁The', '▁face', '▁on', '▁Mars', '▁soon', '▁became', '▁a', '▁pop', '▁icon', ';', '▁shot', '▁in', '▁movies', ',', '▁appeared', '▁in', '▁books', ',', '▁magazine', 's', ',', '▁radio', '▁talk', '▁shows', ',', '▁and', '▁ha', 'un', 'ted', '▁gro', 'cer', 'y', '▁store', '▁check', 'out', '▁lines', '▁for', '▁25', '▁years', '.', '▁Some', '▁people', '▁thought', '▁the', '▁natural', '▁land', 'form', '▁was', '▁evidence', '▁of', '▁life', '▁on', '▁Mars', ',', '▁and', '▁that', '▁us', '▁scientist', 's', '▁wanted', '▁to', '▁hi', 'de', '▁it', ',', '▁but', '▁really', ',', '▁the', '▁defender', 's', '▁of', '▁the', '▁NASA', '▁budget', '▁wish', '▁there', '▁was', '▁an', 'cient', '▁civiliza', 'tion', '▁on', '▁Mars', '.', '▁We', '▁decided', '▁to', '▁take', '▁another', '▁shot', '▁just', '▁to', '▁make', '▁sure', '▁we', '▁were', 'n', \"'\", 't', '▁wrong', ',', '▁on', '▁April', '▁5', ',', '▁1998.', '▁Michael', '▁Malin', '▁and', '▁his', '▁Mars', '▁Or', 'bit', 'er', '▁camera', '▁team', '▁took', '▁a', '▁picture', '▁that', '▁was', '▁ten', '▁times', '▁sharp', 'er', '▁than', '▁the', '▁original', '▁Viking', '▁photos', ',', '▁reveal', 'ing', '▁a', '▁natural', '▁land', 'form', ',', '▁which', '▁meant', '▁no', '▁alien', '▁monument', '.', '▁\"', 'But', '▁that', '▁picture', '▁wasn', \"'\", 't', '▁very', '▁clear', '▁at', '▁all', ',', '▁which', '▁could', '▁mean', '▁alien', '▁mark', 'ings', '▁were', '▁hidden', '▁by', '▁ha', 'ze', '\"', '▁Well', '▁no', ',', '▁yes', '▁that', '▁rumor', '▁started', ',', '▁but', '▁to', '▁prove', '▁them', '▁wrong', '▁on', '▁April', '▁8', ',', '▁2001', '▁we', '▁decided', '▁to', '▁take', '▁another', '▁picture', ',', '▁making', '▁sure', '▁it', '▁was', '▁a', '▁cloud', 'less', '▁summer', '▁day', '.', '▁Malin', \"'\", 's', '▁team', '▁capture', 'd', '▁an', '▁amazing', '▁photo', '▁using', '▁the', '▁camera', \"'\", 's', '▁absolute', '▁maximum', '▁revolution', '.', '▁With', '▁this', '▁camera', '▁you', '▁can', '▁discern', '▁things', '▁in', '▁a', '▁digital', '▁image', ',', '▁3', '▁times', '▁bigger', '▁than', '▁the', '▁pixel', '▁size', '▁which', '▁means', '▁if', '▁there', '▁were', '▁any', '▁sign', 's', '▁of', '▁life', ',', '▁you', '▁could', '▁easily', '▁see', '▁what', '▁they', '▁were', '.', '▁What', '▁the', '▁picture', '▁showed', '▁was', '▁the', '▁but', 'te', '▁or', '▁mesa', ',', '▁which', '▁are', '▁land', 'form', 's', '▁common', '▁around', '▁the', '▁American', '▁West', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=['O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-DAT',\n",
    " 'I-DAT',\n",
    " 'I-DAT',\n",
    " 'I-DAT',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    "'O',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "ff1a9e91-ea51-4d39-a65f-66698582befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DAT': {'precision': 0.029411764705882353, 'recall': 1.0, 'f1': 0.05714285714285715, 'number': 1}, 'LOC': {'precision': 0.2222222222222222, 'recall': 1.0, 'f1': 0.3636363636363636, 'number': 2}, 'overall_precision': 0.06976744186046512, 'overall_recall': 1.0, 'overall_f1': 0.13043478260869565, 'overall_accuracy': 0.9127789046653144}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebdd71-4bb0-4c49-af4d-10343c9e273e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 48 brettlin/distilbert-base-uncased-finetuned-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "39770965-3574-47f2-ad55-44bb35f11864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7', 8: 'LABEL_8', 9: 'LABEL_9', 10: 'LABEL_10', 11: 'LABEL_11', 12: 'LABEL_12', 13: 'LABEL_13', 14: 'LABEL_14', 15: 'LABEL_15', 16: 'LABEL_16', 17: 'LABEL_17', 18: 'LABEL_18', 19: 'LABEL_19', 20: 'LABEL_20', 21: 'LABEL_21', 22: 'LABEL_22', 23: 'LABEL_23', 24: 'LABEL_24', 25: 'LABEL_25', 26: 'LABEL_26', 27: 'LABEL_27', 28: 'LABEL_28', 29: 'LABEL_29', 30: 'LABEL_30', 31: 'LABEL_31', 32: 'LABEL_32', 33: 'LABEL_33', 34: 'LABEL_34', 35: 'LABEL_35', 36: 'LABEL_36', 37: 'LABEL_37', 38: 'LABEL_38', 39: 'LABEL_39', 40: 'LABEL_40', 41: 'LABEL_41', 42: 'LABEL_42', 43: 'LABEL_43', 44: 'LABEL_44', 45: 'LABEL_45', 46: 'LABEL_46', 47: 'LABEL_47', 48: 'LABEL_48', 49: 'LABEL_49', 50: 'LABEL_50', 51: 'LABEL_51', 52: 'LABEL_52', 53: 'LABEL_53', 54: 'LABEL_54', 55: 'LABEL_55', 56: 'LABEL_56', 57: 'LABEL_57', 58: 'LABEL_58', 59: 'LABEL_59', 60: 'LABEL_60', 61: 'LABEL_61', 62: 'LABEL_62', 63: 'LABEL_63', 64: 'LABEL_64', 65: 'LABEL_65', 66: 'LABEL_66', 67: 'LABEL_67', 68: 'LABEL_68', 69: 'LABEL_69', 70: 'LABEL_70', 71: 'LABEL_71', 72: 'LABEL_72', 73: 'LABEL_73', 74: 'LABEL_74', 75: 'LABEL_75', 76: 'LABEL_76', 77: 'LABEL_77', 78: 'LABEL_78', 79: 'LABEL_79', 80: 'LABEL_80', 81: 'LABEL_81', 82: 'LABEL_82', 83: 'LABEL_83', 84: 'LABEL_84', 85: 'LABEL_85', 86: 'LABEL_86', 87: 'LABEL_87', 88: 'LABEL_88', 89: 'LABEL_89', 90: 'LABEL_90', 91: 'LABEL_91', 92: 'LABEL_92', 93: 'LABEL_93', 94: 'LABEL_94', 95: 'LABEL_95', 96: 'LABEL_96', 97: 'LABEL_97', 98: 'LABEL_98', 99: 'LABEL_99', 100: 'LABEL_100', 101: 'LABEL_101', 102: 'LABEL_102', 103: 'LABEL_103', 104: 'LABEL_104', 105: 'LABEL_105', 106: 'LABEL_106', 107: 'LABEL_107', 108: 'LABEL_108', 109: 'LABEL_109', 110: 'LABEL_110', 111: 'LABEL_111', 112: 'LABEL_112', 113: 'LABEL_113', 114: 'LABEL_114', 115: 'LABEL_115', 116: 'LABEL_116'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"brettlin/distilbert-base-uncased-finetuned-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"brettlin/distilbert-base-uncased-finetuned-ner\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c8f4a-5e6d-4c7a-9b4e-263d5181d0eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 49 papluca/xlm-roberta-base-language-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "9e192dd7-a08d-4f3f-a4ea-eb22adb0c755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at papluca/xlm-roberta-base-language-detection and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'ja', 1: 'nl', 2: 'ar', 3: 'pl', 4: 'de', 5: 'it', 6: 'pt', 7: 'tr', 8: 'es', 9: 'hi', 10: 'el', 11: 'ur', 12: 'bg', 13: 'en', 14: 'fr', 15: 'zh', 16: 'ru', 17: 'th', 18: 'sw', 19: 'vi'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "nlp = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d03c8-bed4-4325-8f59-1fe8438a5615",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 50 mbruton/spa_en_mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "87a8abbb-47a5-4f90-8655-d3dd9f8ade18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'r0:arg0|agt', 2: 'r0:arg0|cau', 3: 'r0:arg0|exp', 4: 'r0:arg0|pat', 5: 'r0:arg0|src', 6: 'r0:arg1|ext', 7: 'r0:arg1|loc', 8: 'r0:arg1|pat', 9: 'r0:arg1|tem', 10: 'r0:arg2|atr', 11: 'r0:arg2|ben', 12: 'r0:arg2|efi', 13: 'r0:arg2|exp', 14: 'r0:arg2|ext', 15: 'r0:arg2|ins', 16: 'r0:arg2|loc', 17: 'r0:arg2|tem', 18: 'r0:arg3|ben', 19: 'r0:arg3|ein', 20: 'r0:arg3|exp', 21: 'r0:arg3|fin', 22: 'r0:arg3|ins', 23: 'r0:arg3|loc', 24: 'r0:arg3|ori', 25: 'r0:arg4|des', 26: 'r0:arg4|efi', 27: 'r0:argM|LOC', 28: 'r0:argM|adv', 29: 'r0:argM|atr', 30: 'r0:argM|cau', 31: 'r0:argM|ext', 32: 'r0:argM|fin', 33: 'r0:argM|ins', 34: 'r0:argM|loc', 35: 'r0:argM|mnr', 36: 'r0:argM|tmp', 37: 'r0:root', 38: 'r10:arg0|agt', 39: 'r10:arg1|pat', 40: 'r10:arg1|tem', 41: 'r10:arg2|atr', 42: 'r10:arg2|ben', 43: 'r10:arg2|efi', 44: 'r10:arg2|loc', 45: 'r10:arg3|ben', 46: 'r10:arg4|des', 47: 'r10:argM|adv', 48: 'r10:argM|atr', 49: 'r10:argM|fin', 50: 'r10:argM|loc', 51: 'r10:argM|tmp', 52: 'r10:root', 53: 'r11:arg0|agt', 54: 'r11:arg0|cau', 55: 'r11:arg1|pat', 56: 'r11:arg1|tem', 57: 'r11:arg2|atr', 58: 'r11:arg2|ben', 59: 'r11:arg2|loc', 60: 'r11:arg4|des', 61: 'r11:argM|adv', 62: 'r11:argM|loc', 63: 'r11:argM|mnr', 64: 'r11:argM|tmp', 65: 'r11:root', 66: 'r12:arg0|agt', 67: 'r12:arg0|cau', 68: 'r12:arg1|pat', 69: 'r12:arg1|tem', 70: 'r12:arg2|atr', 71: 'r12:argM|adv', 72: 'r12:argM|cau', 73: 'r12:argM|loc', 74: 'r12:argM|tmp', 75: 'r12:root', 76: 'r13:arg0|agt', 77: 'r13:arg0|cau', 78: 'r13:arg1|pat', 79: 'r13:arg1|tem', 80: 'r13:arg2|atr', 81: 'r13:arg2|ben', 82: 'r13:argM|adv', 83: 'r13:argM|atr', 84: 'r13:argM|loc', 85: 'r13:root', 86: 'r14:arg0|agt', 87: 'r14:arg1|pat', 88: 'r14:arg2|ben', 89: 'r14:argM|adv', 90: 'r14:argM|loc', 91: 'r14:argM|mnr', 92: 'r14:root', 93: 'r15:arg0|cau', 94: 'r15:arg1|tem', 95: 'r15:arg2|atr', 96: 'r15:arg3|ben', 97: 'r15:root', 98: 'r16:arg0|agt', 99: 'r16:arg0|cau', 100: 'r16:arg1|pat', 101: 'r16:arg1|tem', 102: 'r16:argM|loc', 103: 'r16:argM|tmp', 104: 'r16:root', 105: 'r1:arg0|agt', 106: 'r1:arg0|cau', 107: 'r1:arg0|exp', 108: 'r1:arg0|src', 109: 'r1:arg1|ext', 110: 'r1:arg1|loc', 111: 'r1:arg1|pat', 112: 'r1:arg1|tem', 113: 'r1:arg2|atr', 114: 'r1:arg2|ben', 115: 'r1:arg2|efi', 116: 'r1:arg2|exp', 117: 'r1:arg2|ext', 118: 'r1:arg2|ins', 119: 'r1:arg2|loc', 120: 'r1:arg3|atr', 121: 'r1:arg3|ben', 122: 'r1:arg3|des', 123: 'r1:arg3|ein', 124: 'r1:arg3|exp', 125: 'r1:arg3|fin', 126: 'r1:arg3|ins', 127: 'r1:arg3|ori', 128: 'r1:arg4|des', 129: 'r1:arg4|efi', 130: 'r1:argM|adv', 131: 'r1:argM|atr', 132: 'r1:argM|cau', 133: 'r1:argM|ext', 134: 'r1:argM|fin', 135: 'r1:argM|ins', 136: 'r1:argM|loc', 137: 'r1:argM|mnr', 138: 'r1:argM|tmp', 139: 'r1:root', 140: 'r2:arg0|agt', 141: 'r2:arg0|cau', 142: 'r2:arg0|exp', 143: 'r2:arg0|src', 144: 'r2:arg1|ext', 145: 'r2:arg1|loc', 146: 'r2:arg1|pat', 147: 'r2:arg1|tem', 148: 'r2:arg2|atr', 149: 'r2:arg2|ben', 150: 'r2:arg2|efi', 151: 'r2:arg2|exp', 152: 'r2:arg2|ext', 153: 'r2:arg2|ins', 154: 'r2:arg2|loc', 155: 'r2:arg3|atr', 156: 'r2:arg3|ben', 157: 'r2:arg3|ein', 158: 'r2:arg3|exp', 159: 'r2:arg3|fin', 160: 'r2:arg3|loc', 161: 'r2:arg3|ori', 162: 'r2:arg4|des', 163: 'r2:arg4|efi', 164: 'r2:argM|adv', 165: 'r2:argM|atr', 166: 'r2:argM|cau', 167: 'r2:argM|ext', 168: 'r2:argM|fin', 169: 'r2:argM|ins', 170: 'r2:argM|loc', 171: 'r2:argM|mnr', 172: 'r2:argM|tmp', 173: 'r2:root', 174: 'r3:arg0|agt', 175: 'r3:arg0|cau', 176: 'r3:arg0|exp', 177: 'r3:arg0|src', 178: 'r3:arg1|ext', 179: 'r3:arg1|loc', 180: 'r3:arg1|pat', 181: 'r3:arg1|tem', 182: 'r3:arg2|atr', 183: 'r3:arg2|ben', 184: 'r3:arg2|efi', 185: 'r3:arg2|exp', 186: 'r3:arg2|ext', 187: 'r3:arg2|ins', 188: 'r3:arg2|loc', 189: 'r3:arg2|tem', 190: 'r3:arg3|ben', 191: 'r3:arg3|ein', 192: 'r3:arg3|fin', 193: 'r3:arg3|loc', 194: 'r3:arg3|ori', 195: 'r3:arg4|des', 196: 'r3:arg4|efi', 197: 'r3:argM|adv', 198: 'r3:argM|atr', 199: 'r3:argM|cau', 200: 'r3:argM|ext', 201: 'r3:argM|fin', 202: 'r3:argM|ins', 203: 'r3:argM|loc', 204: 'r3:argM|mnr', 205: 'r3:argM|tmp', 206: 'r3:root', 207: 'r4:arg0|agt', 208: 'r4:arg0|cau', 209: 'r4:arg0|exp', 210: 'r4:arg0|src', 211: 'r4:arg1|ext', 212: 'r4:arg1|loc', 213: 'r4:arg1|pat', 214: 'r4:arg1|tem', 215: 'r4:arg2|atr', 216: 'r4:arg2|ben', 217: 'r4:arg2|efi', 218: 'r4:arg2|exp', 219: 'r4:arg2|ext', 220: 'r4:arg2|ins', 221: 'r4:arg2|loc', 222: 'r4:arg3|ben', 223: 'r4:arg3|ein', 224: 'r4:arg3|exp', 225: 'r4:arg3|fin', 226: 'r4:arg3|ori', 227: 'r4:arg4|des', 228: 'r4:arg4|efi', 229: 'r4:argM|adv', 230: 'r4:argM|atr', 231: 'r4:argM|cau', 232: 'r4:argM|ext', 233: 'r4:argM|fin', 234: 'r4:argM|ins', 235: 'r4:argM|loc', 236: 'r4:argM|mnr', 237: 'r4:argM|tmp', 238: 'r4:root', 239: 'r5:arg0|agt', 240: 'r5:arg0|cau', 241: 'r5:arg1|ext', 242: 'r5:arg1|loc', 243: 'r5:arg1|pat', 244: 'r5:arg1|tem', 245: 'r5:arg2|atr', 246: 'r5:arg2|ben', 247: 'r5:arg2|efi', 248: 'r5:arg2|exp', 249: 'r5:arg2|ext', 250: 'r5:arg2|loc', 251: 'r5:arg3|ben', 252: 'r5:arg3|ein', 253: 'r5:arg3|fin', 254: 'r5:arg3|ins', 255: 'r5:arg3|ori', 256: 'r5:arg4|des', 257: 'r5:arg4|efi', 258: 'r5:argM|adv', 259: 'r5:argM|atr', 260: 'r5:argM|cau', 261: 'r5:argM|ext', 262: 'r5:argM|fin', 263: 'r5:argM|loc', 264: 'r5:argM|mnr', 265: 'r5:argM|tmp', 266: 'r5:root', 267: 'r6:arg0|agt', 268: 'r6:arg0|cau', 269: 'r6:arg1|loc', 270: 'r6:arg1|pat', 271: 'r6:arg1|tem', 272: 'r6:arg2|atr', 273: 'r6:arg2|ben', 274: 'r6:arg2|efi', 275: 'r6:arg2|exp', 276: 'r6:arg2|ext', 277: 'r6:arg2|loc', 278: 'r6:arg3|ben', 279: 'r6:arg3|ori', 280: 'r6:arg4|des', 281: 'r6:argM|adv', 282: 'r6:argM|atr', 283: 'r6:argM|cau', 284: 'r6:argM|ext', 285: 'r6:argM|fin', 286: 'r6:argM|loc', 287: 'r6:argM|mnr', 288: 'r6:argM|tmp', 289: 'r6:root', 290: 'r7:arg0|agt', 291: 'r7:arg0|cau', 292: 'r7:arg1|loc', 293: 'r7:arg1|pat', 294: 'r7:arg1|tem', 295: 'r7:arg2|atr', 296: 'r7:arg2|ben', 297: 'r7:arg2|efi', 298: 'r7:arg2|loc', 299: 'r7:arg3|ben', 300: 'r7:arg3|exp', 301: 'r7:arg3|fin', 302: 'r7:arg3|ori', 303: 'r7:arg4|des', 304: 'r7:argM|adv', 305: 'r7:argM|atr', 306: 'r7:argM|cau', 307: 'r7:argM|fin', 308: 'r7:argM|ins', 309: 'r7:argM|loc', 310: 'r7:argM|mnr', 311: 'r7:argM|tmp', 312: 'r7:root', 313: 'r8:arg0|agt', 314: 'r8:arg0|cau', 315: 'r8:arg0|src', 316: 'r8:arg1|pat', 317: 'r8:arg1|tem', 318: 'r8:arg2|atr', 319: 'r8:arg2|ben', 320: 'r8:arg2|ext', 321: 'r8:arg2|loc', 322: 'r8:arg3|ori', 323: 'r8:arg4|des', 324: 'r8:argM|adv', 325: 'r8:argM|cau', 326: 'r8:argM|ext', 327: 'r8:argM|fin', 328: 'r8:argM|loc', 329: 'r8:argM|mnr', 330: 'r8:argM|tmp', 331: 'r8:root', 332: 'r9:arg0|agt', 333: 'r9:arg0|cau', 334: 'r9:arg1|pat', 335: 'r9:arg1|tem', 336: 'r9:arg2|atr', 337: 'r9:arg2|ben', 338: 'r9:arg2|ins', 339: 'r9:arg2|loc', 340: 'r9:arg4|des', 341: 'r9:argM|adv', 342: 'r9:argM|cau', 343: 'r9:argM|fin', 344: 'r9:argM|loc', 345: 'r9:argM|mnr', 346: 'r9:argM|tmp', 347: 'r9:root'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mbruton/spa_en_mBERT\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"mbruton/spa_en_mBERT\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd22ec6-6192-4ff8-b482-b7fdbfac312c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 51 BSC-LT/roberta_model_for_anonimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "6cd4a769-e99c-453f-83c6-447c4c69ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entidad_map {'NASA': 'S-ORG', 'Cara': 'S-OTH', 'Marte': 'S-LOC', 'Vik': 'S-OTH', 'ing': 'I-OTH', '1': 'E-OTH', 'Cid': 'S-LOC', 'Ċ': 'S-OTH', 'Diana': 'B-PER', 'MartÃŃnez': 'S-PER', 'Mars': 'B-OTH', 'Or': 'E-OTH', 'bi': 'E-OTH', 'ter': 'E-OTH'}\n",
      "tokens ['▁Entonces', ',', '▁si', '▁eres', '▁un', '▁científico', '▁de', '▁la', '▁NASA', ',', '▁debería', 's', '▁poder', '▁contar', 'me', '▁toda', '▁la', '▁historia', '▁sobre', '▁la', '▁Cara', '▁en', '▁Marte', ',', '▁que', '▁obviamente', '▁es', '▁evidencia', '▁de', '▁que', '▁hay', '▁vida', '▁en', '▁Marte', '▁y', '▁que', '▁la', '▁cara', '▁fue', '▁creada', '▁por', '▁extra', 'ter', 're', 'stres', ',', '▁¿', 'corre', 'cto', '?\"', '▁No', '.', '▁Hace', '▁ve', 'inti', 'cin', 'co', '▁años', ',', '▁nuestra', '▁nave', '▁espacial', '▁Viking', '▁1', '▁estaba', '▁dando', '▁vuelta', 's', '▁alrededor', '▁del', '▁planeta', ',', '▁tomando', '▁fotografías', ',', '▁cuando', '▁detect', 'ó', '▁la', '▁sombra', '▁de', '▁un', '▁rostro', '▁humano', '.', '▁Los', '▁científicos', '▁estadounidense', 's', '▁descubri', 'eron', '▁que', '▁se', '▁trata', 'ba', '▁simplemente', '▁de', '▁otra', '▁mesa', '▁marcia', 'na', ',', '▁común', '▁alrededor', '▁de', '▁Ci', 'do', 'nia', ',', '▁solo', '▁que', '▁és', 'ta', '▁tenía', '▁sombra', 's', '.', '▁Eso', '▁lo', '▁hizo', '▁parecer', '▁un', '▁fara', 'ón', '▁e', 'gip', 'cio', '.', '▁Muy', '▁pocos', '▁días', '▁después', ',', '▁revela', 'mos', '▁la', '▁imagen', '▁para', '▁que', '▁todos', '▁la', '▁vier', 'an', ',', '▁y', '▁nos', '▁asegura', 'mos', '▁de', '▁notar', '▁que', '▁era', '▁una', '▁enorme', '▁formación', '▁ro', 'cosa', '▁que', '▁simplemente', '▁se', '▁parecía', '▁a', '▁una', '▁cabeza', '▁y', '▁un', '▁rostro', '▁humanos', ',', '▁pero', '▁todo', '▁era', '.', '▁formado', '▁por', '▁sombra', 's', '▁Sólo', '▁lo', '▁anuncia', 'mos', '▁porque', '▁pensamos', '▁que', '▁sería', '▁una', '▁buena', '▁manera', '▁de', '▁involucra', 'r', '▁al', '▁público', '▁con', '▁los', '▁halla', 'z', 'gos', '▁de', '▁la', '▁NASA', '▁y', '▁atra', 'er', '▁la', '▁atención', '▁a', '▁Marte', ',', '▁y', '▁así', '▁fue', '.', '▁El', '▁rostro', '▁de', '▁Marte', '▁pronto', '▁se', '▁convirtió', '▁en', '▁un', '▁í', 'con', 'o', '▁pop', ';', '▁film', 'ada', '▁en', '▁películas', ',', '▁apare', 'ció', '▁en', '▁libros', ',', '▁revista', 's', ',', '▁programas', '▁de', '▁radio', '▁y', '▁en', '▁las', '▁cola', 's', '▁de', '▁las', '▁ca', 'jas', '▁de', '▁las', '▁tiendas', '▁de', '▁com', 'est', 'ibles', '▁durante', '▁25', '▁años', '.', '▁Al', 'guna', 's', '▁personas', '▁pensar', 'on', '▁que', '▁la', '▁forma', '▁natural', '▁del', '▁reli', 'eve', '▁era', '▁evidencia', '▁de', '▁vida', '▁en', '▁Marte', ',', '▁y', '▁que', '▁los', '▁científicos', '▁que', 'ríamos', '▁oculta', 'rla', ',', '▁pero', '▁en', '▁realidad', ',', '▁los', '▁defensor', 'es', '▁del', '▁presupuesto', '▁de', '▁la', '▁NASA', '▁desea', 'rían', '▁que', '▁hubiera', '▁una', '▁civiliza', 'ción', '▁antigua', '▁en', '▁Marte', '.', '▁Deci', 'di', 'mos', '▁tomar', '▁otra', '▁foto', '▁solo', '▁para', '▁asegurar', 'nos', '▁de', '▁no', '▁estar', '▁', 'equivoca', 'dos', ',', '▁el', '▁5', '▁de', '▁abril', '▁de', '▁1998.', '▁Diana', '▁Martínez', '▁y', '▁su', '▁equipo', '▁de', '▁cámara', '▁de', '▁Mars', '▁Or', 'bit', 'er', '▁tomar', 'on', '▁una', '▁fotografía', '▁que', '▁era', '▁die', 'z', '▁veces', '▁más', '▁ní', 'tida', '▁que', '▁las', '▁fotografías', '▁originale', 's', '▁del', '▁Viking', ',', '▁revela', 'ndo', '▁una', '▁forma', '▁de', '▁reli', 'eve', '▁natural', ',', '▁que', '▁No', '▁significa', 'ba', '▁ningún', '▁monumento', '▁ali', 'ení', 'gen', 'a', '.', '▁\"', 'Pero', '▁esa', '▁imagen', '▁no', '▁era', '▁muy', '▁clara', '▁en', '▁absolut', 'o', ',', '▁lo', '▁que', '▁podría', '▁significa', 'r', '▁que', '▁las', '▁marcas', '▁ali', 'ení', 'gen', 'as', '▁estaban', '▁oculta', 's', '▁por', '▁la', '▁neb', 'lina', '\"', '▁Bueno', ',', '▁no', ',', '▁sí', ',', '▁ese', '▁rumor', '▁comenzó', ',', '▁pero', '▁para', '▁demostrar', '▁que', '▁estaban', '▁', 'equivoca', 'dos', ',', '▁el', '▁8', '▁de', '▁abril', '▁de', '▁2001', '▁decidi', 'mos', '▁tomar', '▁otra', '▁fotografía', ',', '▁a', 'segur', 'ándonos', '▁de', '▁que', '▁Era', '▁un', '▁día', '▁de', '▁verano', '▁sin', '▁nu', 'bes', '.', '▁El', '▁equipo', '▁de', '▁Martínez', '▁cap', 'tur', 'ó', '▁una', '▁fotografía', '▁as', 'o', 'mbro', 'sa', '▁utilizando', '▁la', '▁revolución', '▁máxima', '▁absoluta', '▁de', '▁la', '▁cámara', '.', '▁Con', '▁esta', '▁cámara', '▁puedes', '▁discern', 'ir', '▁cosas', '▁en', '▁una', '▁imagen', '▁digital', ',', '▁3', '▁veces', '▁más', '▁grande', '▁que', '▁el', '▁tamaño', '▁de', '▁un', '▁pí', 'xel', ',', '▁lo', '▁que', '▁significa', '▁que', '▁si', '▁hubiera', '▁señal', 'es', '▁de', '▁vida', ',', '▁podría', 's', '▁ver', '▁fácilmente', '▁cu', 'ál', 'es', '▁eran', '.', '▁Lo', '▁que', '▁la', '▁imagen', '▁mostra', 'ba', '▁era', '▁la', '▁col', 'ina', '▁o', '▁mesa', ',', '▁que', '▁son', '▁accidente', 's', '▁geo', 'gráficos', '▁comune', 's', '▁en', '▁el', '▁o', 'este', '▁americano', '.']\n",
      "{0: 'B-LOC', 1: 'B-ORG', 2: 'B-OTH', 3: 'B-PER', 4: 'E-LOC', 5: 'E-ORG', 6: 'E-OTH', 7: 'E-PER', 8: 'I-LOC', 9: 'I-ORG', 10: 'I-OTH', 11: 'I-PER', 12: 'O', 13: 'S-LOC', 14: 'S-ORG', 15: 'S-OTH', 16: 'S-PER'}\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"Entonces, si eres un científico de la NASA, deberías poder contarme toda la historia sobre la Cara en Marte, que obviamente es evidencia\n",
    "de que hay vida en Marte y que la cara fue creada por extraterrestres, ¿correcto?\" No. Hace veinticinco años, nuestra nave espacial Viking 1\n",
    "estaba dando vueltas alrededor del planeta, tomando fotografías, cuando detectó la sombra de un rostro humano. Los científicos estadounidenses\n",
    "descubrieron que se trataba simplemente de otra mesa marciana, común alrededor de Cidonia, solo que ésta tenía sombras. Eso lo hizo parecer un\n",
    "faraón egipcio. Muy pocos días después, revelamos la imagen para que todos la vieran, y nos aseguramos de notar que era una enorme formación \n",
    "rocosa que simplemente se parecía a una cabeza y un rostro humanos, pero todo era. formado por sombras Sólo lo anunciamos porque pensamos \n",
    "que sería una buena manera de involucrar al público con los hallazgos de la NASA y atraer la atención a Marte, y así fue.\n",
    "\n",
    "El rostro de Marte pronto se convirtió en un ícono pop; filmada en películas, apareció en libros, revistas, programas de radio y en \n",
    "las colas de las cajas de las tiendas de comestibles durante 25 años. Algunas personas pensaron que la forma natural del relieve era evidencia\n",
    "de vida en Marte, y que los científicos queríamos ocultarla, pero en realidad, los defensores del presupuesto de la NASA desearían que hubiera\n",
    "una civilización antigua en Marte. Decidimos tomar otra foto solo para asegurarnos de no estar equivocados, el 5 de abril de 1998. Diana Martínez  \n",
    "y su equipo de cámara de Mars Orbiter tomaron una fotografía que era diez veces más nítida que las fotografías originales del Viking, revelando\n",
    "una forma de relieve natural, que No significaba ningún monumento alienígena. \"Pero esa imagen no era muy clara en absoluto, lo que podría\n",
    "significar que las marcas alienígenas estaban ocultas por la neblina\" Bueno, no, sí, ese rumor comenzó, pero para demostrar que estaban\n",
    "equivocados, el 8 de abril de 2001 decidimos tomar otra fotografía, asegurándonos de que Era un día de verano sin nubes. El equipo de Martínez  capturó una fotografía asombrosa utilizando la revolución máxima absoluta de la cámara. Con esta cámara puedes discernir cosas en una imagen digital, 3 veces más grande que el tamaño de un píxel, lo que significa que si hubiera señales de vida, podrías ver fácilmente cuáles eran. Lo que la imagen mostraba era la colina o mesa, que son accidentes geográficos comunes en el oeste americano.\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BSC-LT/roberta_model_for_anonimization\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"BSC-LT/roberta_model_for_anonimization\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "salida = nlp(text)\n",
    "salida\n",
    "\n",
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word'].replace('▁','').replace('Ġ',''): entidad['entity'] for entidad in entidades}\n",
    "    print('entidad_map',entidad_map)\n",
    "    print('tokens',tokens)\n",
    "    labels = [entidad_map.get(token.replace('Ġ','').replace('▁',''), 'O') if token.replace('Ġ','').replace('▁','') != '' else '' for token in tokens]\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = mapeo(salida, tokens)\n",
    "\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "198a89d5-2c56-4fa1-85c0-b19441b18eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n"
     ]
    }
   ],
   "source": [
    "print(len(etiquetas_referencia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "b46cee5f-5626-4ef8-b5de-8c57f5a142a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-OTH', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'E-OTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'E-OTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTH', 'E-OTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['▁Entonces', ',', '▁si', '▁eres', '▁un', '▁científico', '▁de', '▁la', '▁NASA', ',', '▁debería', 's', '▁poder', '▁contar', 'me', '▁toda', '▁la', '▁historia', '▁sobre', '▁la', '▁Cara', '▁en', '▁Marte', ',', '▁que', '▁obviamente', '▁es', '▁evidencia', '▁de', '▁que', '▁hay', '▁vida', '▁en', '▁Marte', '▁y', '▁que', '▁la', '▁cara', '▁fue', '▁creada', '▁por', '▁extra', 'ter', 're', 'stres', ',', '▁¿', 'corre', 'cto', '?\"', '▁No', '.', '▁Hace', '▁ve', 'inti', 'cin', 'co', '▁años', ',', '▁nuestra', '▁nave', '▁espacial', '▁Viking', '▁1', '▁estaba', '▁dando', '▁vuelta', 's', '▁alrededor', '▁del', '▁planeta', ',', '▁tomando', '▁fotografías', ',', '▁cuando', '▁detect', 'ó', '▁la', '▁sombra', '▁de', '▁un', '▁rostro', '▁humano', '.', '▁Los', '▁científicos', '▁estadounidense', 's', '▁descubri', 'eron', '▁que', '▁se', '▁trata', 'ba', '▁simplemente', '▁de', '▁otra', '▁mesa', '▁marcia', 'na', ',', '▁común', '▁alrededor', '▁de', '▁Ci', 'do', 'nia', ',', '▁solo', '▁que', '▁és', 'ta', '▁tenía', '▁sombra', 's', '.', '▁Eso', '▁lo', '▁hizo', '▁parecer', '▁un', '▁fara', 'ón', '▁e', 'gip', 'cio', '.', '▁Muy', '▁pocos', '▁días', '▁después', ',', '▁revela', 'mos', '▁la', '▁imagen', '▁para', '▁que', '▁todos', '▁la', '▁vier', 'an', ',', '▁y', '▁nos', '▁asegura', 'mos', '▁de', '▁notar', '▁que', '▁era', '▁una', '▁enorme', '▁formación', '▁ro', 'cosa', '▁que', '▁simplemente', '▁se', '▁parecía', '▁a', '▁una', '▁cabeza', '▁y', '▁un', '▁rostro', '▁humanos', ',', '▁pero', '▁todo', '▁era', '.', '▁formado', '▁por', '▁sombra', 's', '▁Sólo', '▁lo', '▁anuncia', 'mos', '▁porque', '▁pensamos', '▁que', '▁sería', '▁una', '▁buena', '▁manera', '▁de', '▁involucra', 'r', '▁al', '▁público', '▁con', '▁los', '▁halla', 'z', 'gos', '▁de', '▁la', '▁NASA', '▁y', '▁atra', 'er', '▁la', '▁atención', '▁a', '▁Marte', ',', '▁y', '▁así', '▁fue', '.', '▁El', '▁rostro', '▁de', '▁Marte', '▁pronto', '▁se', '▁convirtió', '▁en', '▁un', '▁í', 'con', 'o', '▁pop', ';', '▁film', 'ada', '▁en', '▁películas', ',', '▁apare', 'ció', '▁en', '▁libros', ',', '▁revista', 's', ',', '▁programas', '▁de', '▁radio', '▁y', '▁en', '▁las', '▁cola', 's', '▁de', '▁las', '▁ca', 'jas', '▁de', '▁las', '▁tiendas', '▁de', '▁com', 'est', 'ibles', '▁durante', '▁25', '▁años', '.', '▁Al', 'guna', 's', '▁personas', '▁pensar', 'on', '▁que', '▁la', '▁forma', '▁natural', '▁del', '▁reli', 'eve', '▁era', '▁evidencia', '▁de', '▁vida', '▁en', '▁Marte', ',', '▁y', '▁que', '▁los', '▁científicos', '▁que', 'ríamos', '▁oculta', 'rla', ',', '▁pero', '▁en', '▁realidad', ',', '▁los', '▁defensor', 'es', '▁del', '▁presupuesto', '▁de', '▁la', '▁NASA', '▁desea', 'rían', '▁que', '▁hubiera', '▁una', '▁civiliza', 'ción', '▁antigua', '▁en', '▁Marte', '.', '▁Deci', 'di', 'mos', '▁tomar', '▁otra', '▁foto', '▁solo', '▁para', '▁asegurar', 'nos', '▁de', '▁no', '▁estar', '▁', 'equivoca', 'dos', ',', '▁el', '▁5', '▁de', '▁abril', '▁de', '▁1998.', '▁Diana', '▁Martínez', '▁y', '▁su', '▁equipo', '▁de', '▁cámara', '▁de', '▁Mars', '▁Or', 'bit', 'er', '▁tomar', 'on', '▁una', '▁fotografía', '▁que', '▁era', '▁die', 'z', '▁veces', '▁más', '▁ní', 'tida', '▁que', '▁las', '▁fotografías', '▁originale', 's', '▁del', '▁Viking', ',', '▁revela', 'ndo', '▁una', '▁forma', '▁de', '▁reli', 'eve', '▁natural', ',', '▁que', '▁No', '▁significa', 'ba', '▁ningún', '▁monumento', '▁ali', 'ení', 'gen', 'a', '.', '▁\"', 'Pero', '▁esa', '▁imagen', '▁no', '▁era', '▁muy', '▁clara', '▁en', '▁absolut', 'o', ',', '▁lo', '▁que', '▁podría', '▁significa', 'r', '▁que', '▁las', '▁marcas', '▁ali', 'ení', 'gen', 'as', '▁estaban', '▁oculta', 's', '▁por', '▁la', '▁neb', 'lina', '\"', '▁Bueno', ',', '▁no', ',', '▁sí', ',', '▁ese', '▁rumor', '▁comenzó', ',', '▁pero', '▁para', '▁demostrar', '▁que', '▁estaban', '▁', 'equivoca', 'dos', ',', '▁el', '▁8', '▁de', '▁abril', '▁de', '▁2001', '▁decidi', 'mos', '▁tomar', '▁otra', '▁fotografía', ',', '▁a', 'segur', 'ándonos', '▁de', '▁que', '▁Era', '▁un', '▁día', '▁de', '▁verano', '▁sin', '▁nu', 'bes', '.', '▁El', '▁equipo', '▁de', '▁Martínez', '▁cap', 'tur', 'ó', '▁una', '▁fotografía', '▁as', 'o', 'mbro', 'sa', '▁utilizando', '▁la', '▁revolución', '▁máxima', '▁absoluta', '▁de', '▁la', '▁cámara', '.', '▁Con', '▁esta', '▁cámara', '▁puedes', '▁discern', 'ir', '▁cosas', '▁en', '▁una', '▁imagen', '▁digital', ',', '▁3', '▁veces', '▁más', '▁grande', '▁que', '▁el', '▁tamaño', '▁de', '▁un', '▁pí', 'xel', ',', '▁lo', '▁que', '▁significa', '▁que', '▁si', '▁hubiera', '▁señal', 'es', '▁de', '▁vida', ',', '▁podría', 's', '▁ver', '▁fácilmente', '▁cu', 'ál', 'es', '▁eran', '.', '▁Lo', '▁que', '▁la', '▁imagen', '▁mostra', 'ba', '▁era', '▁la', '▁col', 'ina', '▁o', '▁mesa', ',', '▁que', '▁son', '▁accidente', 's', '▁geo', 'gráficos', '▁comune', 's', '▁en', '▁el', '▁o', 'este', '▁americano', '.']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=['O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'S-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'S-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'S-ORG',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    " 'B-PER',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O',\n",
    "'O'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "5306301a-7a86-4b61-9afb-06d6a4d62b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NW\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning:  seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[478], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, load_metric\n\u001b[0;32m      2\u001b[0m metric \u001b[38;5;241m=\u001b[39m load_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseqeval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m----> 3\u001b[0m a\u001b[38;5;241m=\u001b[39mmetric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39m[labels], references\u001b[38;5;241m=\u001b[39m[etiquetas_referencia])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\metric.py:455\u001b[0m, in \u001b[0;36mMetric.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {input_name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures}\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed):\n\u001b[1;32m--> 455\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompute_kwargs)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\seqeval\\9642e8a602ba52bd4d8baee1d13b2deb8247d3719041cf02b40bf8367a05aef5\\seqeval.py:135\u001b[0m, in \u001b[0;36mSeqeval._compute\u001b[1;34m(self, predictions, references, suffix, scheme, mode, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheme should be one of [IOB1, IOB2, IOE1, IOE2, IOBES, BILOU], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheme\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 135\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(\n\u001b[0;32m    136\u001b[0m     y_true\u001b[38;5;241m=\u001b[39mreferences,\n\u001b[0;32m    137\u001b[0m     y_pred\u001b[38;5;241m=\u001b[39mpredictions,\n\u001b[0;32m    138\u001b[0m     suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[0;32m    139\u001b[0m     output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    140\u001b[0m     scheme\u001b[38;5;241m=\u001b[39mscheme,\n\u001b[0;32m    141\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    142\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    143\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m    144\u001b[0m )\n\u001b[0;32m    145\u001b[0m report\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro avg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    146\u001b[0m report\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted avg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:680\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, digits, suffix, output_dict, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cr(y_true, y_pred,\n\u001b[0;32m    671\u001b[0m               digits\u001b[38;5;241m=\u001b[39mdigits,\n\u001b[0;32m    672\u001b[0m               output_dict\u001b[38;5;241m=\u001b[39moutput_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m               suffix\u001b[38;5;241m=\u001b[39msuffix\n\u001b[0;32m    677\u001b[0m               )\n\u001b[0;32m    679\u001b[0m target_names_true \u001b[38;5;241m=\u001b[39m {type_name \u001b[38;5;28;01mfor\u001b[39;00m type_name, _, _ \u001b[38;5;129;01min\u001b[39;00m get_entities(y_true, suffix)}\n\u001b[1;32m--> 680\u001b[0m target_names_pred \u001b[38;5;241m=\u001b[39m {type_name \u001b[38;5;28;01mfor\u001b[39;00m type_name, _, _ \u001b[38;5;129;01min\u001b[39;00m get_entities(y_pred, suffix)}\n\u001b[0;32m    681\u001b[0m target_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(target_names_true \u001b[38;5;241m|\u001b[39m target_names_pred)\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_dict:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:188\u001b[0m, in \u001b[0;36mget_entities\u001b[1;34m(seq, suffix)\u001b[0m\n\u001b[0;32m    186\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m chunk[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     tag \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    189\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end_of_chunk(prev_tag, tag, prev_type, type_):\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f2159-3e79-4d4b-a4dc-b690d563763d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 52 aymurai/anonymizer-beto-cased-flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "432508a1-4f16-494f-9ae2-6a206d2dbe2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "aymurai/anonymizer-beto-cased-flair does not appear to have a file named config.json. Checkout 'https://huggingface.co/aymurai/anonymizer-beto-cased-flair/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/aymurai/anonymizer-beto-cased-flair/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    403\u001b[0m         path_or_repo_id,\n\u001b[0;32m    404\u001b[0m         filename,\n\u001b[0;32m    405\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    406\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    407\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    408\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    409\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    410\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    411\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    412\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    413\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    414\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1222\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1224\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1226\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1227\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1228\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1231\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1232\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1233\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1234\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1236\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1237\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1282\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[1;32m-> 1282\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m _get_metadata_or_catch_error(\n\u001b[0;32m   1283\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1284\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1285\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1286\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1287\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1288\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1289\u001b[0m     etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1290\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1291\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1292\u001b[0m     storage_folder\u001b[38;5;241m=\u001b[39mstorage_folder,\n\u001b[0;32m   1293\u001b[0m     relative_filename\u001b[38;5;241m=\u001b[39mrelative_filename,\n\u001b[0;32m   1294\u001b[0m )\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1646\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1647\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   1648\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1649\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1650\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1651\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1652\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1653\u001b[0m )\n\u001b[0;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    373\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    374\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    375\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 396\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:315\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    314\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-66d9e3b0-528c368156e424eb24c70025;cfe7b8be-35bc-497e-ae40-946e3a78bd85)\n\nEntry Not Found for url: https://huggingface.co/aymurai/anonymizer-beto-cased-flair/resolve/main/config.json.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[480], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maymurai/anonymizer-beto-cased-flair\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:972\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    969\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    970\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 972\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    973\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    974\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    690\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    691\u001b[0m         configuration_file,\n\u001b[0;32m    692\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    693\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    694\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    695\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    696\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    697\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    698\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    699\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    700\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    701\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    702\u001b[0m     )\n\u001b[0;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:456\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    455\u001b[0m         revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    461\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "\u001b[1;31mOSError\u001b[0m: aymurai/anonymizer-beto-cased-flair does not appear to have a file named config.json. Checkout 'https://huggingface.co/aymurai/anonymizer-beto-cased-flair/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aymurai/anonymizer-beto-cased-flair\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner\")\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51143fc8-92cc-4c5f-88c5-e295452c3b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b203e13-8095-4a04-b490-78677533942b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 53 google-bert/bert-large-cased-whole-word-masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "f03e9cdc-f9d1-4eca-b059-89e3955e22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-large-cased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-large-cased-whole-word-masking\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-large-cased-whole-word-masking\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb7228-f9d1-49a1-83a2-3051a9a490cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 54 PlanTL-GOB-ES/es_anonimization_core_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "4bff7320-9f0f-4bd8-bf10-902f1c0c9100",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "PlanTL-GOB-ES/es_anonimization_core_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/PlanTL-GOB-ES/es_anonimization_core_lg/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/PlanTL-GOB-ES/es_anonimization_core_lg/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    403\u001b[0m         path_or_repo_id,\n\u001b[0;32m    404\u001b[0m         filename,\n\u001b[0;32m    405\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    406\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    407\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    408\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    409\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    410\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    411\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    412\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    413\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    414\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1222\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1224\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1226\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1227\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1228\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1231\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1232\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1233\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1234\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1236\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1237\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1282\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[1;32m-> 1282\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m _get_metadata_or_catch_error(\n\u001b[0;32m   1283\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1284\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1285\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1286\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1287\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1288\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1289\u001b[0m     etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1290\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1291\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1292\u001b[0m     storage_folder\u001b[38;5;241m=\u001b[39mstorage_folder,\n\u001b[0;32m   1293\u001b[0m     relative_filename\u001b[38;5;241m=\u001b[39mrelative_filename,\n\u001b[0;32m   1294\u001b[0m )\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1646\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1647\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   1648\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1649\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1650\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1651\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1652\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1653\u001b[0m )\n\u001b[0;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    373\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    374\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    375\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 396\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:315\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    314\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-66d9e3d3-2ad29c9f56a9b3fe1351bb77;4a53ad73-20cb-432e-a019-f3da9c333b91)\n\nEntry Not Found for url: https://huggingface.co/PlanTL-GOB-ES/es_anonimization_core_lg/resolve/main/config.json.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[482], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlanTL-GOB-ES/es_anonimization_core_lg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlanTL-GOB-ES/es_anonimization_core_lg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    854\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:972\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    969\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    970\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 972\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    973\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    974\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    690\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    691\u001b[0m         configuration_file,\n\u001b[0;32m    692\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    693\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    694\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    695\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    696\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    697\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    698\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    699\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    700\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    701\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    702\u001b[0m     )\n\u001b[0;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:456\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    455\u001b[0m         revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    461\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "\u001b[1;31mOSError\u001b[0m: PlanTL-GOB-ES/es_anonimization_core_lg does not appear to have a file named config.json. Checkout 'https://huggingface.co/PlanTL-GOB-ES/es_anonimization_core_lg/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PlanTL-GOB-ES/es_anonimization_core_lg\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"PlanTL-GOB-ES/es_anonimization_core_lg\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "ner_results = nlp(text)\n",
    "ner_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4058b1-47a6-4294-a931-3a60dd0735a7",
   "metadata": {},
   "source": [
    "# distilbert-base-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c51532f0-b5c5-43ad-a5a4-936a4876fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.47\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([ids])\n",
    "model_args={\"trust_remote_code\": True}\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-cased\", **model_args)\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids).logits\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(input_ids, labels=labels).loss\n",
    "print('loss:',round(loss.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b63cd1a-6481-4142-a983-234cc003ac21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0', 1: 'LABEL_1'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bf4cdd5-7945-4947-8e45-b030818af8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0', 1: 'LABEL_1'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([ids])\n",
    "model_args={\"trust_remote_code\": True}\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-cased\", **model_args)\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids).logits\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(input_ids, labels=labels).loss\n",
    "print('loss:',round(loss.item(), 2))\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1db0cb12-4989-424d-82db-052395961bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0', 1: 'LABEL_1'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cac168-6e3a-4dc9-bbdf-3f42a2712b95",
   "metadata": {},
   "source": [
    "# distilbert-tuned-4labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "a87bd62f-25c0-4e6d-9ea7-d128544e9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida\n",
      "[{'entity': 'B-ORG', 'score': 0.9631609, 'index': 8, 'word': 'NASA', 'start': 16, 'end': 20}, {'entity': 'B-MISC', 'score': 0.9960303, 'index': 23, 'word': 'Face', 'start': 88, 'end': 92}, {'entity': 'I-MISC', 'score': 0.9996294, 'index': 24, 'word': 'On', 'start': 93, 'end': 95}, {'entity': 'I-MISC', 'score': 0.99930775, 'index': 25, 'word': 'Mars', 'start': 96, 'end': 100}, {'entity': 'B-LOC', 'score': 0.81088793, 'index': 36, 'word': 'Mars', 'start': 152, 'end': 156}, {'entity': 'B-MISC', 'score': 0.9751243, 'index': 58, 'word': 'Viking', 'start': 240, 'end': 246}, {'entity': 'I-MISC', 'score': 0.99428385, 'index': 59, 'word': '1', 'start': 247, 'end': 248}, {'entity': 'B-MISC', 'score': 0.9998406, 'index': 90, 'word': 'Martian', 'start': 407, 'end': 414}, {'entity': 'B-LOC', 'score': 0.8418781, 'index': 96, 'word': 'Cy', 'start': 435, 'end': 437}, {'entity': 'I-LOC', 'score': 0.94802356, 'index': 97, 'word': '##don', 'start': 437, 'end': 440}, {'entity': 'I-LOC', 'score': 0.52800894, 'index': 98, 'word': '##ia', 'start': 440, 'end': 442}, {'entity': 'B-MISC', 'score': 0.9378211, 'index': 111, 'word': 'Egypt', 'start': 496, 'end': 501}, {'entity': 'I-MISC', 'score': 0.99571407, 'index': 112, 'word': '##ion', 'start': 501, 'end': 504}, {'entity': 'I-MISC', 'score': 0.9772943, 'index': 113, 'word': 'Ph', 'start': 505, 'end': 507}, {'entity': 'I-MISC', 'score': 0.634378, 'index': 114, 'word': '##ara', 'start': 507, 'end': 510}, {'entity': 'I-PER', 'score': 0.7494665, 'index': 115, 'word': '##oh', 'start': 510, 'end': 512}, {'entity': 'B-ORG', 'score': 0.88716674, 'index': 180, 'word': 'NASA', 'start': 801, 'end': 805}, {'entity': 'B-LOC', 'score': 0.9616925, 'index': 191, 'word': 'Mars', 'start': 843, 'end': 847}, {'entity': 'B-LOC', 'score': 0.9901412, 'index': 201, 'word': 'Mars', 'start': 874, 'end': 878}, {'entity': 'B-LOC', 'score': 0.9973164, 'index': 245, 'word': 'Mars', 'start': 1087, 'end': 1091}, {'entity': 'B-ORG', 'score': 0.95490736, 'index': 263, 'word': 'NASA', 'start': 1168, 'end': 1172}, {'entity': 'B-LOC', 'score': 0.76706463, 'index': 271, 'word': 'Mars', 'start': 1219, 'end': 1223}, {'entity': 'B-DATE', 'score': 0.9477048, 'index': 290, 'word': 'April', 'start': 1296, 'end': 1301}, {'entity': 'I-DATE', 'score': 0.9566374, 'index': 291, 'word': '5', 'start': 1302, 'end': 1303}, {'entity': 'I-DATE', 'score': 0.50392693, 'index': 293, 'word': '1998', 'start': 1305, 'end': 1309}, {'entity': 'B-PER', 'score': 0.99992716, 'index': 295, 'word': 'Michael', 'start': 1311, 'end': 1318}, {'entity': 'I-PER', 'score': 0.99999297, 'index': 296, 'word': 'Mali', 'start': 1319, 'end': 1323}, {'entity': 'I-PER', 'score': 0.9999925, 'index': 297, 'word': '##n', 'start': 1323, 'end': 1324}, {'entity': 'B-MISC', 'score': 0.97444236, 'index': 300, 'word': 'Mars', 'start': 1333, 'end': 1337}, {'entity': 'I-MISC', 'score': 0.91948974, 'index': 301, 'word': 'Or', 'start': 1338, 'end': 1340}, {'entity': 'I-MISC', 'score': 0.9936864, 'index': 302, 'word': '##bit', 'start': 1340, 'end': 1343}, {'entity': 'I-MISC', 'score': 0.9945115, 'index': 303, 'word': '##er', 'start': 1343, 'end': 1345}, {'entity': 'B-MISC', 'score': 0.98049474, 'index': 318, 'word': 'Viking', 'start': 1418, 'end': 1424}, {'entity': 'B-DATE', 'score': 0.9988972, 'index': 369, 'word': 'April', 'start': 1651, 'end': 1656}, {'entity': 'I-DATE', 'score': 0.9990575, 'index': 370, 'word': '8', 'start': 1657, 'end': 1658}, {'entity': 'I-DATE', 'score': 0.99935144, 'index': 372, 'word': '2001', 'start': 1660, 'end': 1664}, {'entity': 'B-MISC', 'score': 0.99892896, 'index': 390, 'word': 'Mali', 'start': 1744, 'end': 1748}, {'entity': 'I-MISC', 'score': 0.964897, 'index': 391, 'word': '##n', 'start': 1748, 'end': 1749}, {'entity': 'B-MISC', 'score': 0.99987376, 'index': 467, 'word': 'American', 'start': 2093, 'end': 2101}, {'entity': 'B-MISC', 'score': 0.97569567, 'index': 468, 'word': 'West', 'start': 2102, 'end': 2106}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dayannex/distilbert-tuned-4labels\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dayannex/distilbert-tuned-4labels\")\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "salida=classifier(text)\n",
    "#salida = classifier(text, aggregation_strategy=\"simple\")\n",
    "print('salida')\n",
    "print(salida)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "ee44b3f6-edeb-446f-9d0e-acd2abee427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entidad_map {'NASA': 'B-ORG', 'Face': 'B-MISC', 'On': 'I-MISC', 'Mars': 'B-MISC', 'Viking': 'B-MISC', '1': 'I-MISC', 'Martian': 'B-MISC', 'Cy': 'B-LOC', '##don': 'I-LOC', '##ia': 'I-LOC', 'Egypt': 'B-MISC', '##ion': 'I-MISC', 'Ph': 'I-MISC', '##ara': 'I-MISC', '##oh': 'I-PER', 'April': 'B-DATE', '5': 'I-DATE', '1998': 'I-DATE', 'Michael': 'B-PER', 'Mali': 'B-MISC', '##n': 'I-MISC', 'Or': 'I-MISC', '##bit': 'I-MISC', '##er': 'I-MISC', '8': 'I-DATE', '2001': 'I-DATE', 'American': 'B-MISC', 'West': 'B-MISC'}\n",
      "tokens ['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obviously', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'aliens', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'circling', 'the', 'planet', ',', 'snapping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'shadowy', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figured', 'out', 'that', 'it', 'was', 'just', 'another', 'Martian', 'me', '##sa', ',', 'common', 'around', 'Cy', '##don', '##ia', ',', 'only', 'this', 'one', 'had', 'shadows', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'resembled', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'shadows', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'icon', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'haunted', 'grocery', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defenders', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civilization', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'weren', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##bit', '##er', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'revealing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'markings', 'were', 'hidden', 'by', 'haze', '\"', 'Well', 'no', ',', 'yes', 'that', 'rumor', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'amazing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'p', '##ixel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'butt', '##e', 'or', 'me', '##sa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = classifier.tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "def mapeo(entidades, tokens):\n",
    "    entidad_map = {entidad['word'].replace('▁','').replace('Ġ',''): entidad['entity'] for entidad in entidades}\n",
    "    print('entidad_map',entidad_map)\n",
    "    print('tokens',tokens)\n",
    "    labels = [entidad_map.get(token.replace('Ġ','').replace('▁',''), 'O') if token.replace('Ġ','').replace('▁','') != '' else '' for token in tokens]\n",
    "    return labels\n",
    "\n",
    "labels = mapeo(salida, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "9365f978-1858-471a-a413-299ad9a1575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'I-DATE', 'O', 'B-PER', 'B-MISC', 'I-MISC', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'B-MISC', 'O']\n",
      "['So', ',', 'if', 'you', \"'\", 're', 'a', 'NASA', 'scientist', ',', 'you', 'should', 'be', 'able', 'to', 'tell', 'me', 'the', 'whole', 'story', 'about', 'the', 'Face', 'On', 'Mars', ',', 'which', 'obviously', 'is', 'evidence', 'that', 'there', 'is', 'life', 'on', 'Mars', ',', 'and', 'that', 'the', 'face', 'was', 'created', 'by', 'aliens', ',', 'correct', '?', '\"', 'No', ',', 'twenty', 'five', 'years', 'ago', ',', 'our', 'Viking', '1', 'spacecraft', 'was', 'circling', 'the', 'planet', ',', 'snapping', 'photos', ',', 'when', 'it', 'spotted', 'the', 'shadowy', 'like', '##ness', 'of', 'a', 'human', 'face', '.', 'Us', 'scientists', 'figured', 'out', 'that', 'it', 'was', 'just', 'another', 'Martian', 'me', '##sa', ',', 'common', 'around', 'Cy', '##don', '##ia', ',', 'only', 'this', 'one', 'had', 'shadows', 'that', 'made', 'it', 'look', 'like', 'an', 'Egypt', '##ion', 'Ph', '##ara', '##oh', '.', 'Very', 'few', 'days', 'later', ',', 'we', 'revealed', 'the', 'image', 'for', 'all', 'to', 'see', ',', 'and', 'we', 'made', 'sure', 'to', 'note', 'that', 'it', 'was', 'a', 'huge', 'rock', 'formation', 'that', 'just', 'resembled', 'a', 'human', 'head', 'and', 'face', ',', 'but', 'all', 'of', 'it', 'was', 'formed', 'by', 'shadows', '.', 'We', 'only', 'announced', 'it', 'because', 'we', 'thought', 'it', 'would', 'be', 'a', 'good', 'way', 'to', 'engage', 'the', 'public', 'with', 'NASA', \"'\", 's', 'findings', ',', 'and', 'at', '##rra', '##ct', 'attention', 'to', 'Mars', '-', '-', 'and', 'it', 'did', '.', 'The', 'face', 'on', 'Mars', 'soon', 'became', 'a', 'pop', 'icon', ';', 'shot', 'in', 'movies', ',', 'appeared', 'in', 'books', ',', 'magazines', ',', 'radio', 'talk', 'shows', ',', 'and', 'haunted', 'grocery', 'store', 'check', '##out', 'lines', 'for', '25', 'years', '.', 'Some', 'people', 'thought', 'the', 'natural', 'land', '##form', 'was', 'evidence', 'of', 'life', 'on', 'Mars', ',', 'and', 'that', 'us', 'scientists', 'wanted', 'to', 'hide', 'it', ',', 'but', 'really', ',', 'the', 'defenders', 'of', 'the', 'NASA', 'budget', 'wish', 'there', 'was', 'ancient', 'civilization', 'on', 'Mars', '.', 'We', 'decided', 'to', 'take', 'another', 'shot', 'just', 'to', 'make', 'sure', 'we', 'weren', \"'\", 't', 'wrong', ',', 'on', 'April', '5', ',', '1998', '.', 'Michael', 'Mali', '##n', 'and', 'his', 'Mars', 'Or', '##bit', '##er', 'camera', 'team', 'took', 'a', 'picture', 'that', 'was', 'ten', 'times', 'sharp', '##er', 'than', 'the', 'original', 'Viking', 'photos', ',', 'revealing', 'a', 'natural', 'land', '##form', ',', 'which', 'meant', 'no', 'alien', 'monument', '.', '\"', 'But', 'that', 'picture', 'wasn', \"'\", 't', 'very', 'clear', 'at', 'all', ',', 'which', 'could', 'mean', 'alien', 'markings', 'were', 'hidden', 'by', 'haze', '\"', 'Well', 'no', ',', 'yes', 'that', 'rumor', 'started', ',', 'but', 'to', 'prove', 'them', 'wrong', 'on', 'April', '8', ',', '2001', 'we', 'decided', 'to', 'take', 'another', 'picture', ',', 'making', 'sure', 'it', 'was', 'a', 'cloud', '##less', 'summer', 'day', '.', 'Mali', '##n', \"'\", 's', 'team', 'captured', 'an', 'amazing', 'photo', 'using', 'the', 'camera', \"'\", 's', 'absolute', 'maximum', 'revolution', '.', 'With', 'this', 'camera', 'you', 'can', 'disc', '##ern', 'things', 'in', 'a', 'digital', 'image', ',', '3', 'times', 'bigger', 'than', 'the', 'p', '##ixel', 'size', 'which', 'means', 'if', 'there', 'were', 'any', 'signs', 'of', 'life', ',', 'you', 'could', 'easily', 'see', 'what', 'they', 'were', '.', 'What', 'the', 'picture', 'showed', 'was', 'the', 'butt', '##e', 'or', 'me', '##sa', ',', 'which', 'are', 'land', '##form', '##s', 'common', 'around', 'the', 'American', 'West', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(labels)\n",
    "print(tokens)\n",
    "etiquetas_referencia=['O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'I-MISC',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-LOC',\n",
    " 'I-LOC',\n",
    " 'I-LOC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'I-MISC',\n",
    " 'I-MISC',\n",
    " 'I-MISC',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-ORG',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-DATE',\n",
    " 'I-DATE',\n",
    " 'O',\n",
    " 'I-DATE',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'I-PER',\n",
    " 'I-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-DATE',\n",
    " 'I-DATE',\n",
    " 'O',\n",
    " 'I-DATE',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-PER',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'O',\n",
    " 'B-MISC',\n",
    " 'B-MISC',\n",
    " 'O'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "4431d19c-cd46-418a-bb21-7c57ea5f76b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATE': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 4}, 'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'MISC': {'precision': 0.7058823529411765, 'recall': 0.8, 'f1': 0.7500000000000001, 'number': 15}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3}, 'PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2}, 'overall_precision': 0.7407407407407407, 'overall_recall': 0.8, 'overall_f1': 0.7692307692307692, 'overall_accuracy': 0.9829424307036247}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "metric = load_metric(\"seqeval\",**model_args)\n",
    "a=metric.compute(predictions=[labels], references=[etiquetas_referencia])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a10f7-8769-4723-b593-01a689c475f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
